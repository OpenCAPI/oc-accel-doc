{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OC-Accel Overview OpenCAPI Acceleration Framework , abbreviated as OC-Accel , the Integrated Development Environment (IDE) for creating application FPGA-based accelerators. By the nature of the FPGA, all these types of accelerators are reconfigurable. The underlying enabling technology is OpenCAPI 3.0, the third-generation coherent interface allowing processing elements (Code running on CPU cores and accelerator logic implemented in FPGA chip) to seamlessly and coherently share host memory. All the code and related materials are contributed to Github here: https://github.com/OpenCAPI/oc-accel . ![conceptual_system] (pictures/conceptual_system.svg) The predecessor to OC-Accel was SNAP for CAPI 2.0 and CAPI 1.0. What is OpenCAPI OpenCAPI (Open Coherent Accelerator Processor Interface) is the third-generation coherent interface and is an open standard for coherent high-performance bus interface. Driven by emerging, accelerated heterogeneous computing and advanced memory and storage solutions, it provides the open interface that allows any microprocessor to attach to: Coherent user-level accelerators and I/O devices Advanced memories accessible via read/write or user-level DMA semantics Its specifications and ecosystem are managed by the OpenCAPI Consortium . The reference designs are open source and available on Github at the https://github.com/OpenCAPI/OpenCAPI3.0_Client_RefDesign address. What can I do with OC-Accel OC-Accel helps to easily create FPGA-based acceleration engines with OpenCAPI interfaces. More details and a step-by-step guide can be found in the \" User Guide \" tab. Generally, creating an accelerator includes the following steps: Develop your accelerator Many applications are developed using only software. However, there are classes of real-time applications for which the software cannot process the required data in a prescribed time period. Analysis tools such as statistical performance monitors or profiling tools can help to identify compute-intensive algorithms. These algorithms are sometimes called bottlenecks, hotspots or choke-points. Examples of such choke points are: streaming IO, highly complex math functions, operations using extremely large block processing, etc. Adding dedicated and customized hardware to offload these compute-intensive functions greatly improve throughput. These \"hot-spot\" functions are moved to FPGA. These functions are also called \" actions \" in this description. Software/hardware partition : After isolating the functions to run on FPGA side, the parameters need to be nailed down. This can be described as a job data structure. Learn the examples (see \" Examples \" tab) for a start. A few libosnap API functions will help you manipulate the FPGA card and the software/hardware interface. Work on the hardware action: Write the \"hardware action\" in a supported programming language, such as Vivado HLS or Verilog/VHDL. Together with the software part which invokes this hardware action, OC-Accel supports running co-simulation to verify correctness. After the co-simulation is done, generate the FPGA bit image. The development environment for OC-Accel is Linux with Xilinx Vivado installed. You can also install other supported simulators to get better simulation speed. The FPGA card (target hardware) is NOT required during architecture development. Deploy it and run Deploy to Server : Program the bit image to a real FPGA card. Compile the software code on OpenCAPI-enabled systems and run! OC-Accel Framework Now let's have a glance at the diagram of OC-Accel framework. For more details about the directories, files and design hierarchy, see the repository structure page. The framework hardware consists of: TLx/DLx: Transaction layer and Datalink layer of OpenCAPI device. cfg: Config subsystem of OpenCAPI snap_core: In Bridge mode, it provides the protocol translation for two directions. Module \"mmio\" converts TLx commands from Host Server to AXI4-Lite slave interface. Module \"bridge\" converts AXI4-MM commands from User logic \"Hardware Action\" to the host. Hardware action: also named \"action_wrapper\" is where developers implement their accelerator logic. People can take this open-source framework to add other interfaces (for example, NVMe, Ethernet, HBM, etc) depending on the capabilities of the FPGA card. The framework software consists of: libosnap: a few user-space functions to talk to upper applications. Developers can define them freely. libocxl: a few user-space functions talking to kernel module ocxl. ocxl: Linux kernel module to support OpenCAPI hardware. Already included with the OS distributions. For more information, please refer to \" Deep Dive \" tab on the menu bar. Dependencies Required tools for development Development is usually done on a Linux (x86) computer . Xilinx Vivado : OC-Accel currently supports Xilinx FPGA devices exclusively. For synthesis, simulation model and image build, the Xilinx Vivado 2019.2 or newer tool suites are recommended. Build process: Building the code and running the make environment requires the usual development tools gcc, make, sed, awk . If not installed already, the installer package build-essential will set up the most important tools. Configuring the OC-Accel framework will automatically call a standalone tool that is based on the Linux kernel kconfig tool. The ncurses library must be installed to use the simple menu-driven user interface for kconfig . For basic examples, python is optional but suggested to install. 2.7.x is fine to use optional configurations scripts. When using the python based examples, swig will be used to define a local environment. Simulators: You can use the build-in simulation xsim from Xilinx Vivado, or you can also use other simulators like Cadence irun or xcelium . For simulation, OC-Accel also relies on the xterm program. Check the System Firmware setup page Supported FPGA cards OC-Accel framework needs a FPGA card with OpenCAPI interface, and a Slim-SAS cable to connect to the server. Today it supports: Alphadata 9V3 Alphadata 9H3 Alphadata 9H7 Bittware 250-SoC For FPGA vendors, it's easy to enable a new FPGA card with OpenCAPI interface to run OC-Accel, go to [New board support] page to learn how to. Supported Servers for deployment OpenCAPI interface needs the support on the processor side. Today you can run OpenCAPI acceleration on POWER9 servers with LaGrange or Monza processors installed. Today you can choose: LaGrange processor-based systems: IPS FP5290 Wistron Mihawk IBM IC922 Monza processor-based systems: IBM AC922 (an Acorn card is also required in place of one GPU to get the OpenCAPI links) Make sure your check the required firmware at system_firmware_setup How to report an issue Submit an \"Issue\" on the GitHub. How to search information There are two ways: Use the \"Search\" button on the menu bar (up right). Or search the opened webpage by Ctrl+F . Git clone this repository and use grep or any of your favorite tools to search \"web-doc\" folder in a terminal. All content on this website is plain text so you can search easily. For example: cd web-doc grep KEYWORD * -r Compliance with SNAP1.0/2.0 OpenCAPI is actually the third generation of CAPI technology . That's why its version starts from OpenCAPI3.0. The same acceleration frameworks for CAPI1.0 and CAPI2.0 are called SNAP1.0/2.0 and are available in an open-source git repository at the https://github.com/open-power/snap address. The SNAP1.0/2.0 supported cards can be found here . SNAP1.0 runs on POWER8 servers, with PCIe Gen3x8 cards. SNAP2.0 runs on POWER9 servers, with PCIe Gen3x16 or PCIe Gen4x8 cards. OC-Accel runs on POWER9 servers, using OpenCAPI x8 interface. Generally, your actions running on SNAP1.0/2.0 can be moved to OC-Accel directly without changing source-code. Check Migration Guide for more information. From FPGA to ASIC OC-Accel, together with the OpenCAPI3.0 Device Reference designs , allow people to move their design from FPGA to ASIC easily in order to achieve higher clock frequency and logic density. OC-Accel has implemented many scripts based on the Vivado tool, but all the components and workflow scripts are open-sourced. The steps to construct a project and perform software/hardware co-simulation are clear and easy to manipulate. After replacing some Xilinx IPs (like PLL, RAM, DDR Controller and PHY Serdes) with the selected Foundry's IPs, the full oc_fpga_top design is suitable for ASIC tape out, resulting in an even higher-performance OpenCAPI ASIC device.","title":"Overview"},{"location":"#oc-accel-overview","text":"OpenCAPI Acceleration Framework , abbreviated as OC-Accel , the Integrated Development Environment (IDE) for creating application FPGA-based accelerators. By the nature of the FPGA, all these types of accelerators are reconfigurable. The underlying enabling technology is OpenCAPI 3.0, the third-generation coherent interface allowing processing elements (Code running on CPU cores and accelerator logic implemented in FPGA chip) to seamlessly and coherently share host memory. All the code and related materials are contributed to Github here: https://github.com/OpenCAPI/oc-accel . ![conceptual_system] (pictures/conceptual_system.svg) The predecessor to OC-Accel was SNAP for CAPI 2.0 and CAPI 1.0.","title":"OC-Accel Overview"},{"location":"#what-is-opencapi","text":"OpenCAPI (Open Coherent Accelerator Processor Interface) is the third-generation coherent interface and is an open standard for coherent high-performance bus interface. Driven by emerging, accelerated heterogeneous computing and advanced memory and storage solutions, it provides the open interface that allows any microprocessor to attach to: Coherent user-level accelerators and I/O devices Advanced memories accessible via read/write or user-level DMA semantics Its specifications and ecosystem are managed by the OpenCAPI Consortium . The reference designs are open source and available on Github at the https://github.com/OpenCAPI/OpenCAPI3.0_Client_RefDesign address.","title":"What is OpenCAPI"},{"location":"#what-can-i-do-with-oc-accel","text":"OC-Accel helps to easily create FPGA-based acceleration engines with OpenCAPI interfaces. More details and a step-by-step guide can be found in the \" User Guide \" tab. Generally, creating an accelerator includes the following steps:","title":"What can I do with OC-Accel"},{"location":"#develop-your-accelerator","text":"Many applications are developed using only software. However, there are classes of real-time applications for which the software cannot process the required data in a prescribed time period. Analysis tools such as statistical performance monitors or profiling tools can help to identify compute-intensive algorithms. These algorithms are sometimes called bottlenecks, hotspots or choke-points. Examples of such choke points are: streaming IO, highly complex math functions, operations using extremely large block processing, etc. Adding dedicated and customized hardware to offload these compute-intensive functions greatly improve throughput. These \"hot-spot\" functions are moved to FPGA. These functions are also called \" actions \" in this description. Software/hardware partition : After isolating the functions to run on FPGA side, the parameters need to be nailed down. This can be described as a job data structure. Learn the examples (see \" Examples \" tab) for a start. A few libosnap API functions will help you manipulate the FPGA card and the software/hardware interface. Work on the hardware action: Write the \"hardware action\" in a supported programming language, such as Vivado HLS or Verilog/VHDL. Together with the software part which invokes this hardware action, OC-Accel supports running co-simulation to verify correctness. After the co-simulation is done, generate the FPGA bit image. The development environment for OC-Accel is Linux with Xilinx Vivado installed. You can also install other supported simulators to get better simulation speed. The FPGA card (target hardware) is NOT required during architecture development.","title":"Develop your accelerator"},{"location":"#deploy-it-and-run","text":"Deploy to Server : Program the bit image to a real FPGA card. Compile the software code on OpenCAPI-enabled systems and run!","title":"Deploy it and run"},{"location":"#oc-accel-framework","text":"Now let's have a glance at the diagram of OC-Accel framework. For more details about the directories, files and design hierarchy, see the repository structure page. The framework hardware consists of: TLx/DLx: Transaction layer and Datalink layer of OpenCAPI device. cfg: Config subsystem of OpenCAPI snap_core: In Bridge mode, it provides the protocol translation for two directions. Module \"mmio\" converts TLx commands from Host Server to AXI4-Lite slave interface. Module \"bridge\" converts AXI4-MM commands from User logic \"Hardware Action\" to the host. Hardware action: also named \"action_wrapper\" is where developers implement their accelerator logic. People can take this open-source framework to add other interfaces (for example, NVMe, Ethernet, HBM, etc) depending on the capabilities of the FPGA card. The framework software consists of: libosnap: a few user-space functions to talk to upper applications. Developers can define them freely. libocxl: a few user-space functions talking to kernel module ocxl. ocxl: Linux kernel module to support OpenCAPI hardware. Already included with the OS distributions. For more information, please refer to \" Deep Dive \" tab on the menu bar.","title":"OC-Accel Framework"},{"location":"#dependencies","text":"","title":"Dependencies"},{"location":"#required-tools-for-development","text":"Development is usually done on a Linux (x86) computer . Xilinx Vivado : OC-Accel currently supports Xilinx FPGA devices exclusively. For synthesis, simulation model and image build, the Xilinx Vivado 2019.2 or newer tool suites are recommended. Build process: Building the code and running the make environment requires the usual development tools gcc, make, sed, awk . If not installed already, the installer package build-essential will set up the most important tools. Configuring the OC-Accel framework will automatically call a standalone tool that is based on the Linux kernel kconfig tool. The ncurses library must be installed to use the simple menu-driven user interface for kconfig . For basic examples, python is optional but suggested to install. 2.7.x is fine to use optional configurations scripts. When using the python based examples, swig will be used to define a local environment. Simulators: You can use the build-in simulation xsim from Xilinx Vivado, or you can also use other simulators like Cadence irun or xcelium . For simulation, OC-Accel also relies on the xterm program. Check the System Firmware setup page","title":"Required tools for development"},{"location":"#supported-fpga-cards","text":"OC-Accel framework needs a FPGA card with OpenCAPI interface, and a Slim-SAS cable to connect to the server. Today it supports: Alphadata 9V3 Alphadata 9H3 Alphadata 9H7 Bittware 250-SoC For FPGA vendors, it's easy to enable a new FPGA card with OpenCAPI interface to run OC-Accel, go to [New board support] page to learn how to.","title":"Supported FPGA cards"},{"location":"#supported-servers-for-deployment","text":"OpenCAPI interface needs the support on the processor side. Today you can run OpenCAPI acceleration on POWER9 servers with LaGrange or Monza processors installed. Today you can choose: LaGrange processor-based systems: IPS FP5290 Wistron Mihawk IBM IC922 Monza processor-based systems: IBM AC922 (an Acorn card is also required in place of one GPU to get the OpenCAPI links) Make sure your check the required firmware at system_firmware_setup","title":"Supported Servers for deployment"},{"location":"#how-to-report-an-issue","text":"Submit an \"Issue\" on the GitHub.","title":"How to report an issue"},{"location":"#how-to-search-information","text":"There are two ways: Use the \"Search\" button on the menu bar (up right). Or search the opened webpage by Ctrl+F . Git clone this repository and use grep or any of your favorite tools to search \"web-doc\" folder in a terminal. All content on this website is plain text so you can search easily. For example: cd web-doc grep KEYWORD * -r","title":"How to search information"},{"location":"#compliance-with-snap1020","text":"OpenCAPI is actually the third generation of CAPI technology . That's why its version starts from OpenCAPI3.0. The same acceleration frameworks for CAPI1.0 and CAPI2.0 are called SNAP1.0/2.0 and are available in an open-source git repository at the https://github.com/open-power/snap address. The SNAP1.0/2.0 supported cards can be found here . SNAP1.0 runs on POWER8 servers, with PCIe Gen3x8 cards. SNAP2.0 runs on POWER9 servers, with PCIe Gen3x16 or PCIe Gen4x8 cards. OC-Accel runs on POWER9 servers, using OpenCAPI x8 interface. Generally, your actions running on SNAP1.0/2.0 can be moved to OC-Accel directly without changing source-code. Check Migration Guide for more information.","title":"Compliance with SNAP1.0/2.0"},{"location":"#from-fpga-to-asic","text":"OC-Accel, together with the OpenCAPI3.0 Device Reference designs , allow people to move their design from FPGA to ASIC easily in order to achieve higher clock frequency and logic density. OC-Accel has implemented many scripts based on the Vivado tool, but all the components and workflow scripts are open-sourced. The steps to construct a project and perform software/hardware co-simulation are clear and easy to manipulate. After replacing some Xilinx IPs (like PLL, RAM, DDR Controller and PHY Serdes) with the selected Foundry's IPs, the full oc_fpga_top design is suitable for ASIC tape out, resulting in an even higher-performance OpenCAPI ASIC device.","title":"From FPGA to ASIC"},{"location":"repository/","text":"This page introduces the components and files in OC-Accel. For a step-by-step guidance, please start from User Guide Steps in a glance . Repository Structure The diagram below shows the entire diretory structure of OC-Accel GIT repository. It links to another repository OpenCAPI3.0_Client_RefDesign or oc-bip which contains the card specific packages and modules to support OpenCAPI protocol. Sub directories in oc-accel The framework has some facilitating components: scripts : scripts for development environment. It displays a simple user interaction interface to select the card, the application to run (action), simulator, and other options. defconfig : configuration files for Jenkins regression test (users don't need them.) web-doc : documentations (this webpage) It has the modules to bridge OpenCAPI protocol: Software : provides user library to operate OpenCAPI cards like open_device(), attach_action(), etc. It includes header files and some tools. Hardware : It has a TLx-to-AXI bridge Verilog design in hdl , the scripts to build a Vivado project and run the process in setup , the simulation scripts in sim , and the link to oc-bip . Then it is the User Application actions directory. OC-Accel has already provided several examples in actions directory, including Verilog/VHDL examples and HLS (High Level Synthesis) examples. When a user wants to create a new Acceleration Application, he or she creates a new directory here. Under actions/<NAME> , use application also have software part sw , hardware part hw and test scripts test . Sub directories in oc-bip Any card vendor can add their card package support in oc-bip . The concept is similar to DSA (Device Support Archive) or BSP (Board Support Package). Board_support_packages : Card vendor need to create a separate folder for a new device. It includes: Constraint files (xdc) to describe the Card pins, flash interface, configurations and so on. Tcl files to create necessary Vivado IPs. Enprypted Verilog files to use Xilinx high speed serdes IOs. Verilog files for parameters and FPGA top. config_subsystem : Shared common logic for OpenCAPI Config. scripts : to pack the entire oc-bip to a Vivado IP (oc_bsp_wrap.xci). sim : Top Verilog file for simulation. Tlx : OpenCAPI Device transaction layer reference design. Dlx : OpenCAPI Device data link layer reference design. For more information, please refer to New Board Support . Filesets and Hardware Hierarchy Files used in Simulation Step In accelerator development, software and hardware co-simulation is a very important step. The simulation doesn't contain module tlx and dlx, but replies on OCSE to emulate the behavior of host. For more information, please refer to Co-Simulation . OCSE (OpenCAPI Simulation Engine) is also required. (TODO: update link) Top hierarchy in Simulation Step top.sv is in oc-bip/sim directory. oc_cfg is OpenCAPI Configuration subsystem. oc_function is the DUT (Design under Test) in this step. oc_snap_core is in hardware/hdl User logic action_wrapper will be implemented in actions/<NAME>/hw We use OCSE's DPI-C functions to drive and respond to the bus transactions. Files used in Implementation Step After co-simulation passed, it's time to do the Synthesis and Implementation in Vivado. For more information, please refer to User guide: build image . Top hierarchy in Implementation Step To generate a FPGA bitstream (binary image), the top design file is oc_fpga_top.v . oc_fpga_top is located in hardware/oc-bip/board_support_packeages/<CARD>/Verilog/framework_top oc_bsp_wrap includes TLx, Dlx, PHY, Flash subsystem and Card information (VPD). A script create_oc_bsp.tcl will assemble these components to a Vivado IP. oc_cfg is OpenCAPI Configuration subsystem. oc_function is what we have just simulated and proved that the functions can work correctly. oc_snap_core is in hardware/hdl User logic action_wrapper will be implemented in actions/<NAME>/hw Files used in Deployment When FPGA bit image is generated, use the tool oc-flash-script in oc-utils (TODO: update link) to download it from Power9 host server to the FPGA flash. After reboot, the bit image takes effect and you can ask application to call FPGA acceleration, with the help of libosnap and libocxl . libocxl need to be installed. Please follow the README file on its homepage. Application software and libosnap need to be compiled on Power9 host server also. For more information, please refer to User-guide: deploy .","title":"About the repository"},{"location":"repository/#repository-structure","text":"The diagram below shows the entire diretory structure of OC-Accel GIT repository. It links to another repository OpenCAPI3.0_Client_RefDesign or oc-bip which contains the card specific packages and modules to support OpenCAPI protocol.","title":"Repository Structure"},{"location":"repository/#sub-directories-in-oc-accel","text":"The framework has some facilitating components: scripts : scripts for development environment. It displays a simple user interaction interface to select the card, the application to run (action), simulator, and other options. defconfig : configuration files for Jenkins regression test (users don't need them.) web-doc : documentations (this webpage) It has the modules to bridge OpenCAPI protocol: Software : provides user library to operate OpenCAPI cards like open_device(), attach_action(), etc. It includes header files and some tools. Hardware : It has a TLx-to-AXI bridge Verilog design in hdl , the scripts to build a Vivado project and run the process in setup , the simulation scripts in sim , and the link to oc-bip . Then it is the User Application actions directory. OC-Accel has already provided several examples in actions directory, including Verilog/VHDL examples and HLS (High Level Synthesis) examples. When a user wants to create a new Acceleration Application, he or she creates a new directory here. Under actions/<NAME> , use application also have software part sw , hardware part hw and test scripts test .","title":"Sub directories in oc-accel"},{"location":"repository/#sub-directories-in-oc-bip","text":"Any card vendor can add their card package support in oc-bip . The concept is similar to DSA (Device Support Archive) or BSP (Board Support Package). Board_support_packages : Card vendor need to create a separate folder for a new device. It includes: Constraint files (xdc) to describe the Card pins, flash interface, configurations and so on. Tcl files to create necessary Vivado IPs. Enprypted Verilog files to use Xilinx high speed serdes IOs. Verilog files for parameters and FPGA top. config_subsystem : Shared common logic for OpenCAPI Config. scripts : to pack the entire oc-bip to a Vivado IP (oc_bsp_wrap.xci). sim : Top Verilog file for simulation. Tlx : OpenCAPI Device transaction layer reference design. Dlx : OpenCAPI Device data link layer reference design. For more information, please refer to New Board Support .","title":"Sub directories in oc-bip"},{"location":"repository/#filesets-and-hardware-hierarchy","text":"","title":"Filesets and Hardware Hierarchy"},{"location":"repository/#files-used-in-simulation-step","text":"In accelerator development, software and hardware co-simulation is a very important step. The simulation doesn't contain module tlx and dlx, but replies on OCSE to emulate the behavior of host. For more information, please refer to Co-Simulation . OCSE (OpenCAPI Simulation Engine) is also required. (TODO: update link)","title":"Files used in Simulation Step"},{"location":"repository/#top-hierarchy-in-simulation-step","text":"top.sv is in oc-bip/sim directory. oc_cfg is OpenCAPI Configuration subsystem. oc_function is the DUT (Design under Test) in this step. oc_snap_core is in hardware/hdl User logic action_wrapper will be implemented in actions/<NAME>/hw We use OCSE's DPI-C functions to drive and respond to the bus transactions.","title":"Top hierarchy in Simulation Step"},{"location":"repository/#files-used-in-implementation-step","text":"After co-simulation passed, it's time to do the Synthesis and Implementation in Vivado. For more information, please refer to User guide: build image .","title":"Files used in Implementation Step"},{"location":"repository/#top-hierarchy-in-implementation-step","text":"To generate a FPGA bitstream (binary image), the top design file is oc_fpga_top.v . oc_fpga_top is located in hardware/oc-bip/board_support_packeages/<CARD>/Verilog/framework_top oc_bsp_wrap includes TLx, Dlx, PHY, Flash subsystem and Card information (VPD). A script create_oc_bsp.tcl will assemble these components to a Vivado IP. oc_cfg is OpenCAPI Configuration subsystem. oc_function is what we have just simulated and proved that the functions can work correctly. oc_snap_core is in hardware/hdl User logic action_wrapper will be implemented in actions/<NAME>/hw","title":"Top hierarchy in Implementation Step"},{"location":"repository/#files-used-in-deployment","text":"When FPGA bit image is generated, use the tool oc-flash-script in oc-utils (TODO: update link) to download it from Power9 host server to the FPGA flash. After reboot, the bit image takes effect and you can ask application to call FPGA acceleration, with the help of libosnap and libocxl . libocxl need to be installed. Please follow the README file on its homepage. Application software and libosnap need to be compiled on Power9 host server also. For more information, please refer to User-guide: deploy .","title":"Files used in Deployment"},{"location":"system_firmware_setup/","text":"POWER System Firmware Setup For the development any X86 systeme supporting Vivado suits well. To to benefit from the unique \"Coherent Accelerator Processor Interface\" version 3 named OpenCAPI technology user needs to deploy on a POWER server hosting a P9 processor. The system should be updated to the latest OP940 firmware release. This firmware package and instructions for applying it, can be found at IBM FixCentral under \"8335-GTX\" designation. Setup Command to run Recommended Release OS type and level cat /etc/os-release or lsb_release -a Ubuntu 18.04 or 20.04 LTS / RH 7.6-ALT or RH 8.2 FW level sudo lsmcode FW920.20 / BMC2.04 Note: When using RH7.6 -ALT release is required to support OpenCAPI on P9","title":"System Firmware Setup"},{"location":"system_firmware_setup/#power-system-firmware-setup","text":"For the development any X86 systeme supporting Vivado suits well. To to benefit from the unique \"Coherent Accelerator Processor Interface\" version 3 named OpenCAPI technology user needs to deploy on a POWER server hosting a P9 processor. The system should be updated to the latest OP940 firmware release. This firmware package and instructions for applying it, can be found at IBM FixCentral under \"8335-GTX\" designation. Setup Command to run Recommended Release OS type and level cat /etc/os-release or lsb_release -a Ubuntu 18.04 or 20.04 LTS / RH 7.6-ALT or RH 8.2 FW level sudo lsmcode FW920.20 / BMC2.04 Note: When using RH7.6 -ALT release is required to support OpenCAPI on P9","title":"POWER System Firmware Setup"},{"location":"actions-doc/hdl_example/","text":"hdl_example This is the 512b VHDL design inherited from SNAP1/2.","title":"hdl_example"},{"location":"actions-doc/hdl_example/#hdl_example","text":"This is the 512b VHDL design inherited from SNAP1/2.","title":"hdl_example"},{"location":"actions-doc/hdl_single_engine/","text":"hdl_single_engine A very simple example to interface with AXI-lite slave and AXI-master and measure the bandwidth and latency.","title":"hdl_single_engine"},{"location":"actions-doc/hdl_single_engine/#hdl_single_engine","text":"A very simple example to interface with AXI-lite slave and AXI-master and measure the bandwidth and latency.","title":"hdl_single_engine"},{"location":"actions-doc/hls_decimal_mult/","text":"HLS_DECIMAL_MULT EXAMPLE Code location: Code can be found at: https://github.com/OpenCAPI/oc-accel/blob/master/actions/hls_decimal_mult/ In short: This example provides a simple base allowing to discover how to exchange single precision floating points (float) or double precision floating points (double) between the application on server and the action in the FPGA. C code is multiplying 3 decimal numbers read from host memory and writing result in host memory code can be executed on the CPU (application + software action) code can be simulated (application + software and hardware action) code can then run in hardware when the FPGA is programmed (application + software and hardware action) The example code shows the following: Application writes floats or doubles to system host memory and read results to display and compares results processed and expected Software or hardware action reads floats or doubles from system memory, multiply decimals 3 by 3 and write the results back in memory. The key point here is for the FPGA to understand the data read from the host memory and formatted by the server Operating System. The code shows the conversion to be done so that the number read can be used as a float or double in HLS code. As an example it is important to understand that 4.5 is represented in host memory differently depending on the type used: as a double as 0x4012_0000_0000_0000 as a float as 0x4090_0000. Usage: ./snap_decimal_mult -n12 -v Application calls the hardware action and multiply 12 values 3 by 3. Dumps of data displayed SNAP_CONFIG=CPU ./snap_decimal_mult #Application calls the software action SNAP_TRACE=0xF ./snap_decimal_mult #to display all MMIO exchanged between application and action ./../tests/hw_test.sh #to execute automatic testing Parameters: arguments in command line: \u200b -n [value] defines the number of decimals to process (lower or equal than MAX_NB_OF_DECIMAL_READ) \u200b -w writes to files the result processed by the action (dec_mult_action.bin) and the expected results (dec_mult_ref.bin). Used for automatic testing. \u200b -v verbose mode which will display a dump of the inputs and results from host memory parameters in include/common_decimal.h: \u200b #define MAX_NB_OF_DECIMAL_READ 16 defines the maximum number of decimals to read \u200b typedef float mat_elmt_t; definse the type used: float or double Files used: | | Makefile General Makefile used to automatically prepare the final files | README.md Documentation file for this example | \u251c\u2500\u2500\u2500sw Software directory containing application called from POWER host and software action | snap_decimal_mult.c APPLICATION which calls the software or the hardware action depending on the flag used | (use SNAP_CONFIG=CPU to call software action and SNAP_CONFIG=FPGA or nothing to call hardware action) | action_decimal_mult.c SOFTWARE ACTION which will be executed on the CPU only | Makefile Makefile to compile the software files | \u251c\u2500\u2500\u2500include Common directory to sw and hw | common_decimal.h COMMON HEADER file used by the application and the software/hardware action. | (It contains the main structure and the defines parameters) | \u251c\u2500\u2500\u2500hw Hardware directory containing the hardware action | action_decimal_mult.cpp HARDWARE ACTION which will be executed on FPGA and is called by the application | action_decimal_mult.H header file containing hardware action parameters | Makefile Makefile to compile the hardware action using Vivado HLS synthesizer | \u2514\u2500\u2500\u2500tests Test directory containing all automated tests hw_test.sh Basic test shell running snap_decimal_mult application","title":"hls_decimal_mult"},{"location":"actions-doc/hls_decimal_mult/#hls_decimal_mult-example","text":"","title":"HLS_DECIMAL_MULT EXAMPLE"},{"location":"actions-doc/hls_decimal_mult/#code-location","text":"Code can be found at: https://github.com/OpenCAPI/oc-accel/blob/master/actions/hls_decimal_mult/","title":"Code location:"},{"location":"actions-doc/hls_decimal_mult/#in-short","text":"This example provides a simple base allowing to discover how to exchange single precision floating points (float) or double precision floating points (double) between the application on server and the action in the FPGA. C code is multiplying 3 decimal numbers read from host memory and writing result in host memory code can be executed on the CPU (application + software action) code can be simulated (application + software and hardware action) code can then run in hardware when the FPGA is programmed (application + software and hardware action) The example code shows the following: Application writes floats or doubles to system host memory and read results to display and compares results processed and expected Software or hardware action reads floats or doubles from system memory, multiply decimals 3 by 3 and write the results back in memory. The key point here is for the FPGA to understand the data read from the host memory and formatted by the server Operating System. The code shows the conversion to be done so that the number read can be used as a float or double in HLS code. As an example it is important to understand that 4.5 is represented in host memory differently depending on the type used: as a double as 0x4012_0000_0000_0000 as a float as 0x4090_0000.","title":"In short:"},{"location":"actions-doc/hls_decimal_mult/#usage","text":"./snap_decimal_mult -n12 -v Application calls the hardware action and multiply 12 values 3 by 3. Dumps of data displayed SNAP_CONFIG=CPU ./snap_decimal_mult #Application calls the software action SNAP_TRACE=0xF ./snap_decimal_mult #to display all MMIO exchanged between application and action ./../tests/hw_test.sh #to execute automatic testing","title":"Usage:"},{"location":"actions-doc/hls_decimal_mult/#parameters","text":"arguments in command line: \u200b -n [value] defines the number of decimals to process (lower or equal than MAX_NB_OF_DECIMAL_READ) \u200b -w writes to files the result processed by the action (dec_mult_action.bin) and the expected results (dec_mult_ref.bin). Used for automatic testing. \u200b -v verbose mode which will display a dump of the inputs and results from host memory parameters in include/common_decimal.h: \u200b #define MAX_NB_OF_DECIMAL_READ 16 defines the maximum number of decimals to read \u200b typedef float mat_elmt_t; definse the type used: float or double","title":"Parameters:"},{"location":"actions-doc/hls_decimal_mult/#files-used","text":"| | Makefile General Makefile used to automatically prepare the final files | README.md Documentation file for this example | \u251c\u2500\u2500\u2500sw Software directory containing application called from POWER host and software action | snap_decimal_mult.c APPLICATION which calls the software or the hardware action depending on the flag used | (use SNAP_CONFIG=CPU to call software action and SNAP_CONFIG=FPGA or nothing to call hardware action) | action_decimal_mult.c SOFTWARE ACTION which will be executed on the CPU only | Makefile Makefile to compile the software files | \u251c\u2500\u2500\u2500include Common directory to sw and hw | common_decimal.h COMMON HEADER file used by the application and the software/hardware action. | (It contains the main structure and the defines parameters) | \u251c\u2500\u2500\u2500hw Hardware directory containing the hardware action | action_decimal_mult.cpp HARDWARE ACTION which will be executed on FPGA and is called by the application | action_decimal_mult.H header file containing hardware action parameters | Makefile Makefile to compile the hardware action using Vivado HLS synthesizer | \u2514\u2500\u2500\u2500tests Test directory containing all automated tests hw_test.sh Basic test shell running snap_decimal_mult application","title":"Files used:"},{"location":"actions-doc/hls_hbm_memcopy_1024/","text":"hls_hbm_memcopy_1024 This is an action using HLS, 1024b and HBM memory found on fpga used on cards like OC-9Hx. 1024b is the optimum configuration as P9 OpenCAPI uses a 1024 bits wide bus. It can be checked in action/Kconfig file that the ACTION_HALF_WIDTH bloc is not used for this example, so the interface uses the OpenCAPI 1024 bit bus. Note on HBM usage in OC-ACCEL: OC-9H3 and OC-9H7 cards for example each contain 8GB of HBM reachable through up to 32 AXI buses connected to 256MB HBM memories. The test done here exercises only 1 HBM. As all these HBM can be accessed independently in parallel, this means that the overall throughout of the 8GB of HBM can be multiplied by 32. The HBM can also be configured in different manners (One 8GB HBM, multiple access to 256MB modules,...). The default choice in Kconfig menu is 12 HBM memories set in parallel. Reducing to 1 memory can easily be done by modifying at the same time: in the Kconfig menu in the hls_hbm_memcopy_1024.cpp code changing the parameter #define HBM_AXI_IF_NB to 1. Bandwidth Evaluation Test This generic test can be also used to evaluate the throughput to/from FPGA and LCL memories (local can be DDR or HBM depending on cards used). It reports bandwidth of: Host -> FPGA_RAM FPGA_RAM -> Host FPGA (HBM -> RAM) FPGA (RAM -> HBM) hw_throughput_test Example on IC922 with a OC-AD9H3 card: $ cd actions/hls_hbm_memcopy_1024/tests $ sudo ./hw_throughput_test.sh -dINCR +-------------------------------------------------------------------------------+ | OC-Accel hls_hbm_memcopy_1024 Throughput (MBytes/s) | +-------------------------------------------------------------------------------+ bytes Host->FPGA_RAM FPGA_RAM->Host FPGA(HBM->RAM) FPGA(RAM->HBM) ------------------------------------------------------------------------------- 512 0.739 0.742 0.741 9.481 1024 17.965 18.963 1.488 1.482 2048 2.955 2.934 2.968 3.012 4096 5.945 5.885 5.902 5.911 8192 11.924 11.907 11.703 148.945 16384 292.571 292.571 227.556 23.406 32768 46.612 47.080 46.217 555.390 65536 897.753 1110.780 1024.000 101.292 131072 186.447 185.918 185.918 199.805 262144 403.298 366.635 359.594 2759.411 524288 6393.756 5825.422 673.892 682.667 1048576 1396.240 7231.559 1216.445 1299.351 2097152 12409.183 8774.695 5475.593 5282.499 4194304 4832.147 4185.932 2166.479 2075.361 8388608 4639.717 4269.012 3063.772 4080.062 16777216 10343.536 9805.503 5027.634 4178.634 33554432 10485.760 10277.008 5053.378 4919.283 67108864 13166.346 13560.086 5627.106 5445.822 134217728 15080.644 16192.270 5969.477 5760.912 268435456 16460.354 17956.750 6062.776 5929.392 536870912 17014.893 19000.917 1073741824 17391.630 19322.677 ok Test OK To get the best results, it may be useful to ensure you have the ocapi link attached to the core where the program is executed. If you have 2 nodes (check with numactl -s), you can try the 4 following combinations: sudo numactl -m0 -N0 ./oc-accel/actions/hls_hbm_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N0 ./oc-accel/actions/hls_hbm_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m0 -N8 ./oc-accel/actions/hls_hbm_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N8 ./oc-accel/actions/hls_hbm_memcopy_1024/tests/hw_throughput_test.sh -d INCR Note: \"-m\" stands for memory: allocate selected memory from nodes \"-N\" stands for nodes: execute command on the CPUs of selected nodes","title":"hls_hbm_memcopy_1024"},{"location":"actions-doc/hls_hbm_memcopy_1024/#hls_hbm_memcopy_1024","text":"This is an action using HLS, 1024b and HBM memory found on fpga used on cards like OC-9Hx. 1024b is the optimum configuration as P9 OpenCAPI uses a 1024 bits wide bus. It can be checked in action/Kconfig file that the ACTION_HALF_WIDTH bloc is not used for this example, so the interface uses the OpenCAPI 1024 bit bus.","title":"hls_hbm_memcopy_1024"},{"location":"actions-doc/hls_hbm_memcopy_1024/#note-on-hbm-usage-in-oc-accel","text":"OC-9H3 and OC-9H7 cards for example each contain 8GB of HBM reachable through up to 32 AXI buses connected to 256MB HBM memories. The test done here exercises only 1 HBM. As all these HBM can be accessed independently in parallel, this means that the overall throughout of the 8GB of HBM can be multiplied by 32. The HBM can also be configured in different manners (One 8GB HBM, multiple access to 256MB modules,...). The default choice in Kconfig menu is 12 HBM memories set in parallel. Reducing to 1 memory can easily be done by modifying at the same time: in the Kconfig menu in the hls_hbm_memcopy_1024.cpp code changing the parameter #define HBM_AXI_IF_NB to 1.","title":"Note on HBM usage in OC-ACCEL:"},{"location":"actions-doc/hls_hbm_memcopy_1024/#bandwidth-evaluation-test","text":"This generic test can be also used to evaluate the throughput to/from FPGA and LCL memories (local can be DDR or HBM depending on cards used). It reports bandwidth of: Host -> FPGA_RAM FPGA_RAM -> Host FPGA (HBM -> RAM) FPGA (RAM -> HBM)","title":"Bandwidth Evaluation Test"},{"location":"actions-doc/hls_hbm_memcopy_1024/#hw_throughput_test","text":"Example on IC922 with a OC-AD9H3 card: $ cd actions/hls_hbm_memcopy_1024/tests $ sudo ./hw_throughput_test.sh -dINCR +-------------------------------------------------------------------------------+ | OC-Accel hls_hbm_memcopy_1024 Throughput (MBytes/s) | +-------------------------------------------------------------------------------+ bytes Host->FPGA_RAM FPGA_RAM->Host FPGA(HBM->RAM) FPGA(RAM->HBM) ------------------------------------------------------------------------------- 512 0.739 0.742 0.741 9.481 1024 17.965 18.963 1.488 1.482 2048 2.955 2.934 2.968 3.012 4096 5.945 5.885 5.902 5.911 8192 11.924 11.907 11.703 148.945 16384 292.571 292.571 227.556 23.406 32768 46.612 47.080 46.217 555.390 65536 897.753 1110.780 1024.000 101.292 131072 186.447 185.918 185.918 199.805 262144 403.298 366.635 359.594 2759.411 524288 6393.756 5825.422 673.892 682.667 1048576 1396.240 7231.559 1216.445 1299.351 2097152 12409.183 8774.695 5475.593 5282.499 4194304 4832.147 4185.932 2166.479 2075.361 8388608 4639.717 4269.012 3063.772 4080.062 16777216 10343.536 9805.503 5027.634 4178.634 33554432 10485.760 10277.008 5053.378 4919.283 67108864 13166.346 13560.086 5627.106 5445.822 134217728 15080.644 16192.270 5969.477 5760.912 268435456 16460.354 17956.750 6062.776 5929.392 536870912 17014.893 19000.917 1073741824 17391.630 19322.677 ok Test OK To get the best results, it may be useful to ensure you have the ocapi link attached to the core where the program is executed. If you have 2 nodes (check with numactl -s), you can try the 4 following combinations: sudo numactl -m0 -N0 ./oc-accel/actions/hls_hbm_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N0 ./oc-accel/actions/hls_hbm_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m0 -N8 ./oc-accel/actions/hls_hbm_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N8 ./oc-accel/actions/hls_hbm_memcopy_1024/tests/hw_throughput_test.sh -d INCR Note: \"-m\" stands for memory: allocate selected memory from nodes \"-N\" stands for nodes: execute command on the CPUs of selected nodes","title":"hw_throughput_test"},{"location":"actions-doc/hls_helloworld_1024/","text":"hls_helloworld_1024 High Level Synthesis example. It consists in exchanging a text file with the server host memory and manipulate it in hardware to change the case and return the result into server host memory. Inherited from SNAP1/2 512bit wide bus example, it has been changed to use a 1024 bit wide bus. This allows to remove the bus converter and uses direct 1024 bits AXI bus. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is not used for this example, so the interface uses the OpenCAPI 1024 bit bus.","title":"hls_helloworld_1024"},{"location":"actions-doc/hls_helloworld_1024/#hls_helloworld_1024","text":"High Level Synthesis example. It consists in exchanging a text file with the server host memory and manipulate it in hardware to change the case and return the result into server host memory. Inherited from SNAP1/2 512bit wide bus example, it has been changed to use a 1024 bit wide bus. This allows to remove the bus converter and uses direct 1024 bits AXI bus. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is not used for this example, so the interface uses the OpenCAPI 1024 bit bus.","title":"hls_helloworld_1024"},{"location":"actions-doc/hls_helloworld_512/","text":"hls_helloworld_512 Code location: Code can be found at: https://github.com/OpenCAPI/oc-accel/blob/master/actions/hls_helloworld_python/ In short: High Level Synthesis example. It consists in exchanging a text file with the server host memory and manipulate it in hardware to change the case and return the result into server host memory. Inherited from SNAP1/2 512bit wide bus example, it keeps the same bus width at the cost of hardware AXI converter to use the OC 1024 bit wide bus. It shows the smooth transition from CAPI1/2 design to OC. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is set for this example, so the interface will implement the half width converter. This allows older actions to be converted in a snap at the cost of lower performance.","title":"hls_helloworld_512"},{"location":"actions-doc/hls_helloworld_512/#hls_helloworld_512","text":"","title":"hls_helloworld_512"},{"location":"actions-doc/hls_helloworld_512/#code-location","text":"Code can be found at: https://github.com/OpenCAPI/oc-accel/blob/master/actions/hls_helloworld_python/","title":"Code location:"},{"location":"actions-doc/hls_helloworld_512/#in-short","text":"High Level Synthesis example. It consists in exchanging a text file with the server host memory and manipulate it in hardware to change the case and return the result into server host memory. Inherited from SNAP1/2 512bit wide bus example, it keeps the same bus width at the cost of hardware AXI converter to use the OC 1024 bit wide bus. It shows the smooth transition from CAPI1/2 design to OC. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is set for this example, so the interface will implement the half width converter. This allows older actions to be converted in a snap at the cost of lower performance.","title":"In short:"},{"location":"actions-doc/hls_helloworld_python/","text":"hls_helloworld_python Code location: Code can be found at: https://github.com/OpenCAPI/oc-accel/blob/master/actions/hls_helloworld_python/ In short: This example provides a simple base allowing to discover OC-ACCEL. It uses a python described application (running on the host) to exercise an \"hls\" written action (the part running in the FPGA). The action is the exact same action as in the helloworld_1024 example. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is not used for this example, so the interface uses the OpenCAPI 1024 bit bus. Hence the binary file to load into the FPGA is the same as the helloworld_1024 binary file. Users' Guide: Python code is changing characters case of a user phrase \u200b code can be executed on the CPU (will transform all characters to lower case) \u200b code can be simulated (will transform all characters to upper case in simulation) \u200b code can then run in hardware when the FPGA is programmed (will transform all characters to upper case in hardware) The example code uses the copy mechanism to get/put the file from/to system host memory to/from DDR FPGA attached memory Prerequisites: Some packages may need to be installed on your system: # Installing python development library, swig and curl sudo apt-get install python3-dev python3-venv swig curl Get started - Compilation: To get started use the following steps: git clone https://github.com/OpenCAPI/oc-accel.git #if using regular https method git git@github.com:OpenCAPI/oc-accel.git # if using ssh method with privileges git clone https://github.com/OpenCAPI/ocse cd oc-accel make snap_config #select (X) HLS_helloworld_python vim snap_env.sh -> export SNAP_ROOT=/home/~/oc-accel #or your oc-accel directory # Check ocse PATH if not default ~/ocse . snap_env.sh make software # this will compile any required tools cd actions/hls_helloworld_python/sw python3 -m venv env # to create a local dev environment # note you will have a \"env\" before the prompt to remind # you the local environment you work in source env/bin/activate # pip3 install -r requirements.txt # to be improved : some errors might occur depending on environment make pywrap # to compile the appropriate libraries for SWIG Get started - Simulation: To launch a Python shell and use the OCSE (RTL simulation) Run action simulation in your swig env cd ${SNAP_ROOT} make sim On the xTerm that pops-up: oc_maint -vvv LD_LIBRARY_PATH=$OCSE_ROOT/libocxl/ python3 # once inside python call the libs import sys import os snap_action_sw=os.environ['SNAP_ROOT'] + \"/actions/hls_helloworld_python/sw\" print(snap_action_sw) sys.path.append(snap_action_sw) import snap_helloworld_python input = \"Hello world. This is my first CAPI SNAP experience with Python. It's extremely fun\" output = \"11111111111111111111111111111111111111111111111111111111111111111111111111111111111111\" out, output = snap_helloworld_python.uppercase(input) print(\"Output from FPGA:\"+output) print(\"Output from CPU :\"+input.upper()) exit() # from python shell exit # from xTerm Get started -Simulation with Jupyter Notebook To launch a Jupyter Notebook and use the OCSE (RTL simulation) Run action simulation cd ${SNAP_ROOT} make sim On the xTerm that pops-up: oc_maint -vvv cp ../../../../actions/hls_helloworld_python/sw/snap_helloworld_python.py . cp ../../../../actions/hls_helloworld_python/sw/trieres_helloworld_cosim.ipynb . # For Jupyter: jupyter notebook trieres_helloworld_cosim.ipynb # and follow the instrunctions: Select every cell and run it with Ctrl+d # For Jupyter Lab: jupyter-lab trieres_helloworld_cosim.ipynb # and follow the instrunctions: Select every cell and run it with Ctrl+d Ctrl+c # kill Jupyter Notebook / Jupyter Lab exit # from xTerm Get started - Excution on P9 To launch a Jupyter Notebook on P9 (on the FPGA card) Ensure you have compiled oc-accel's software on P9 cd ${SNAP_ROOT} make software Continue as any action (you may need sudo when execution jupyter to have valid access rights for the card): sudo oc_maint -vvv cd ${SNAP_ROOT}/actions/hls_helloworld_python/sw/ sudo jupyter notebook trieres_helloworld.ipynb # and follow the instrunctions: Select every cell and run it with Ctrl+d Ctrl+c # kill Jupyter Notebook exit # from xTerm","title":"hls_helloworld_python"},{"location":"actions-doc/hls_helloworld_python/#hls_helloworld_python","text":"","title":"hls_helloworld_python"},{"location":"actions-doc/hls_helloworld_python/#code-location","text":"Code can be found at: https://github.com/OpenCAPI/oc-accel/blob/master/actions/hls_helloworld_python/","title":"Code location:"},{"location":"actions-doc/hls_helloworld_python/#in-short","text":"This example provides a simple base allowing to discover OC-ACCEL. It uses a python described application (running on the host) to exercise an \"hls\" written action (the part running in the FPGA). The action is the exact same action as in the helloworld_1024 example. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is not used for this example, so the interface uses the OpenCAPI 1024 bit bus. Hence the binary file to load into the FPGA is the same as the helloworld_1024 binary file.","title":"In short:"},{"location":"actions-doc/hls_helloworld_python/#users-guide","text":"Python code is changing characters case of a user phrase \u200b code can be executed on the CPU (will transform all characters to lower case) \u200b code can be simulated (will transform all characters to upper case in simulation) \u200b code can then run in hardware when the FPGA is programmed (will transform all characters to upper case in hardware) The example code uses the copy mechanism to get/put the file from/to system host memory to/from DDR FPGA attached memory","title":"Users' Guide:"},{"location":"actions-doc/hls_helloworld_python/#prerequisites","text":"Some packages may need to be installed on your system: # Installing python development library, swig and curl sudo apt-get install python3-dev python3-venv swig curl","title":"Prerequisites:"},{"location":"actions-doc/hls_helloworld_python/#get-started-compilation","text":"To get started use the following steps: git clone https://github.com/OpenCAPI/oc-accel.git #if using regular https method git git@github.com:OpenCAPI/oc-accel.git # if using ssh method with privileges git clone https://github.com/OpenCAPI/ocse cd oc-accel make snap_config #select (X) HLS_helloworld_python vim snap_env.sh -> export SNAP_ROOT=/home/~/oc-accel #or your oc-accel directory # Check ocse PATH if not default ~/ocse . snap_env.sh make software # this will compile any required tools cd actions/hls_helloworld_python/sw python3 -m venv env # to create a local dev environment # note you will have a \"env\" before the prompt to remind # you the local environment you work in source env/bin/activate # pip3 install -r requirements.txt # to be improved : some errors might occur depending on environment make pywrap # to compile the appropriate libraries for SWIG","title":"Get started - Compilation:"},{"location":"actions-doc/hls_helloworld_python/#get-started-simulation","text":"To launch a Python shell and use the OCSE (RTL simulation) Run action simulation in your swig env cd ${SNAP_ROOT} make sim On the xTerm that pops-up: oc_maint -vvv LD_LIBRARY_PATH=$OCSE_ROOT/libocxl/ python3 # once inside python call the libs import sys import os snap_action_sw=os.environ['SNAP_ROOT'] + \"/actions/hls_helloworld_python/sw\" print(snap_action_sw) sys.path.append(snap_action_sw) import snap_helloworld_python input = \"Hello world. This is my first CAPI SNAP experience with Python. It's extremely fun\" output = \"11111111111111111111111111111111111111111111111111111111111111111111111111111111111111\" out, output = snap_helloworld_python.uppercase(input) print(\"Output from FPGA:\"+output) print(\"Output from CPU :\"+input.upper()) exit() # from python shell exit # from xTerm","title":"Get started - Simulation:"},{"location":"actions-doc/hls_helloworld_python/#_1","text":"","title":""},{"location":"actions-doc/hls_helloworld_python/#get-started-simulation-with-jupyter-notebook","text":"To launch a Jupyter Notebook and use the OCSE (RTL simulation) Run action simulation cd ${SNAP_ROOT} make sim On the xTerm that pops-up: oc_maint -vvv cp ../../../../actions/hls_helloworld_python/sw/snap_helloworld_python.py . cp ../../../../actions/hls_helloworld_python/sw/trieres_helloworld_cosim.ipynb . # For Jupyter: jupyter notebook trieres_helloworld_cosim.ipynb # and follow the instrunctions: Select every cell and run it with Ctrl+d # For Jupyter Lab: jupyter-lab trieres_helloworld_cosim.ipynb # and follow the instrunctions: Select every cell and run it with Ctrl+d Ctrl+c # kill Jupyter Notebook / Jupyter Lab exit # from xTerm","title":"Get started -Simulation with Jupyter Notebook"},{"location":"actions-doc/hls_helloworld_python/#_2","text":"","title":""},{"location":"actions-doc/hls_helloworld_python/#get-started-excution-on-p9","text":"To launch a Jupyter Notebook on P9 (on the FPGA card) Ensure you have compiled oc-accel's software on P9 cd ${SNAP_ROOT} make software Continue as any action (you may need sudo when execution jupyter to have valid access rights for the card): sudo oc_maint -vvv cd ${SNAP_ROOT}/actions/hls_helloworld_python/sw/ sudo jupyter notebook trieres_helloworld.ipynb # and follow the instrunctions: Select every cell and run it with Ctrl+d Ctrl+c # kill Jupyter Notebook exit # from xTerm","title":"Get started - Excution on P9"},{"location":"actions-doc/hls_image_filter/","text":"hls_image_filter Code location: Code can be found at: https://github.com/OpenCAPI/oc-accel/blob/master/actions/hls_image_filter/ In short: This is an action is an example converted from CAPI2 and thus using HLS, 512 bits bus wide towards OpenCAPI interface which is 1024 bits wide. To achieve this a 1024 to 512b converter is introduced, as P9 OpenCAPI uses a 1024 bits wide bus. This allows older actions to be converted in a snap at the cost of lower performance. The goal is to convert into grayscale the pixels from a bitmap file when they contains too much red component. Other pixels are left unmodified.","title":"hls_image_filter"},{"location":"actions-doc/hls_image_filter/#hls_image_filter","text":"","title":"hls_image_filter"},{"location":"actions-doc/hls_image_filter/#code-location","text":"Code can be found at: https://github.com/OpenCAPI/oc-accel/blob/master/actions/hls_image_filter/","title":"Code location:"},{"location":"actions-doc/hls_image_filter/#in-short","text":"This is an action is an example converted from CAPI2 and thus using HLS, 512 bits bus wide towards OpenCAPI interface which is 1024 bits wide. To achieve this a 1024 to 512b converter is introduced, as P9 OpenCAPI uses a 1024 bits wide bus. This allows older actions to be converted in a snap at the cost of lower performance. The goal is to convert into grayscale the pixels from a bitmap file when they contains too much red component. Other pixels are left unmodified.","title":"In short:"},{"location":"actions-doc/hls_memcopy_1024/","text":"hls_memcopy_1024 This is an action using HLS, 1024b. This is the optimum configuration as P9 OpenCAPI uses a 1024 bits wide bus. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is not used for this example, so the interface uses the OpenCAPI 1024 bit bus. This generic test can be also used to evaluate the throughput to/from FPGA and LCL memories (local can be DDR or HBM depending on cards used). It reports bandwidth of: Host -> FPGA_RAM FPGA_RAM -> Host FPGA (DDR -> RAM) FPGA (RAM -> DDR) hw_test Example on IC922 with a OC-AD9V3 card: $ cd actions/hls_memcopy_1024/tests $ sudo ./hw_throughput_test.sh -dINCR Build Date: [00000008] 0000202009150921 +-------------------------------------------------------------------------------+ | OC-Accel hls_memcopy_1024 Throughput (MBytes/s) | +-------------------------------------------------------------------------------+ +------------LCL stands for DDR or HBM memory accordingto hardware--------------+ bytes Host->FPGA_RAM FPGA_RAM->Host FPGA(LCL->BRAM) FPGA(BRAM->LCL) ------------------------------------------------------------------------------- 512 8.828 10.240 10.240 11.907 1024 23.814 20.480 1.484 1.476 2048 3.225 2.926 2.985 2.985 4096 5.971 6.554 6.491 80.314 8192 11.924 6.192 6.141 6.466 16384 12.337 12.911 12.921 12.870 32768 24.768 24.693 24.787 25.863 65536 49.461 95.118 92.959 102.721 131072 204.800 188.052 188.322 97.815 262144 195.484 203.055 195.193 202.741 524288 404.856 399.305 380.194 383.251 1048576 759.838 1351.258 775.574 741.567 2097152 1457.368 1408.430 1402.777 1391.607 4194304 2720.042 4185.932 4096.000 4096.000 8388608 7483.147 6732.430 6091.945 6061.133 16777216 7584.637 10292.771 6193.140 6181.730 33554432 10525.230 13584.790 9683.819 9703.422 67108864 13899.930 16615.218 10789.206 10764.977 134217728 17563.168 16927.447 11443.237 11411.131 268435456 17688.156 20650.470 11786.409 11749.265 To get the best results, it may be useful to ensure you have the ocapi link attached to the core where the program is executed. If you have 2 nodes (check with numactl -s), you can try the 4 following combinations: sudo numactl -m0 -N0 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N0 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m0 -N8 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N8 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR Note: \"-m\" stands for memory: allocate selected memory from nodes \"-N\" stands for nodes: execute command on the CPUs of selected nodes","title":"hls_memcopy_1024"},{"location":"actions-doc/hls_memcopy_1024/#hls_memcopy_1024","text":"This is an action using HLS, 1024b. This is the optimum configuration as P9 OpenCAPI uses a 1024 bits wide bus. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is not used for this example, so the interface uses the OpenCAPI 1024 bit bus. This generic test can be also used to evaluate the throughput to/from FPGA and LCL memories (local can be DDR or HBM depending on cards used). It reports bandwidth of: Host -> FPGA_RAM FPGA_RAM -> Host FPGA (DDR -> RAM) FPGA (RAM -> DDR)","title":"hls_memcopy_1024"},{"location":"actions-doc/hls_memcopy_1024/#hw_test","text":"Example on IC922 with a OC-AD9V3 card: $ cd actions/hls_memcopy_1024/tests $ sudo ./hw_throughput_test.sh -dINCR Build Date: [00000008] 0000202009150921 +-------------------------------------------------------------------------------+ | OC-Accel hls_memcopy_1024 Throughput (MBytes/s) | +-------------------------------------------------------------------------------+ +------------LCL stands for DDR or HBM memory accordingto hardware--------------+ bytes Host->FPGA_RAM FPGA_RAM->Host FPGA(LCL->BRAM) FPGA(BRAM->LCL) ------------------------------------------------------------------------------- 512 8.828 10.240 10.240 11.907 1024 23.814 20.480 1.484 1.476 2048 3.225 2.926 2.985 2.985 4096 5.971 6.554 6.491 80.314 8192 11.924 6.192 6.141 6.466 16384 12.337 12.911 12.921 12.870 32768 24.768 24.693 24.787 25.863 65536 49.461 95.118 92.959 102.721 131072 204.800 188.052 188.322 97.815 262144 195.484 203.055 195.193 202.741 524288 404.856 399.305 380.194 383.251 1048576 759.838 1351.258 775.574 741.567 2097152 1457.368 1408.430 1402.777 1391.607 4194304 2720.042 4185.932 4096.000 4096.000 8388608 7483.147 6732.430 6091.945 6061.133 16777216 7584.637 10292.771 6193.140 6181.730 33554432 10525.230 13584.790 9683.819 9703.422 67108864 13899.930 16615.218 10789.206 10764.977 134217728 17563.168 16927.447 11443.237 11411.131 268435456 17688.156 20650.470 11786.409 11749.265 To get the best results, it may be useful to ensure you have the ocapi link attached to the core where the program is executed. If you have 2 nodes (check with numactl -s), you can try the 4 following combinations: sudo numactl -m0 -N0 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N0 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m0 -N8 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N8 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR Note: \"-m\" stands for memory: allocate selected memory from nodes \"-N\" stands for nodes: execute command on the CPUs of selected nodes","title":"hw_test"},{"location":"actions-doc/hls_memcopy_512/","text":"hls_memcopy_(512) This is an action using HLS, 512 bits bus wide towards OpenCAPI interface which is 1024 bits wide. To achieve this a 1024 to 512b converter is introduced, as P9 OpenCAPI uses a 1024 bits wide bus. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is set for this example, so the interface will implement the half width converter. This allows older actions to be converted in a snap at the cost of lower performance. This generic test can be also used to evaluate the throughput to/from FPGA and LCL memories (local can be DDR or HBM depending on cards used). It reports bandwidth of: Host -> FPGA_RAM FPGA_RAM -> Host FPGA (LCL -> RAM) FPGA (RAM -> LCL) hw_test Example on IC922 with a OC-AD9V3 card: $ cd actions/hls_memcopy_512/tests $ sudo ./hw_throughput_test.sh -dINCR Build Date: [00000008] 0000202009150920 +-------------------------------------------------------------------------------+ | OC-Accel hls_memcopy_512 Throughput (MBytes/s) | +-------------------------------------------------------------------------------+ +------------LCL stands for DDR or HBM memory accordingto hardware--------------+ bytes Host->FPGA_RAM FPGA_RAM->Host FPGA(LCL->BRAM) FPGA(BRAM->LCL) ------------------------------------------------------------------------------- 512 10.240 10.240 10.449 10.449 1024 1.497 20.480 21.333 25.600 2048 40.960 41.796 42.667 41.796 4096 81.920 81.920 83.592 83.592 8192 132.129 11.855 11.872 11.977 16384 23.918 321.255 12.319 12.319 32768 24.582 46.946 47.628 47.628 65536 94.432 95.394 94.980 94.980 131072 188.593 189.959 187.782 186.979 262144 370.260 366.635 372.364 371.309 524288 717.220 729.191 718.203 717.220 1048576 1354.749 1361.787 1353.001 1476.868 2097152 2621.440 2427.259 2441.388 2467.238 4194304 4084.035 2523.649 2520.615 2593.880 8388608 4167.217 4120.141 4158.953 4136.394 16777216 6028.464 8140.328 7084.973 8101.022 33554432 9877.666 8212.049 7983.448 8152.194 67108864 9884.941 9565.117 9774.084 9765.551 134217728 10507.925 10888.110 10844.124 10824.883 268435456 12041.244 11543.625 11482.887 11460.336 To get the best results, it may be useful to ensure you have the ocapi link attached to the core where the program is executed. If you have 2 nodes (check with numactl -s), you can try the 4 following combinations: sudo numactl -m0 -N0 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N0 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m0 -N8 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N8 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR Note: \"-m\" stands for memory: allocate selected memory from nodes \"-N\" stands for nodes: execute command on the CPUs of selected nodes","title":"hls_memcopy_512"},{"location":"actions-doc/hls_memcopy_512/#hls_memcopy_512","text":"This is an action using HLS, 512 bits bus wide towards OpenCAPI interface which is 1024 bits wide. To achieve this a 1024 to 512b converter is introduced, as P9 OpenCAPI uses a 1024 bits wide bus. It can be checked in /action.Kconfig file that the ACTION_HALF_WIDTH bloc is set for this example, so the interface will implement the half width converter. This allows older actions to be converted in a snap at the cost of lower performance. This generic test can be also used to evaluate the throughput to/from FPGA and LCL memories (local can be DDR or HBM depending on cards used). It reports bandwidth of: Host -> FPGA_RAM FPGA_RAM -> Host FPGA (LCL -> RAM) FPGA (RAM -> LCL)","title":"hls_memcopy_(512)"},{"location":"actions-doc/hls_memcopy_512/#hw_test","text":"Example on IC922 with a OC-AD9V3 card: $ cd actions/hls_memcopy_512/tests $ sudo ./hw_throughput_test.sh -dINCR Build Date: [00000008] 0000202009150920 +-------------------------------------------------------------------------------+ | OC-Accel hls_memcopy_512 Throughput (MBytes/s) | +-------------------------------------------------------------------------------+ +------------LCL stands for DDR or HBM memory accordingto hardware--------------+ bytes Host->FPGA_RAM FPGA_RAM->Host FPGA(LCL->BRAM) FPGA(BRAM->LCL) ------------------------------------------------------------------------------- 512 10.240 10.240 10.449 10.449 1024 1.497 20.480 21.333 25.600 2048 40.960 41.796 42.667 41.796 4096 81.920 81.920 83.592 83.592 8192 132.129 11.855 11.872 11.977 16384 23.918 321.255 12.319 12.319 32768 24.582 46.946 47.628 47.628 65536 94.432 95.394 94.980 94.980 131072 188.593 189.959 187.782 186.979 262144 370.260 366.635 372.364 371.309 524288 717.220 729.191 718.203 717.220 1048576 1354.749 1361.787 1353.001 1476.868 2097152 2621.440 2427.259 2441.388 2467.238 4194304 4084.035 2523.649 2520.615 2593.880 8388608 4167.217 4120.141 4158.953 4136.394 16777216 6028.464 8140.328 7084.973 8101.022 33554432 9877.666 8212.049 7983.448 8152.194 67108864 9884.941 9565.117 9774.084 9765.551 134217728 10507.925 10888.110 10844.124 10824.883 268435456 12041.244 11543.625 11482.887 11460.336 To get the best results, it may be useful to ensure you have the ocapi link attached to the core where the program is executed. If you have 2 nodes (check with numactl -s), you can try the 4 following combinations: sudo numactl -m0 -N0 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N0 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m0 -N8 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR sudo numactl -m8 -N8 ./oc-accel/actions/hls_memcopy_1024/tests/hw_throughput_test.sh -d INCR Note: \"-m\" stands for memory: allocate selected memory from nodes \"-N\" stands for nodes: execute command on the CPUs of selected nodes","title":"hw_test"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/","text":"Application Note : OpenCAPI Quickstart Installing an OC-AD9H7 on IC922 OpenCAPI QuickStart OC-AD9H7 on IC922 By IBM Systems Group October, 2020 Authors : OpenCAPI Support Team - Montpellier, FRANCE Foreword We will use SNAP word from time to time when mentionning the framework used with the previous versions of CAPI1.0 & CAPI2.0 technologies. Since OpenCAPI (sometimes called CAPI3.0) is the 3rd generation of CAPI, some former SNAP names can be found. Most of them have been changed though (eg snap_maint became oc_maint ). The supplier reference is ADM-PCIE-9H7, we will use OC-AD9H7 in OC-ACCEL. IC922 Hardware Setup Unlike CAPI2.0, OpenCAPI (OC) doesn\u2019t use PCI links, however, the card requires PCIe power supply and mechanical socket to work. An OC card can thus be theoretically installed in any PCIe socket. As the OC card can also be used in CAPI2 mode, Figure 1 indicates the slots where is can be placed for CAPI2/OpenCAPI dual usage. Note that since OC-AD9H7 is 3/4 height it can only be placed in slots 4 and 9. Figure 1. Rear view of a IC922 system with PCIe CAPI enabled slots indicated Please check details at: https://www.ibm.com/support/knowledgecenter/en/9183-22X/p9iaf/p9iaf_pcie_slot_details.htm OC-AD9H7 board Setup The OC-AD9H7 has an octuple DIP switch SW1, located on the rear side of the board. Check that all switches are configured to default settings (all \"OFF\"), but SW1-6 at \"ON\". We don\u2019t use these VPD data in SNAP/OC. At the time of writing, only one OpenCAPI link can be configured with oc-accel. So please only connect CAPI_0 connector only. Information come from Alphadata web site: https://www.alpha-data.com/dcp/products.php?product=adm-pcie-9h7 https://www.alpha-data.com/pdfs/adm-pcie-9h7%20user%20manual_v1_3.pdf Setup tools on the POWER server environment Setup the followings to get your environment on the Power server 1) Clone the oc-accel framework (no contribution mode) git clone https://github.com/OpenCAPI/oc-accel.git Clone the oc-accel framework (contribution mode using ssh) git clone git@github.com:OpenCAPI/oc-accel.git 2) Install the libocxl libraries + reboot the server after installation. (for ubuntu) sudo apt-get install libocxl-dev (for RHEL) sudo yum install libocxl-devel (libcxl-devel package is provided by RedHat Optional repository) 3) Clone the FPGA Image loader sudo git clone https://github.ibm.com/OC-Enablement/oc-utils/ cd oc_utils sudo make install Privileges Important: libocxl requires root privileges to allow card exchanges (like oc-reset, oc_maint, usage in general). When using the card without sudo privileges, you get an normal error. Your administrator can provide user privileges using this process: Permanently create a /etc/udev/rules.d/20-ocaccel.rules file including: SUBSYSTEM==\"ocxl\", DEVPATH==\"*/ocxl/IBM,oc-snap*\", MODE=\"666\", RUN=\"/bin/chmod 666 %S/%p/global_mmio_area\" Reboot Check that the cards are recognized as accelerators Check that the cards are recognized by the Firmware and the OS lspci|grep accel 0006:00:00.0 Processing accelerators: IBM Device 062b 0006:00:00.1 Processing accelerators: IBM Device 062b In this example, for this card, physical port is 0 and virtual port is 6. If no card is found with this command, then your OC-9H7 card may not have a OpenCAPI image in it, or your firmware is too old. Here is an example of 2 cards used in a P9 server one being recognized as CAPI2 the other as OC : lspci|grep acc 0006:00:00.0 Processing accelerators: IBM Device 062b #OC 0006:00:00.1 Processing accelerators: IBM Device 062b 0008:00:00.0 Processing accelerators: IBM Device 0632 (rev 01) #CAPI2 0030:01:00.0 Processing accelerators: IBM Device 0477 (rev 02) First programming of a brand new OC-9H7 card (no CAPI image was ever installed) There are several ways to program a OC-AD9H7 card. If no CAPI image was ever installed on this board, then you\u2019ll need to follow these following instructions to set it once. Then you\u2019ll be able to go faster using the next paragraph. A basic test image for the OC-9H7 card\u2019s FPGA can be obtained from IBM Box. Programming this image onto the card will require Xilinx Vivado version 2018.1 or newer and usbprogrammer. To do so, access the device from Vivado\u2019s hardware manager and follow these steps: Right click on the \"xcvu37p_0\" entry in the device list and select \"add configuration memory device\". Select \"mt25qu01g-spi-x1_x2_x4_x8\" from the list of config mem devices. When prompted, choose to program the config mem device. In the subsequent menu, select the two .mcs files, followed by the two optional .prm files. The programming should complete in a few minutes. Note that the card needs to be power cycled (i.e. by rebooting the system) before the new image will take effect. On some servers the fast reboot service is operational, a complete power cycle is necessary to make sure the card powers off. Standard programming of a OC-9H7 card (CAPI image already in it) Images for the OC-AD9H7 card\u2019s FPGA will be created from the OC-ACCEL environment and will be placed in ~snap/hardware/build/Images Once successfully synthesized. Then using the FPGA Image loader, you will program and reset the FPGA with your binary files using the following command: sudo oc-flash-script my_user_image.bin Depending on the format of the FPGA board Flash devices, you may need 1or 2 binary files. OC-AD9H7 card needs 2 binary files noted as primary and secondary. You will so call the loader with the 2 files in the following order: sudo oc-flash-script my_user_image_primary.bin my_user_image_secondary.bin Running the Test Image if card has been already programmed with a OC image From the snap directory, compile once the software and applications cd oc-accel git pull # in case you already cloned earlier and want to stay up to date make software apps To \"locally make\" your software you can set the default ACTION_ROOT variable. The default test which has been compiled in the Box\u2019s image is a hls_memcopy example. export ACTION_ROOT=${HOME}/oc-accel/actions/hls_memcopy_1024 then set all paths automatically sourcing the script as follow: source snap_path.sh # Find available card oc_find_card -v -A ALL #This should return the card position OC-AD9H7 card has been detected in card position: 0 ... then let OC-ACCEL framework discover the cards and actions with the oc_maint command. Mention the card slot number if different from 0 using -C option. This command needs to be run almost once after a reset and will return the action stated here as \u201cIBM HLS Memcopy\u201d oc_maint -v [-C1] You can now: Either run the memcopy program doing any transfer you want. You will get the explanations on how to use this exampleby typing snap_memcopy or run automatic test of hls_memcopy_1024 using ./actions/hls_memcopy_1024/tests/hw_throughput.sh -d INCR which will give you the bandwidth measured between FPGA, host memory and on-board for different file size exchanged.","title":"OC-AD9H7 installation in IC922"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#application-note-opencapi-quickstart-installing-an-oc-ad9h7-on-ic922","text":"OpenCAPI QuickStart OC-AD9H7 on IC922 By IBM Systems Group October, 2020 Authors : OpenCAPI Support Team - Montpellier, FRANCE","title":"Application Note : OpenCAPI Quickstart Installing an OC-AD9H7 on IC922"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#foreword","text":"We will use SNAP word from time to time when mentionning the framework used with the previous versions of CAPI1.0 & CAPI2.0 technologies. Since OpenCAPI (sometimes called CAPI3.0) is the 3rd generation of CAPI, some former SNAP names can be found. Most of them have been changed though (eg snap_maint became oc_maint ). The supplier reference is ADM-PCIE-9H7, we will use OC-AD9H7 in OC-ACCEL.","title":"Foreword"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#ic922-hardware-setup","text":"Unlike CAPI2.0, OpenCAPI (OC) doesn\u2019t use PCI links, however, the card requires PCIe power supply and mechanical socket to work. An OC card can thus be theoretically installed in any PCIe socket. As the OC card can also be used in CAPI2 mode, Figure 1 indicates the slots where is can be placed for CAPI2/OpenCAPI dual usage. Note that since OC-AD9H7 is 3/4 height it can only be placed in slots 4 and 9. Figure 1. Rear view of a IC922 system with PCIe CAPI enabled slots indicated Please check details at: https://www.ibm.com/support/knowledgecenter/en/9183-22X/p9iaf/p9iaf_pcie_slot_details.htm","title":"IC922 Hardware Setup"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#oc-ad9h7-board-setup","text":"The OC-AD9H7 has an octuple DIP switch SW1, located on the rear side of the board. Check that all switches are configured to default settings (all \"OFF\"), but SW1-6 at \"ON\". We don\u2019t use these VPD data in SNAP/OC. At the time of writing, only one OpenCAPI link can be configured with oc-accel. So please only connect CAPI_0 connector only. Information come from Alphadata web site: https://www.alpha-data.com/dcp/products.php?product=adm-pcie-9h7 https://www.alpha-data.com/pdfs/adm-pcie-9h7%20user%20manual_v1_3.pdf","title":"OC-AD9H7 board Setup"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#setup-tools-on-the-power-server-environment","text":"Setup the followings to get your environment on the Power server 1) Clone the oc-accel framework (no contribution mode) git clone https://github.com/OpenCAPI/oc-accel.git Clone the oc-accel framework (contribution mode using ssh) git clone git@github.com:OpenCAPI/oc-accel.git 2) Install the libocxl libraries + reboot the server after installation. (for ubuntu) sudo apt-get install libocxl-dev (for RHEL) sudo yum install libocxl-devel (libcxl-devel package is provided by RedHat Optional repository) 3) Clone the FPGA Image loader sudo git clone https://github.ibm.com/OC-Enablement/oc-utils/ cd oc_utils sudo make install","title":"Setup tools on the POWER server environment"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#privileges","text":"Important: libocxl requires root privileges to allow card exchanges (like oc-reset, oc_maint, usage in general). When using the card without sudo privileges, you get an normal error. Your administrator can provide user privileges using this process: Permanently create a /etc/udev/rules.d/20-ocaccel.rules file including: SUBSYSTEM==\"ocxl\", DEVPATH==\"*/ocxl/IBM,oc-snap*\", MODE=\"666\", RUN=\"/bin/chmod 666 %S/%p/global_mmio_area\" Reboot","title":"Privileges"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#check-that-the-cards-are-recognized-as-accelerators","text":"Check that the cards are recognized by the Firmware and the OS lspci|grep accel 0006:00:00.0 Processing accelerators: IBM Device 062b 0006:00:00.1 Processing accelerators: IBM Device 062b In this example, for this card, physical port is 0 and virtual port is 6. If no card is found with this command, then your OC-9H7 card may not have a OpenCAPI image in it, or your firmware is too old. Here is an example of 2 cards used in a P9 server one being recognized as CAPI2 the other as OC : lspci|grep acc 0006:00:00.0 Processing accelerators: IBM Device 062b #OC 0006:00:00.1 Processing accelerators: IBM Device 062b 0008:00:00.0 Processing accelerators: IBM Device 0632 (rev 01) #CAPI2 0030:01:00.0 Processing accelerators: IBM Device 0477 (rev 02)","title":"Check that the cards are recognized as accelerators"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#first-programming-of-a-brand-new-oc-9h7-card-no-capi-image-was-ever-installed","text":"There are several ways to program a OC-AD9H7 card. If no CAPI image was ever installed on this board, then you\u2019ll need to follow these following instructions to set it once. Then you\u2019ll be able to go faster using the next paragraph. A basic test image for the OC-9H7 card\u2019s FPGA can be obtained from IBM Box. Programming this image onto the card will require Xilinx Vivado version 2018.1 or newer and usbprogrammer. To do so, access the device from Vivado\u2019s hardware manager and follow these steps: Right click on the \"xcvu37p_0\" entry in the device list and select \"add configuration memory device\". Select \"mt25qu01g-spi-x1_x2_x4_x8\" from the list of config mem devices. When prompted, choose to program the config mem device. In the subsequent menu, select the two .mcs files, followed by the two optional .prm files. The programming should complete in a few minutes. Note that the card needs to be power cycled (i.e. by rebooting the system) before the new image will take effect. On some servers the fast reboot service is operational, a complete power cycle is necessary to make sure the card powers off.","title":"First programming of a brand new OC-9H7 card (no CAPI image was ever installed)"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#standard-programming-of-a-oc-9h7-card-capi-image-already-in-it","text":"Images for the OC-AD9H7 card\u2019s FPGA will be created from the OC-ACCEL environment and will be placed in ~snap/hardware/build/Images Once successfully synthesized. Then using the FPGA Image loader, you will program and reset the FPGA with your binary files using the following command: sudo oc-flash-script my_user_image.bin Depending on the format of the FPGA board Flash devices, you may need 1or 2 binary files. OC-AD9H7 card needs 2 binary files noted as primary and secondary. You will so call the loader with the 2 files in the following order: sudo oc-flash-script my_user_image_primary.bin my_user_image_secondary.bin","title":"Standard programming of a OC-9H7 card (CAPI image already in it)"},{"location":"apps_notes/Installing_an_OC-AD9H7_on_IC922/#running-the-test-image-if-card-has-been-already-programmed-with-a-oc-image","text":"From the snap directory, compile once the software and applications cd oc-accel git pull # in case you already cloned earlier and want to stay up to date make software apps To \"locally make\" your software you can set the default ACTION_ROOT variable. The default test which has been compiled in the Box\u2019s image is a hls_memcopy example. export ACTION_ROOT=${HOME}/oc-accel/actions/hls_memcopy_1024 then set all paths automatically sourcing the script as follow: source snap_path.sh # Find available card oc_find_card -v -A ALL #This should return the card position OC-AD9H7 card has been detected in card position: 0 ... then let OC-ACCEL framework discover the cards and actions with the oc_maint command. Mention the card slot number if different from 0 using -C option. This command needs to be run almost once after a reset and will return the action stated here as \u201cIBM HLS Memcopy\u201d oc_maint -v [-C1] You can now: Either run the memcopy program doing any transfer you want. You will get the explanations on how to use this exampleby typing snap_memcopy or run automatic test of hls_memcopy_1024 using ./actions/hls_memcopy_1024/tests/hw_throughput.sh -d INCR which will give you the bandwidth measured between FPGA, host memory and on-board for different file size exchanged.","title":"Running the Test Image if card has been already programmed with a OC image"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/","text":"Application Note : OpenCAPI Quickstart Installing an OC-AD9V3 on AC922 OpenCAPI QuickStart By IBM Systems Group November, 2019 Authors Alexandre CASTELLANE, Fabrice MOYEN, Bruno MESNET, OpenCAPI Enablement Foreword We will use SNAP word from time to time when mentionning the framework used with the previous versions of CAPI1.0 & CAPI2.0 technologies. Since OpenCAPI (sometimes called CAPI3.0) is the 3rd generation of CAPI, some former SNAP names can be found. Most of them have been changed though (eg snap_maint became oc_maint ). AC922 Hardware Setup Unlike CAPI2.0 OpenCAPI (OC) doesn\u2019t use PCI links, however, the card requires PCIe power supply and mechanical socket to work. An OC card can thus be theoretically installed in any PCIe socket. As the OCcard can also be used in CAPI2 mode, Figure 1 indicates the slots where is can be placed for CAPI2/OpenCAPI dual usage. Figure 1. Rear view of a system with PCIe CAPI enabled slots indicated As a reminder and since both CAPI2.0 and OC cards can be used simultaneously, we provide the CAPI2.0 comptaible slots view: Slot identification and (location code) Description Coherent Accelerator Processor Interface (CAPI) Adapter size Processor module Coherent Accelerator Processor Interface (CAPI) 1 (P1-C5) Half-height, half-length Processor Module 1 / CPU1 No PCIe4 x4 Half-height, half-length Processor Module 1 / CPU1 No 2 (P1-C4) PCIe4 x8 Half-height, half-length Processor Module 2 / CPU0 (shared) Yes 3 (P1-C3) PCIe4 x16 Half-height, half-length Processor Module 2 / CPU0 Yes 4 (P1-C2) PCIe4 x16 Half-height, half-length Processor Module 1 / CPU1 Yes Table 1. PCIe slot locations and CAPI2.0 compatibility in AC922 Please check details at: https://www.ibm.com/support/knowledgecenter/en/POWER9/p9eik/p9eik_pcie_slot_details.htm The FPGA cards can be put in any of the PCIe slots 2, 3 or 4 which are CAPI enabled PCIe slots if you intend to use them in CAPI2.0 mode. Slot 1 can also be used if only OpenCAPI mode is desired depending on the OpenCAPI connector position (for example 9V3 cannot be placed in slot1) Any half-height, half-length cards can be used. AD9V3 with the fan option may add mechanical restrictions in the choice of the slots. Important : The slot 2 will only allow PCIe4 x8 configured cards. Even if the connector is a PCIe x16 connector, a PCIe3x16 board configured as PCIe3 x16 like AD9V3 is configured in SNAP per default cannot be put in this slot 2. Important : If you expect to use an AD9V3 card in OpenCAPI3.0 mode, you will need to put it in slot 2 or 3 due to the length of the OpenCAPI cables and the OpenCAPI connector location specific to this card. OC adapter card The OC card uses the same physical link as a GPU. An adaptation card is required to get the links hooked up to the card with a specific high speed connector. The OC links cannot be used (firmware feature) with NVLINK, so using a OC card prevent using a GPU on the same socket. If you configure in this configuration you\u2019ll get the following message at the BMC : [ 104.351386662,3] NPU: NVLink and OpenCAPI devices on same chip not supported, aborting NPU init Make sure you place 2 GPUs on one socket and one adapter on the other one, whichever you want. AD9V3 board Setup The ADM-PCIE-9V3 has 2 quad DIP switch SW1 and SW2, located on the rear side of the board. Check that both switches are configured to default setting. SW2 all set to OFF works ok since we don\u2019t use these VPD data in SNAP/OC. Information come from Alphadata web site: [ https://www.alpha-data.com/pdfs/adm-pcie-9v3.pdf Setup tools on the POWER server environment Setup the followings to get your environment on the Power server 1) Clone the oc-accel framework (no contribution mode) git clone https://github.com/OpenCAPI/oc-accel.git Clone the oc-accel framework (contribution mode using ssh) git clone git@github.com:OpenCAPI/oc-accel.git 2) Install the libocxl libraries + reboot the server after installation. (for ubuntu) sudo apt-get install libocxl-dev (for RHEL) sudo yum install libocxl-devel (libcxl-devel package is provided by RedHat Optional repository) 3) Clone the FPGA Image loader sudo git clone https://github.ibm.com/OC-Enablement/oc-utils/ cd oc_utils sudo make install Privileges Important: libocxl requires root privileges to allow card exchanges (like reset, oc_maint, usage in general). When using the card without sudo privileges, you get an normal error. Your administrator can provide user privileges using this process: Permanently create a /etc/udev/rules.d/20-ocaccel.rules file including: SUBSYSTEM==\"ocxl\", DEVPATH==\"*/ocxl/IBM,oc-snap*\", MODE=\"666\", RUN=\"/bin/chmod 666 %S/%p/global_mmio_area\" Reboot Check that the cards are recognized as accelerators Check that the cards are recognized by the Firmware and the OS lspci|grep accel 0006:00:00.0 Processing accelerators: IBM Device 062b 0006:00:00.1 Processing accelerators: IBM Device 062b In this example, for this card, physical port is 0 and virtual port is 6. If no card is found with this command, then your 9V3 card may not have a OpenCAPI image in it, or your firmware is too old. Here is an example of 2 cards used in the server one being recognized as CAPI2 the other as OC : lspci|grep acc 0006:00:00.0 Processing accelerators: IBM Device 062b #OC 0006:00:00.1 Processing accelerators: IBM Device 062b 0008:00:00.0 Processing accelerators: IBM Device 0632 (rev 01) #CAPI2 0030:01:00.0 Processing accelerators: IBM Device 0477 (rev 02) First programming of a brand new 9V3 card (no CAPI image was ever installed) There are several ways to program a 9V3 card. If no CAPI image was ever installed on this board, then you\u2019ll need to follow these following instructions to set it once. Then you\u2019ll be able to go faster using the next paragraph. A basic test image for the 9V3 card\u2019s FPGA can be obtained from Box. Programming this image onto the card will require Xilinx Vivado version 2018.1 or newerand a Xilinx Platform Cable USB II ribbon programmer. To do so, access the device from Vivado\u2019s hardware manager and follow these steps: Right click on the \"xcvu3p_0\" entry in the device list and select \"add configuration memory device\". Select \"mt25qu256-spi-x1_x2_x4_x8\" from the list of config mem devices. When prompted, choose to program the config mem device. In the subsequent menu, select the two .mcs files, followed by the two .prm files. The programming should complete in a few minutes. Note that the card needs to be power cycled (i.e. by rebooting the system) before the new image will take effect.In very rare cases, a second reboot may be needed. Standard programming of a 9V3 card (CAPI image already in it) Images for the 9V3 card\u2019s FPGA will be created from the OC-ACCEL environment and will be placed in ~snap/hardware/build/Images Once successfully synthesized. Then using the FPGA Image loader, you will program and reset the FPGA with your binary files using the following command: sudo oc-flash-script my_user_image.bin Depending on the size of the FPGA board Flash devices, you may need 1or 2 binary files. AD9V3 card needs 2 binary files noted as primary and secondary. You will so call the loader with the 2 files in the following order: sudo oc-flash-script my_user_image_primary.bin my_user_image_secondary.bin Running the Test Image if card has been already programmed with a OC image From the snap directory, compile once the software and applications cd oc-accel git pull # in case you already cloned earlier and want to stay up to date make software apps To \"locally make\" your software you can set the default ACTION_ROOT variable. The default test which has been compiled in the Box\u2019s image is a hls_memcopy example. export ACTION_ROOT=${HOME}/oc-accel/actions/hls_memcopy_1024 then set all paths automatically sourcing the script as follow: source snap_path.sh # Find available card oc_find_card -v -A ALL #This should return the card position OC-AD9V3 card has been detected in card position: 0 ... then let OC-ACCEL framework discover the cards and actions with the oc_maint command. Mention the card slot number if different from 0 using -C option. This command needs to be run almost once after a reset and will return the action stated here as \u201cIBM HLS Memcopy\u201d oc_maint -v [-C1] You can now: Either run the memcopy program doing any transfer you want. You will get the explanations on how to use this exampleby typing snap_memcopy or run automatic test of hls_memcopy_1024 using ./actions/hls_memcopy_1024/tests/hw_throughput.sh -d INCR which will give you the bandwidth measured between FPGA, host memory and on-board for different file size exchanged.","title":"OC-AD9V3 installation in AC922"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#application-note-opencapi-quickstart-installing-an-oc-ad9v3-on-ac922","text":"OpenCAPI QuickStart By IBM Systems Group November, 2019 Authors Alexandre CASTELLANE, Fabrice MOYEN, Bruno MESNET, OpenCAPI Enablement","title":"Application Note : OpenCAPI Quickstart Installing an OC-AD9V3 on AC922"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#foreword","text":"We will use SNAP word from time to time when mentionning the framework used with the previous versions of CAPI1.0 & CAPI2.0 technologies. Since OpenCAPI (sometimes called CAPI3.0) is the 3rd generation of CAPI, some former SNAP names can be found. Most of them have been changed though (eg snap_maint became oc_maint ).","title":"Foreword"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#ac922-hardware-setup","text":"Unlike CAPI2.0 OpenCAPI (OC) doesn\u2019t use PCI links, however, the card requires PCIe power supply and mechanical socket to work. An OC card can thus be theoretically installed in any PCIe socket. As the OCcard can also be used in CAPI2 mode, Figure 1 indicates the slots where is can be placed for CAPI2/OpenCAPI dual usage. Figure 1. Rear view of a system with PCIe CAPI enabled slots indicated As a reminder and since both CAPI2.0 and OC cards can be used simultaneously, we provide the CAPI2.0 comptaible slots view: Slot identification and (location code) Description Coherent Accelerator Processor Interface (CAPI) Adapter size Processor module Coherent Accelerator Processor Interface (CAPI) 1 (P1-C5) Half-height, half-length Processor Module 1 / CPU1 No PCIe4 x4 Half-height, half-length Processor Module 1 / CPU1 No 2 (P1-C4) PCIe4 x8 Half-height, half-length Processor Module 2 / CPU0 (shared) Yes 3 (P1-C3) PCIe4 x16 Half-height, half-length Processor Module 2 / CPU0 Yes 4 (P1-C2) PCIe4 x16 Half-height, half-length Processor Module 1 / CPU1 Yes Table 1. PCIe slot locations and CAPI2.0 compatibility in AC922 Please check details at: https://www.ibm.com/support/knowledgecenter/en/POWER9/p9eik/p9eik_pcie_slot_details.htm The FPGA cards can be put in any of the PCIe slots 2, 3 or 4 which are CAPI enabled PCIe slots if you intend to use them in CAPI2.0 mode. Slot 1 can also be used if only OpenCAPI mode is desired depending on the OpenCAPI connector position (for example 9V3 cannot be placed in slot1) Any half-height, half-length cards can be used. AD9V3 with the fan option may add mechanical restrictions in the choice of the slots. Important : The slot 2 will only allow PCIe4 x8 configured cards. Even if the connector is a PCIe x16 connector, a PCIe3x16 board configured as PCIe3 x16 like AD9V3 is configured in SNAP per default cannot be put in this slot 2. Important : If you expect to use an AD9V3 card in OpenCAPI3.0 mode, you will need to put it in slot 2 or 3 due to the length of the OpenCAPI cables and the OpenCAPI connector location specific to this card.","title":"AC922 Hardware Setup"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#oc-adapter-card","text":"The OC card uses the same physical link as a GPU. An adaptation card is required to get the links hooked up to the card with a specific high speed connector. The OC links cannot be used (firmware feature) with NVLINK, so using a OC card prevent using a GPU on the same socket. If you configure in this configuration you\u2019ll get the following message at the BMC : [ 104.351386662,3] NPU: NVLink and OpenCAPI devices on same chip not supported, aborting NPU init Make sure you place 2 GPUs on one socket and one adapter on the other one, whichever you want.","title":"OC adapter card"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#ad9v3-board-setup","text":"The ADM-PCIE-9V3 has 2 quad DIP switch SW1 and SW2, located on the rear side of the board. Check that both switches are configured to default setting. SW2 all set to OFF works ok since we don\u2019t use these VPD data in SNAP/OC. Information come from Alphadata web site: [ https://www.alpha-data.com/pdfs/adm-pcie-9v3.pdf","title":"AD9V3 board Setup"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#setup-tools-on-the-power-server-environment","text":"Setup the followings to get your environment on the Power server 1) Clone the oc-accel framework (no contribution mode) git clone https://github.com/OpenCAPI/oc-accel.git Clone the oc-accel framework (contribution mode using ssh) git clone git@github.com:OpenCAPI/oc-accel.git 2) Install the libocxl libraries + reboot the server after installation. (for ubuntu) sudo apt-get install libocxl-dev (for RHEL) sudo yum install libocxl-devel (libcxl-devel package is provided by RedHat Optional repository) 3) Clone the FPGA Image loader sudo git clone https://github.ibm.com/OC-Enablement/oc-utils/ cd oc_utils sudo make install","title":"Setup tools on the POWER server environment"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#privileges","text":"Important: libocxl requires root privileges to allow card exchanges (like reset, oc_maint, usage in general). When using the card without sudo privileges, you get an normal error. Your administrator can provide user privileges using this process: Permanently create a /etc/udev/rules.d/20-ocaccel.rules file including: SUBSYSTEM==\"ocxl\", DEVPATH==\"*/ocxl/IBM,oc-snap*\", MODE=\"666\", RUN=\"/bin/chmod 666 %S/%p/global_mmio_area\" Reboot","title":"Privileges"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#check-that-the-cards-are-recognized-as-accelerators","text":"Check that the cards are recognized by the Firmware and the OS lspci|grep accel 0006:00:00.0 Processing accelerators: IBM Device 062b 0006:00:00.1 Processing accelerators: IBM Device 062b In this example, for this card, physical port is 0 and virtual port is 6. If no card is found with this command, then your 9V3 card may not have a OpenCAPI image in it, or your firmware is too old. Here is an example of 2 cards used in the server one being recognized as CAPI2 the other as OC : lspci|grep acc 0006:00:00.0 Processing accelerators: IBM Device 062b #OC 0006:00:00.1 Processing accelerators: IBM Device 062b 0008:00:00.0 Processing accelerators: IBM Device 0632 (rev 01) #CAPI2 0030:01:00.0 Processing accelerators: IBM Device 0477 (rev 02)","title":"Check that the cards are recognized as accelerators"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#first-programming-of-a-brand-new-9v3-card-no-capi-image-was-ever-installed","text":"There are several ways to program a 9V3 card. If no CAPI image was ever installed on this board, then you\u2019ll need to follow these following instructions to set it once. Then you\u2019ll be able to go faster using the next paragraph. A basic test image for the 9V3 card\u2019s FPGA can be obtained from Box. Programming this image onto the card will require Xilinx Vivado version 2018.1 or newerand a Xilinx Platform Cable USB II ribbon programmer. To do so, access the device from Vivado\u2019s hardware manager and follow these steps: Right click on the \"xcvu3p_0\" entry in the device list and select \"add configuration memory device\". Select \"mt25qu256-spi-x1_x2_x4_x8\" from the list of config mem devices. When prompted, choose to program the config mem device. In the subsequent menu, select the two .mcs files, followed by the two .prm files. The programming should complete in a few minutes. Note that the card needs to be power cycled (i.e. by rebooting the system) before the new image will take effect.In very rare cases, a second reboot may be needed.","title":"First programming of a brand new 9V3 card (no CAPI image was ever installed)"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#standard-programming-of-a-9v3-card-capi-image-already-in-it","text":"Images for the 9V3 card\u2019s FPGA will be created from the OC-ACCEL environment and will be placed in ~snap/hardware/build/Images Once successfully synthesized. Then using the FPGA Image loader, you will program and reset the FPGA with your binary files using the following command: sudo oc-flash-script my_user_image.bin Depending on the size of the FPGA board Flash devices, you may need 1or 2 binary files. AD9V3 card needs 2 binary files noted as primary and secondary. You will so call the loader with the 2 files in the following order: sudo oc-flash-script my_user_image_primary.bin my_user_image_secondary.bin","title":"Standard programming of a 9V3 card (CAPI image already in it)"},{"location":"apps_notes/Installing_an_OC-AD9V3_on_AC922/#running-the-test-image-if-card-has-been-already-programmed-with-a-oc-image","text":"From the snap directory, compile once the software and applications cd oc-accel git pull # in case you already cloned earlier and want to stay up to date make software apps To \"locally make\" your software you can set the default ACTION_ROOT variable. The default test which has been compiled in the Box\u2019s image is a hls_memcopy example. export ACTION_ROOT=${HOME}/oc-accel/actions/hls_memcopy_1024 then set all paths automatically sourcing the script as follow: source snap_path.sh # Find available card oc_find_card -v -A ALL #This should return the card position OC-AD9V3 card has been detected in card position: 0 ... then let OC-ACCEL framework discover the cards and actions with the oc_maint command. Mention the card slot number if different from 0 using -C option. This command needs to be run almost once after a reset and will return the action stated here as \u201cIBM HLS Memcopy\u201d oc_maint -v [-C1] You can now: Either run the memcopy program doing any transfer you want. You will get the explanations on how to use this exampleby typing snap_memcopy or run automatic test of hls_memcopy_1024 using ./actions/hls_memcopy_1024/tests/hw_throughput.sh -d INCR which will give you the bandwidth measured between FPGA, host memory and on-board for different file size exchanged.","title":"Running the Test Image if card has been already programmed with a OC image"},{"location":"deep-dive/board-package/","text":"Enable a new FPGA card to OC-Accel Create a folder in hardware/oc-bip/board_support_packages/<NEW_CARD> Call the make process under hardware/oc-bip/<NEW_CARD> Add the CARD_TYPE, and other specific IPs in OC-Accel hardware/setup Modify the related CARD_TYPE related information in software Add the card choice in Kconfig Menu scripts/Kconfig TODO: More details to be put down.","title":"New Board Support"},{"location":"deep-dive/board-package/#enable-a-new-fpga-card-to-oc-accel","text":"Create a folder in hardware/oc-bip/board_support_packages/<NEW_CARD> Call the make process under hardware/oc-bip/<NEW_CARD> Add the CARD_TYPE, and other specific IPs in OC-Accel hardware/setup Modify the related CARD_TYPE related information in software Add the card choice in Kconfig Menu scripts/Kconfig TODO: More details to be put down.","title":"Enable a new FPGA card to OC-Accel"},{"location":"deep-dive/hardware-logic/","text":"OC-Accel Hardware Diagram and Clock Domain Here is a more detailed diagram for OC-Accel Bridge mode. oc_bsp_wrap and oc_cfg come from oc-bip ( OpenCAPI3.0 Device Reference design ) oc_function comes from OC-Accel hardware/hdl/core . oc_bsp_wrap has DLX, PHY, TLX, VPD (Virtual Product Data) and flash_sub_system. oc_cfg handles the registers defined in OpenCAPI CFG specification. Under oc_function, framework_afu can be replaced by other reference AFUs like AFP3 or MCP3 in OpenCAPI3.0 Device Reference design . framework_afu contains its cfg_descriptor to describe the basic settings for this AFU. It has following clock domains: clk_tlx : 400MHz (Don't change) clk_afu : 200MHz (Adjustable) clk_act : 200MHz (Adjustable) Other : Depending on the requirements of peripheral IPs The blocks with transitional color means it has the clock converter logic or asynchronous clock interfaces. A partial reconfiguration region is defined as shown in the dotted box. This area is dynamic area that allows user to change their functional logic and all of the other regions are static in PR (Partial Reconfiguration) Flow. AXI4 feature list Here lists the AXI4 feature list of oc_snap_core: AXI Lite-M Signal Width ADDR 32bits DATA 32bits AXI-S Signal Bit Width Comment ADDR (AWADDR/ARADDR) 64 Unaligned address supported DATA (WDATA/RDATA) 1024 ID (AWID/ARID/BID/RID) 1 to 5 2 to 32 AXI IDs USER (AWUSER/ARUSER) 9 Support up to 512 PASID (multi-process contexts) SIZE (AWSIZE/ARSIZE) 3 All sized transactions from 1 byte to 128bytes are supported BURST (AWBURST/ARBURST) 2 INCR RESP (BRESP/RRESP) 2 OKAY or ERROR WSTRB 128 All patterns are supported Note Choose less AXI IDs can save the area of snap_core. The AXI ID ports have at least 1 bit. Drive zero if the Action hardware design doesn't use it. AXI signals cache , lock , qos , region are not supported Warning Burst type \"FIXED\" is coded in snap_core but hasn't been tested. TLx feature list TLX to AFU Commands They are connected to mmio_wrapper. pr_mem_read (4B, 8B) pr_wr_mem (4B, 8B) intrp_rdy AFU to TLX Commands They are connected to bridge_wrapper. assign_actag rd_wnitc (128B, 64B) dma_w (128B, 64B) dma_pr_w (1B to 32B) rd_pr_wnitc (1B to 32B) intrp_req","title":"Hardware Logic"},{"location":"deep-dive/hardware-logic/#oc-accel-hardware","text":"","title":"OC-Accel Hardware"},{"location":"deep-dive/hardware-logic/#diagram-and-clock-domain","text":"Here is a more detailed diagram for OC-Accel Bridge mode. oc_bsp_wrap and oc_cfg come from oc-bip ( OpenCAPI3.0 Device Reference design ) oc_function comes from OC-Accel hardware/hdl/core . oc_bsp_wrap has DLX, PHY, TLX, VPD (Virtual Product Data) and flash_sub_system. oc_cfg handles the registers defined in OpenCAPI CFG specification. Under oc_function, framework_afu can be replaced by other reference AFUs like AFP3 or MCP3 in OpenCAPI3.0 Device Reference design . framework_afu contains its cfg_descriptor to describe the basic settings for this AFU. It has following clock domains: clk_tlx : 400MHz (Don't change) clk_afu : 200MHz (Adjustable) clk_act : 200MHz (Adjustable) Other : Depending on the requirements of peripheral IPs The blocks with transitional color means it has the clock converter logic or asynchronous clock interfaces. A partial reconfiguration region is defined as shown in the dotted box. This area is dynamic area that allows user to change their functional logic and all of the other regions are static in PR (Partial Reconfiguration) Flow.","title":"Diagram and Clock Domain"},{"location":"deep-dive/hardware-logic/#axi4-feature-list","text":"Here lists the AXI4 feature list of oc_snap_core:","title":"AXI4 feature list"},{"location":"deep-dive/hardware-logic/#axi-lite-m","text":"Signal Width ADDR 32bits DATA 32bits","title":"AXI Lite-M"},{"location":"deep-dive/hardware-logic/#axi-s","text":"Signal Bit Width Comment ADDR (AWADDR/ARADDR) 64 Unaligned address supported DATA (WDATA/RDATA) 1024 ID (AWID/ARID/BID/RID) 1 to 5 2 to 32 AXI IDs USER (AWUSER/ARUSER) 9 Support up to 512 PASID (multi-process contexts) SIZE (AWSIZE/ARSIZE) 3 All sized transactions from 1 byte to 128bytes are supported BURST (AWBURST/ARBURST) 2 INCR RESP (BRESP/RRESP) 2 OKAY or ERROR WSTRB 128 All patterns are supported Note Choose less AXI IDs can save the area of snap_core. The AXI ID ports have at least 1 bit. Drive zero if the Action hardware design doesn't use it. AXI signals cache , lock , qos , region are not supported Warning Burst type \"FIXED\" is coded in snap_core but hasn't been tested.","title":"AXI-S"},{"location":"deep-dive/hardware-logic/#tlx-feature-list","text":"","title":"TLx feature list"},{"location":"deep-dive/hardware-logic/#tlx-to-afu-commands","text":"They are connected to mmio_wrapper. pr_mem_read (4B, 8B) pr_wr_mem (4B, 8B) intrp_rdy","title":"TLX to AFU Commands"},{"location":"deep-dive/hardware-logic/#afu-to-tlx-commands","text":"They are connected to bridge_wrapper. assign_actag rd_wnitc (128B, 64B) dma_w (128B, 64B) dma_pr_w (1B to 32B) rd_pr_wnitc (1B to 32B) intrp_req","title":"AFU to TLX Commands"},{"location":"deep-dive/registers/","text":"OC-Accel Registers Configuration Registers link the software and hardware together. User application software code can use the provided libosnap APIs to read and write the registers implemented in FPGA logic, thus configure and control the functions in hardware. Those registers are also called MMIO (memory mapped IO) registers, because they are mapped into a large memory map. The \"addresses\" of these registers are 64bits wide. Memory map The OpenCAPI3.0 device memory map concepts (BAR, MMIO Global, MMIO Per PASID, and also memory space) are specified with respect to OpenCAPI configuration space specification . Here is a conceptual memory map: For OC-Accel: It only supports 1 AFU It only supports OpenCAPI3.0 C1 mode. MEM_SIZE = 0 It supports 512 PASIDs (User Process ID associated with a request) It only uses BAR0 Global MMIO Offset = 0, Size = 2GB Per PASID MMIO Offset = 2GB, Stride Size = 4MB So the above memory map is specified to: The settings can be found in: hardware/oc-bip/config_subsystem/cfg_descriptor.v Address Layout OC-Accel registers have two categories: Global Registers , 8B, defined in Global MMIO space. Use snap_global_read/write64() to access them. Action Registers , 4B, defined in Per PASID MMIO space. Use snap_action_read/write32() to access them. The higher 32bits of tlx_afu_cmd_pa (Physical Address) should be matched with BAR0. The lower 32bits, also called mmio_address, is processed in OC-Accel. Global Registers: Summary mmio_address[30:8] mmio_address [7:0] Abbr. Register Name 0x0 (Basic) 0x00 IVR Implementation Version Register 0x08 BDR Build Date Register 0x10 SCR SNAP Command Register 0x18 SSR SNAP Status Register 0x30 CAP Capacity Register 0x1A0 (Debug) 0x00 DBG_CLR Clear Debug Register 0x08 CNT_TLX_CMD Number of TLX Commands 0x10 CNT_TLX_RSP Number of TLX Responses 0x18 CNT_TLX_RTY Number of TLX Retry Responses 0x20 CNT_TLX_FAIL Number of TLX Fail Responses 0x28 CNT_TLX_XLP Number of TLX Translate Pending Responses 0x30 CNT_TLX_XLD Number of TLX Translate Done Responses 0x38 CNT_TLX_XLR Number of TLX Translate Retry Responses 0x40 CNT_AXI_CMD Number of total AXI Commands 0x48 CNT_AXI_RSP Number of total AXI Responses 0x50 BUF_CNT Counts in data buffers 0x58 TRAFIIC_IDLE No traffic over a period 0x60 TLX_IDLE_LIM Length of the period for TLX \"no traffic\" 0x68 AXI_IDLE_LIM Length of the period for AXI \"no traffic\" 0x1C0 (FIR) 0x00 FIFO_OVFL FIFO Overflow Status 0x08 FIR_TLX Errors on TLX interface Note FIR means \"Fault Isolation Register\". It usually means some errors happened. Global Registers: Details SNAP Basic Registers Implementation Version Register (IVR) Offset: 0x00 POR value depends on source for the build. Example for build based on commit with SHA ID eb43f4d80334d6a127af150345fed12dc5f45b7c and with distance 13 to SNAP Release v1.25.4: 0x0119040D_EB43F4D8 Bits Attributes Description 63..40 RO SNAP Release 63..56 RO Major release number 55..48 RO Intermediate release number 47..40 RO Minor release number 39..32 RO Distance of commit to SNAP release 31..0 RO First eight digits of SHA ID for commit Build Date Register (BDR) Offset: 0x08 POR value depends on build date and time. Example for build on January 12th, 2017 at 15:27: 0x00002017_01121527 Bits Attributes Description 63..48 RO Reserved 47.. 0 RO BCD coded build date and time 47..32 RO YYYY (year) 31..24 RO mm (month) 23..16 RO dd (day of month) 15..08 RO HH (hour) 07..00 RO MM (minute) SNAP Command Register (SCR) Offset: 0x10 Send SNAP commands via this register Bits Attributes Description 63..1 RO Reserved 0 WO soft reset to odma and action_wrapper SNAP Status Register (SSR) Offset: 0x18 Status of snap_core Bits Attributes Description 63..4 RO Reserved 3 RO SNAP fatal error: some bits are asserted in FIR registers 2 RO SNAP AXI side busy (?) 1 RO SNAP TLX side busy (?) 0 RO SNAP idle: Data buffers in snap_core are empty SNAP Capability Register (CAP) Offset: 0x30 Define the capability of the card Bitwise definition Bits Attributes Description 63..32 RO Reserved 31..16 RO Size of attached on-card SDRAM in MB 15..8 RO Reserved 7..0 RO Card type: 0x31 : AD9V3 0x32 : AD9H7 SNAP Debug Registers base_addr: 0x1A0 Note Subject to change. Debug Clear and Debug Counters DBG_CLR: Clear all of the following debug registers For following registers: bit[63:32] for Reads, bit [31:0] for Writes. CNT_TLX_CMD: Number of TLX Commands CNT_TLX_RSP: Number of TLX Responses CNT_TLX_RTY: Number of TLX Retry Responses CNT_TLX_FAIL: Number of TLX Fail Responses CNT_TLX_XLP: Number of TLX Translate Pending Responses CNT_TLX_XLD: Number of TLX Translate Done Responses CNT_TLX_XLR: Number of TLX Translate Retry Responses CNT_AXI_CMD: Number of total AXI Commands CNT_AXI_RSP: Number of total AXI Responses BUF_CNT: How many entries are valid for Read buffer and Write buffer Traffic Idle status TRAFFIC_IDLE: Used together with TLX_IDLE_LIM and AXI_IDLE_LIM. Bits Attributes Description 63..6 RO Reserved 5 RO tlx_cmd_idle in a certain number of cycles 4 RO tlx_rsp_idle in a certain number of cycles 3 RO axi_cmd_idle (Read) in a certain number of cycles 2 RO axi_rsp_idle (Read) in a certain number of cycles 1 RO axi_cmd_idle (Write) in a certain number of cycles 0 RO axi_rsp_idle (Write) in a certain number of cycles SNAP FIR Registers base_addr: 0x1C0 FIFO Overflow Status (FIFO_OVFL) offset: 0x00 Bits Attributes Description 63..6 RO Reserved 7 RO fir_fifo_overflow_cmdencw (Write Command Encoder) 6 RO fir_fifo_overflow_cmdencr (Read Command Encoder) 5 RO fir_fifo_overflow_cmdcnv (Command Clock Converter) 4 RO fir_fifo_overflow_rspcnv (Response Clock Converter) 3 RO fir_fifo_overflow_rspdecw (Write Response Decoder) 2 RO fir_fifo_overflow_rspdecr (Read Response Decoder) 1 RO fir_fifo_overflow_dbw (Write Data Buffer) 0 RO fir_fifo_overflow_dbr (Read Data Buffer) TLX Interface errors (FIR_TLX) offset: 0x08 Bits Attributes Description 63..3 RO Reserved 3 RO fir_tlx_response_unsupport 2 RO fir_tlx_rsp_err 1:0 RO fir_tlx_command_credit Action Registers: Summary mmio_address[30:22] means PASID. That means, the first process opens the OC Device, it will attach PASID=0 when it calls mmio_action_read/write32() . Meanwhile, the second process, the third process may attach PASID=1 and PASID=2 when they access the OC Device. Each process has its own \"process context\", and when OC Device wants to visit the host memory, it has to know which \"process context\" it belongs to, that means, the OC device needs to send the PASID with its commands. This PASID takes AWUSER or ARUSER as the vehicle to transfer from Action wrapper to snap_core. HDL design The user can freely define and implement the Action registers if it is written in Verilog/VHDL. However, there are still a registers that are recommended to implement. They are: mmio_address [21:0] Abbr. Register Name 0x00 ACR Action Control Register 0x04 IER Interrupt Enable Register 0x10 ATR Action Type Register 0x14 AVR Action Version Register 0x18 ISL Interrupt Handle SRC Address Low 0x1C ISH Interrupt Handle SRC Address High 0x30 -> end Reserved and can be freely used Action Control Register (ACR) offset: 0x00 Bits Attributes Description 31..4 RO Reserved 3 RO Action Ready 2 RO Action Idle 1 RO Action Done 0 RW Write 1 to start Action, Read it to know whether the action has been started Interrupt Enable Register (IER) offset: 0x04 Bits Attributes Description 31..1 RO Reserved 0 RW Enable Interrupt Action Type Register (ATR) offset: 0x10 Bits Attributes Description 31..0 RO Action Type, i.e, 0x10141008 Action Version Register (AVR) offset: 0x14 Bits Attributes Description 31..0 RO Action Release Version, user defined Interrupt Handle SRC Address Low (ISL) offset: 0x18 Bits Attributes Description 31..0 RO Interrupt Handle Source Address Low 32bits Interrupt Handle SRC Address High (ISH) offset: 0x1C Bits Attributes Description 31..0 RO Interrupt Handle Source Address High 32bits HLS design OC-Accel has already defined the Action Register Layout for HLS Actions. mmio_address [21:0] Abbr. Register Name 0x00 ACR Action Control Register 0x04 IER Global Interrupt Enable Register 0x08 IIE IP Interrupt Enable 0x0C IIS IP Interrupt Status 0x10 ATR Action Type Register 0x14 AVR Action Release Register 0x18 ISL Interrupt Handle SRC Address Low 0x1C ISH Interrupt Handle SRC Address High 0x100 CONTROL1 sat + flags + seq 0x104 CONTROL2 Return Code 0x108 CONTROL3 Reserved 0x10C CONTROL4 Reserved 0x110 - 0x178 Job Data Registers (108 bytes) Note 0x00 to 0x0C are defined by Xilinx Document UG902. Action Control Register (ACR) offset: 0x00 Bits Attributes Description 31..8 RO Reserved 7 RW Auto Restart 6..4 RO Reserved 3 RO Action Ready 2 RO Action Idle 1 RO/Clear on Read Action Done 0 RW/Clear on Read Start Action Global Interrupt Enable (IER) offset: 0x04 Bits Attributes Description 31..1 RO Reserved 0 RW Enable Interrupt IP Interrupt Enable (IIE) offset: 0x08 Bits Attributes Description 31..2 RO Reserved 1 RW Interrupt for ap_ready is enabled 0 RW Interrupt for ap_done is enabled IP Interrupt Status (IIS) offset: 0x0C Bits Attributes Description 31..2 RO Reserved 1 RW Status for ap_ready interrupt 0 RW Status for ap_done interrupt ATR, AVR, ISL, ISH These 4 registers have the same definitions as in HDL Action. offset: 0x10, ATR, Action Type Register offset: 0x14, AVR, Action Release Register offset: 0x18, ISL, Interrupt Handle SRC Address Low offset: 0x1C, ISH, Interrupt Handle SRC Address High HLS CONTROL Registers 4 registers are defined in actions/include/hls_snap.H . They take the addresses of 0x100, 0x104, 0x108 and 0x10C. typedef struct { snapu8_t sat; // short action type snapu8_t flags; snapu16_t seq; snapu32_t Retc; snapu64_t Reserved; // Priv_data } CONTROL; HLS Job DATA Registers 0x110 to 0x178 are user defined Job data structure. The size limit is 108 bytes. This is usually defined in actions/hls_<action_name>/include/<action_name>.h For example: typedef struct helloworld_job { struct snap_addr in; /* input data */ struct snap_addr out; /* offset table */ } helloworld_job_t;","title":"Registers"},{"location":"deep-dive/registers/#oc-accel-registers","text":"Configuration Registers link the software and hardware together. User application software code can use the provided libosnap APIs to read and write the registers implemented in FPGA logic, thus configure and control the functions in hardware. Those registers are also called MMIO (memory mapped IO) registers, because they are mapped into a large memory map. The \"addresses\" of these registers are 64bits wide.","title":"OC-Accel Registers"},{"location":"deep-dive/registers/#memory-map","text":"The OpenCAPI3.0 device memory map concepts (BAR, MMIO Global, MMIO Per PASID, and also memory space) are specified with respect to OpenCAPI configuration space specification . Here is a conceptual memory map: For OC-Accel: It only supports 1 AFU It only supports OpenCAPI3.0 C1 mode. MEM_SIZE = 0 It supports 512 PASIDs (User Process ID associated with a request) It only uses BAR0 Global MMIO Offset = 0, Size = 2GB Per PASID MMIO Offset = 2GB, Stride Size = 4MB So the above memory map is specified to: The settings can be found in: hardware/oc-bip/config_subsystem/cfg_descriptor.v","title":"Memory map"},{"location":"deep-dive/registers/#address-layout","text":"OC-Accel registers have two categories: Global Registers , 8B, defined in Global MMIO space. Use snap_global_read/write64() to access them. Action Registers , 4B, defined in Per PASID MMIO space. Use snap_action_read/write32() to access them. The higher 32bits of tlx_afu_cmd_pa (Physical Address) should be matched with BAR0. The lower 32bits, also called mmio_address, is processed in OC-Accel.","title":"Address Layout"},{"location":"deep-dive/registers/#global-registers-summary","text":"mmio_address[30:8] mmio_address [7:0] Abbr. Register Name 0x0 (Basic) 0x00 IVR Implementation Version Register 0x08 BDR Build Date Register 0x10 SCR SNAP Command Register 0x18 SSR SNAP Status Register 0x30 CAP Capacity Register 0x1A0 (Debug) 0x00 DBG_CLR Clear Debug Register 0x08 CNT_TLX_CMD Number of TLX Commands 0x10 CNT_TLX_RSP Number of TLX Responses 0x18 CNT_TLX_RTY Number of TLX Retry Responses 0x20 CNT_TLX_FAIL Number of TLX Fail Responses 0x28 CNT_TLX_XLP Number of TLX Translate Pending Responses 0x30 CNT_TLX_XLD Number of TLX Translate Done Responses 0x38 CNT_TLX_XLR Number of TLX Translate Retry Responses 0x40 CNT_AXI_CMD Number of total AXI Commands 0x48 CNT_AXI_RSP Number of total AXI Responses 0x50 BUF_CNT Counts in data buffers 0x58 TRAFIIC_IDLE No traffic over a period 0x60 TLX_IDLE_LIM Length of the period for TLX \"no traffic\" 0x68 AXI_IDLE_LIM Length of the period for AXI \"no traffic\" 0x1C0 (FIR) 0x00 FIFO_OVFL FIFO Overflow Status 0x08 FIR_TLX Errors on TLX interface Note FIR means \"Fault Isolation Register\". It usually means some errors happened.","title":"Global Registers: Summary"},{"location":"deep-dive/registers/#global-registers-details","text":"","title":"Global Registers: Details"},{"location":"deep-dive/registers/#snap-basic-registers","text":"","title":"SNAP Basic Registers"},{"location":"deep-dive/registers/#implementation-version-register-ivr","text":"Offset: 0x00 POR value depends on source for the build. Example for build based on commit with SHA ID eb43f4d80334d6a127af150345fed12dc5f45b7c and with distance 13 to SNAP Release v1.25.4: 0x0119040D_EB43F4D8 Bits Attributes Description 63..40 RO SNAP Release 63..56 RO Major release number 55..48 RO Intermediate release number 47..40 RO Minor release number 39..32 RO Distance of commit to SNAP release 31..0 RO First eight digits of SHA ID for commit","title":"Implementation Version Register (IVR)"},{"location":"deep-dive/registers/#build-date-register-bdr","text":"Offset: 0x08 POR value depends on build date and time. Example for build on January 12th, 2017 at 15:27: 0x00002017_01121527 Bits Attributes Description 63..48 RO Reserved 47.. 0 RO BCD coded build date and time 47..32 RO YYYY (year) 31..24 RO mm (month) 23..16 RO dd (day of month) 15..08 RO HH (hour) 07..00 RO MM (minute)","title":"Build Date Register (BDR)"},{"location":"deep-dive/registers/#snap-command-register-scr","text":"Offset: 0x10 Send SNAP commands via this register Bits Attributes Description 63..1 RO Reserved 0 WO soft reset to odma and action_wrapper","title":"SNAP Command Register (SCR)"},{"location":"deep-dive/registers/#snap-status-register-ssr","text":"Offset: 0x18 Status of snap_core Bits Attributes Description 63..4 RO Reserved 3 RO SNAP fatal error: some bits are asserted in FIR registers 2 RO SNAP AXI side busy (?) 1 RO SNAP TLX side busy (?) 0 RO SNAP idle: Data buffers in snap_core are empty","title":"SNAP Status Register (SSR)"},{"location":"deep-dive/registers/#snap-capability-register-cap","text":"Offset: 0x30 Define the capability of the card Bitwise definition Bits Attributes Description 63..32 RO Reserved 31..16 RO Size of attached on-card SDRAM in MB 15..8 RO Reserved 7..0 RO Card type: 0x31 : AD9V3 0x32 : AD9H7","title":"SNAP Capability Register (CAP)"},{"location":"deep-dive/registers/#snap-debug-registers","text":"base_addr: 0x1A0 Note Subject to change.","title":"SNAP Debug Registers"},{"location":"deep-dive/registers/#debug-clear-and-debug-counters","text":"DBG_CLR: Clear all of the following debug registers For following registers: bit[63:32] for Reads, bit [31:0] for Writes. CNT_TLX_CMD: Number of TLX Commands CNT_TLX_RSP: Number of TLX Responses CNT_TLX_RTY: Number of TLX Retry Responses CNT_TLX_FAIL: Number of TLX Fail Responses CNT_TLX_XLP: Number of TLX Translate Pending Responses CNT_TLX_XLD: Number of TLX Translate Done Responses CNT_TLX_XLR: Number of TLX Translate Retry Responses CNT_AXI_CMD: Number of total AXI Commands CNT_AXI_RSP: Number of total AXI Responses BUF_CNT: How many entries are valid for Read buffer and Write buffer","title":"Debug Clear and Debug Counters"},{"location":"deep-dive/registers/#traffic-idle-status","text":"TRAFFIC_IDLE: Used together with TLX_IDLE_LIM and AXI_IDLE_LIM. Bits Attributes Description 63..6 RO Reserved 5 RO tlx_cmd_idle in a certain number of cycles 4 RO tlx_rsp_idle in a certain number of cycles 3 RO axi_cmd_idle (Read) in a certain number of cycles 2 RO axi_rsp_idle (Read) in a certain number of cycles 1 RO axi_cmd_idle (Write) in a certain number of cycles 0 RO axi_rsp_idle (Write) in a certain number of cycles","title":"Traffic Idle status"},{"location":"deep-dive/registers/#snap-fir-registers","text":"base_addr: 0x1C0","title":"SNAP FIR Registers"},{"location":"deep-dive/registers/#fifo-overflow-status-fifo_ovfl","text":"offset: 0x00 Bits Attributes Description 63..6 RO Reserved 7 RO fir_fifo_overflow_cmdencw (Write Command Encoder) 6 RO fir_fifo_overflow_cmdencr (Read Command Encoder) 5 RO fir_fifo_overflow_cmdcnv (Command Clock Converter) 4 RO fir_fifo_overflow_rspcnv (Response Clock Converter) 3 RO fir_fifo_overflow_rspdecw (Write Response Decoder) 2 RO fir_fifo_overflow_rspdecr (Read Response Decoder) 1 RO fir_fifo_overflow_dbw (Write Data Buffer) 0 RO fir_fifo_overflow_dbr (Read Data Buffer)","title":"FIFO Overflow Status (FIFO_OVFL)"},{"location":"deep-dive/registers/#tlx-interface-errors-fir_tlx","text":"offset: 0x08 Bits Attributes Description 63..3 RO Reserved 3 RO fir_tlx_response_unsupport 2 RO fir_tlx_rsp_err 1:0 RO fir_tlx_command_credit","title":"TLX Interface errors (FIR_TLX)"},{"location":"deep-dive/registers/#action-registers-summary","text":"mmio_address[30:22] means PASID. That means, the first process opens the OC Device, it will attach PASID=0 when it calls mmio_action_read/write32() . Meanwhile, the second process, the third process may attach PASID=1 and PASID=2 when they access the OC Device. Each process has its own \"process context\", and when OC Device wants to visit the host memory, it has to know which \"process context\" it belongs to, that means, the OC device needs to send the PASID with its commands. This PASID takes AWUSER or ARUSER as the vehicle to transfer from Action wrapper to snap_core.","title":"Action Registers: Summary"},{"location":"deep-dive/registers/#hdl-design","text":"The user can freely define and implement the Action registers if it is written in Verilog/VHDL. However, there are still a registers that are recommended to implement. They are: mmio_address [21:0] Abbr. Register Name 0x00 ACR Action Control Register 0x04 IER Interrupt Enable Register 0x10 ATR Action Type Register 0x14 AVR Action Version Register 0x18 ISL Interrupt Handle SRC Address Low 0x1C ISH Interrupt Handle SRC Address High 0x30 -> end Reserved and can be freely used","title":"HDL design"},{"location":"deep-dive/registers/#action-control-register-acr","text":"offset: 0x00 Bits Attributes Description 31..4 RO Reserved 3 RO Action Ready 2 RO Action Idle 1 RO Action Done 0 RW Write 1 to start Action, Read it to know whether the action has been started","title":"Action Control Register (ACR)"},{"location":"deep-dive/registers/#interrupt-enable-register-ier","text":"offset: 0x04 Bits Attributes Description 31..1 RO Reserved 0 RW Enable Interrupt","title":"Interrupt Enable Register (IER)"},{"location":"deep-dive/registers/#action-type-register-atr","text":"offset: 0x10 Bits Attributes Description 31..0 RO Action Type, i.e, 0x10141008","title":"Action Type Register (ATR)"},{"location":"deep-dive/registers/#action-version-register-avr","text":"offset: 0x14 Bits Attributes Description 31..0 RO Action Release Version, user defined","title":"Action Version Register (AVR)"},{"location":"deep-dive/registers/#interrupt-handle-src-address-low-isl","text":"offset: 0x18 Bits Attributes Description 31..0 RO Interrupt Handle Source Address Low 32bits","title":"Interrupt Handle SRC Address Low (ISL)"},{"location":"deep-dive/registers/#interrupt-handle-src-address-high-ish","text":"offset: 0x1C Bits Attributes Description 31..0 RO Interrupt Handle Source Address High 32bits","title":"Interrupt Handle SRC Address High (ISH)"},{"location":"deep-dive/registers/#hls-design","text":"OC-Accel has already defined the Action Register Layout for HLS Actions. mmio_address [21:0] Abbr. Register Name 0x00 ACR Action Control Register 0x04 IER Global Interrupt Enable Register 0x08 IIE IP Interrupt Enable 0x0C IIS IP Interrupt Status 0x10 ATR Action Type Register 0x14 AVR Action Release Register 0x18 ISL Interrupt Handle SRC Address Low 0x1C ISH Interrupt Handle SRC Address High 0x100 CONTROL1 sat + flags + seq 0x104 CONTROL2 Return Code 0x108 CONTROL3 Reserved 0x10C CONTROL4 Reserved 0x110 - 0x178 Job Data Registers (108 bytes) Note 0x00 to 0x0C are defined by Xilinx Document UG902.","title":"HLS design"},{"location":"deep-dive/registers/#action-control-register-acr_1","text":"offset: 0x00 Bits Attributes Description 31..8 RO Reserved 7 RW Auto Restart 6..4 RO Reserved 3 RO Action Ready 2 RO Action Idle 1 RO/Clear on Read Action Done 0 RW/Clear on Read Start Action","title":"Action Control Register (ACR)"},{"location":"deep-dive/registers/#global-interrupt-enable-ier","text":"offset: 0x04 Bits Attributes Description 31..1 RO Reserved 0 RW Enable Interrupt","title":"Global Interrupt Enable (IER)"},{"location":"deep-dive/registers/#ip-interrupt-enable-iie","text":"offset: 0x08 Bits Attributes Description 31..2 RO Reserved 1 RW Interrupt for ap_ready is enabled 0 RW Interrupt for ap_done is enabled","title":"IP Interrupt Enable (IIE)"},{"location":"deep-dive/registers/#ip-interrupt-status-iis","text":"offset: 0x0C Bits Attributes Description 31..2 RO Reserved 1 RW Status for ap_ready interrupt 0 RW Status for ap_done interrupt","title":"IP Interrupt Status (IIS)"},{"location":"deep-dive/registers/#atr-avr-isl-ish","text":"These 4 registers have the same definitions as in HDL Action. offset: 0x10, ATR, Action Type Register offset: 0x14, AVR, Action Release Register offset: 0x18, ISL, Interrupt Handle SRC Address Low offset: 0x1C, ISH, Interrupt Handle SRC Address High","title":"ATR, AVR, ISL, ISH"},{"location":"deep-dive/registers/#hls-control-registers","text":"4 registers are defined in actions/include/hls_snap.H . They take the addresses of 0x100, 0x104, 0x108 and 0x10C. typedef struct { snapu8_t sat; // short action type snapu8_t flags; snapu16_t seq; snapu32_t Retc; snapu64_t Reserved; // Priv_data } CONTROL;","title":"HLS CONTROL Registers"},{"location":"deep-dive/registers/#hls-job-data-registers","text":"0x110 to 0x178 are user defined Job data structure. The size limit is 108 bytes. This is usually defined in actions/hls_<action_name>/include/<action_name>.h For example: typedef struct helloworld_job { struct snap_addr in; /* input data */ struct snap_addr out; /* offset table */ } helloworld_job_t;","title":"HLS Job DATA Registers"},{"location":"deep-dive/software-api/","text":"OC-Accel Software Environment Variables To debug libsnap functionality or associated actions, there are currently some environment variables available: SNAP_TRACE : 0x1 General libsnap trace 0x2 Enable register read/write trace 0x4 Enable simulation specific trace 0x8 Enable action traces. For example, use SNAP_TRACE=0xF to enable all above. Applications might use more bits above those defined here. Tools snap_maint: Currently it just prints information. snap_peek: debug tools to read MMIO registers. snap_poke: debug tools to write MMIO registers. APIs Refer to: include/libosnap.h lib/osnap.c","title":"Software API"},{"location":"deep-dive/software-api/#oc-accel-software","text":"","title":"OC-Accel Software"},{"location":"deep-dive/software-api/#environment-variables","text":"To debug libsnap functionality or associated actions, there are currently some environment variables available: SNAP_TRACE : 0x1 General libsnap trace 0x2 Enable register read/write trace 0x4 Enable simulation specific trace 0x8 Enable action traces. For example, use SNAP_TRACE=0xF to enable all above. Applications might use more bits above those defined here.","title":"Environment Variables"},{"location":"deep-dive/software-api/#tools","text":"snap_maint: Currently it just prints information. snap_peek: debug tools to read MMIO registers. snap_poke: debug tools to write MMIO registers.","title":"Tools"},{"location":"deep-dive/software-api/#apis","text":"Refer to: include/libosnap.h lib/osnap.c","title":"APIs"},{"location":"misc/doc-guide/","text":"How to generate this website This static documentation website is created by MkDocs and is using a theme from bootswatch . It uses \"github pages\" and this site is hosted by Github. The documentation source files are written in Markdown format. With MkDocs tool, the generated site files (html files) are automatically pushed into a specific branch gh-pages of the git repository. Installation 1. Install python and pip python and pip 2. Install mkdocs-bootswatch pip install mkdocs-bootswatch Please refer to bootswatch for more information. 3. Install a markdown editor You can simply edit the markdown (.md) files by any text editor, but it's better to user a professional markdown editor. typora . It supports all of the platforms (Windows/MacOS/Linux). Please configure typora to strict Markdown mode. That ensures you get the same output effects on both typora and mkdocs . vscode . It's also a good editor and has abundant functions and extensions. You can install extensions of Markdown, Preview and Spell checker. 4. Install other optional tools pdf2svg: This tool can convert a pdf lossless picture to svg format. For Mac OS, it can be easily installed by Homebrew , simply by brew install pdf2svg . Alternative choice is Inkscape which is a free drawing tool and can help you draw and convert vector graphics. Website Structure First, you need to git clone the oc-accel repository and go to web-doc directory. Make sure you are working on a branch other than master. $ git clone git@github.com:OpenCAPI/oc-accel-doc.git $ cd oc-accel/web-doc Create a branch if needed: $ git branch <new-branch-name> $ git checkout <A branch other than master> The docs folder is where to put the markdown files, and the mkdocs.yml lists the website structure and global definitons. For example, this site has a structure like: nav: - Home: 'index.md' - User Guide: - 'Prepare Environment': 'user-guide/prepare-env.md' - 'Run an example': 'user-guide/run-example.md' - 'Create a new action': 'user-guide/new-action.md' - 'Co-Simulation': 'user-guide/co-simulation.md' - 'FPGA Image build': 'user-guide/make-image.md' - 'Optimize HLS action': 'user-guide/optimize-hls.md' - 'Deploy on Power Server': 'user-guide/deploy.md' - 'Debug an issue': 'user-guide/debug-issue.md' - 'Command Reference': 'user-guide/command-reference.md' - Examples: - 'hdl_example': 'actions-doc/hdl-example.md' - 'hdl_helloworld': 'actions-doc/hdl-helloworld.md' - 'hls_helloworld_1024': 'actions-doc/hls-helloworld_1024.md' - 'hls_memcopy': 'actions-doc/hls-memcopy.md' - Deep Dive: - 'SNAP Software API': 'deep-dive/libosnap.md' - 'SNAP Registers': 'deep-dive/registers.md' - 'SNAP Logic Design': 'deep-dive/snap_core.md' - 'New Board Support': 'deep-dive/board-package.md' - Misc: - 'Document Guide': 'misc/doc-guide.md' You can edit them as needed. Write Markdown pages On your local desktop, edit markdown files under web-doc/docs folder. If you want to add/delete/rename the files, you also need to edit mkdocs.yml Now it's time to work with an editor (i.e, typora) to write the documents. You also may need to learn some markdown syntax. Don't worry, that's easy. And please turn on the \"spell checking\" in your Markdown editor. In your terminal (MacOS or Linux), or cmd (Windows), start a serve process: # enter in the directory where mkdocs.yml is located cd ~/oc-accel-doc/web-doc #launch a local webserver to test your mods mkdocs serve Then open a web browser, input http://127.0.0.1:8000 . So whenever you save any markdown files, you can check the generated website immediately. Play with pictures The first rule Reduce the usage of pictures. Avoid unnecessary screenshots. It's quite easy You can insert jpg, png, svg files. You can also simply copy paste pictures from clipboard and paste them. Copy the files into a directory ./${filename}.assets , and here ${filename} is the name of markdown file. Use relative links in the document. Note If you are using Typora, please enable \"Copy images into ./${filename}.assets folder\" in Preferences of typora. Tools to draw diagrams You can take any drawing tools to create diagrams. You can save them as PNG format, but the better way is to save to SVG format. For the diagrams from Microsoft PowerPoint, you can select the region of a diagram in PPT, Ctrl-C to copy it, and Ctrl-V to paste it in Typora directly. In this case, the diagram is saved as an PNG file. But there is a better way to get the smallest file size and best quality: In PowerPoint, select the region of diagram, right-click mouse -> \"Save as Picture ...\" and save it as \"PDF\" format. Open the PDF file with Inkscape . (Right-click the file -> \"Open with ...\", choose Inkscape in the poped up list). Unclick \"Embed images\" and then \"OK\". In Inkscape, save it as SVG file. Insert the SVG file into Typora. In my experiment, the PNG file is 188KB. But with the above flow to save it as SVG file, its size is 62KB. As a vectored diagram, it doesn't have any quality loss when zooming in. Warning Please use normal fonts in PPT, for example \"Arial\". Otherwise you may get a SVG file with a replaced font and that may look different. Code blocks and Admonitions Code blocks Please assign the code language so it can be correctly rendered. For example ``` C for C language. // A function to implement bubble sort void bubbleSort(int arr[], int n) { int i, j; for (i = 0; i < n-1; i++) // Last i elements are already in place for (j = 0; j < n-i-1; j++) if (arr[j] > arr[j+1]) swap(&arr[j], &arr[j+1]); } Admonitions You can use !!! Note or !!! Warning or !!! Danger to start a paragraph of admonitions. Then use 4 spaces to start the admonition text. For example !!! Danger \"Error Message\" This is a dangerous error. It will be shown as: Error Message This is a dangerous error. Deploy to Github Pages When most of the edition work is done, and it's time to commit your documents to oc-accel github. First, you should commit and push your changes of source files (in web-doc ) to git repository. Create pull request, ask someone to review the documents, merge them into master branch after getting approvements. Then you can simply publish website with just one step: cd <PATH>/oc-accel-doc/web-doc mkdocs gh-deploy The entire website will be pushed to gh-pages branch of oc-snap repository. The documentation website will be available at https://opencapi.github.io/oc-accel/ !","title":"Document Guide"},{"location":"misc/doc-guide/#how-to-generate-this-website","text":"This static documentation website is created by MkDocs and is using a theme from bootswatch . It uses \"github pages\" and this site is hosted by Github. The documentation source files are written in Markdown format. With MkDocs tool, the generated site files (html files) are automatically pushed into a specific branch gh-pages of the git repository.","title":"How to generate this website"},{"location":"misc/doc-guide/#installation","text":"","title":"Installation"},{"location":"misc/doc-guide/#1-install-python-and-pip","text":"python and pip","title":"1. Install python and pip"},{"location":"misc/doc-guide/#2-install-mkdocs-bootswatch","text":"pip install mkdocs-bootswatch Please refer to bootswatch for more information.","title":"2. Install mkdocs-bootswatch"},{"location":"misc/doc-guide/#3-install-a-markdown-editor","text":"You can simply edit the markdown (.md) files by any text editor, but it's better to user a professional markdown editor. typora . It supports all of the platforms (Windows/MacOS/Linux). Please configure typora to strict Markdown mode. That ensures you get the same output effects on both typora and mkdocs . vscode . It's also a good editor and has abundant functions and extensions. You can install extensions of Markdown, Preview and Spell checker.","title":"3. Install a markdown editor"},{"location":"misc/doc-guide/#4-install-other-optional-tools","text":"pdf2svg: This tool can convert a pdf lossless picture to svg format. For Mac OS, it can be easily installed by Homebrew , simply by brew install pdf2svg . Alternative choice is Inkscape which is a free drawing tool and can help you draw and convert vector graphics.","title":"4. Install other optional tools"},{"location":"misc/doc-guide/#website-structure","text":"First, you need to git clone the oc-accel repository and go to web-doc directory. Make sure you are working on a branch other than master. $ git clone git@github.com:OpenCAPI/oc-accel-doc.git $ cd oc-accel/web-doc Create a branch if needed: $ git branch <new-branch-name> $ git checkout <A branch other than master> The docs folder is where to put the markdown files, and the mkdocs.yml lists the website structure and global definitons. For example, this site has a structure like: nav: - Home: 'index.md' - User Guide: - 'Prepare Environment': 'user-guide/prepare-env.md' - 'Run an example': 'user-guide/run-example.md' - 'Create a new action': 'user-guide/new-action.md' - 'Co-Simulation': 'user-guide/co-simulation.md' - 'FPGA Image build': 'user-guide/make-image.md' - 'Optimize HLS action': 'user-guide/optimize-hls.md' - 'Deploy on Power Server': 'user-guide/deploy.md' - 'Debug an issue': 'user-guide/debug-issue.md' - 'Command Reference': 'user-guide/command-reference.md' - Examples: - 'hdl_example': 'actions-doc/hdl-example.md' - 'hdl_helloworld': 'actions-doc/hdl-helloworld.md' - 'hls_helloworld_1024': 'actions-doc/hls-helloworld_1024.md' - 'hls_memcopy': 'actions-doc/hls-memcopy.md' - Deep Dive: - 'SNAP Software API': 'deep-dive/libosnap.md' - 'SNAP Registers': 'deep-dive/registers.md' - 'SNAP Logic Design': 'deep-dive/snap_core.md' - 'New Board Support': 'deep-dive/board-package.md' - Misc: - 'Document Guide': 'misc/doc-guide.md' You can edit them as needed.","title":"Website Structure"},{"location":"misc/doc-guide/#write-markdown-pages","text":"On your local desktop, edit markdown files under web-doc/docs folder. If you want to add/delete/rename the files, you also need to edit mkdocs.yml Now it's time to work with an editor (i.e, typora) to write the documents. You also may need to learn some markdown syntax. Don't worry, that's easy. And please turn on the \"spell checking\" in your Markdown editor. In your terminal (MacOS or Linux), or cmd (Windows), start a serve process: # enter in the directory where mkdocs.yml is located cd ~/oc-accel-doc/web-doc #launch a local webserver to test your mods mkdocs serve Then open a web browser, input http://127.0.0.1:8000 . So whenever you save any markdown files, you can check the generated website immediately.","title":"Write Markdown pages"},{"location":"misc/doc-guide/#play-with-pictures","text":"","title":"Play with pictures"},{"location":"misc/doc-guide/#the-first-rule","text":"Reduce the usage of pictures. Avoid unnecessary screenshots.","title":"The first rule"},{"location":"misc/doc-guide/#its-quite-easy","text":"You can insert jpg, png, svg files. You can also simply copy paste pictures from clipboard and paste them. Copy the files into a directory ./${filename}.assets , and here ${filename} is the name of markdown file. Use relative links in the document. Note If you are using Typora, please enable \"Copy images into ./${filename}.assets folder\" in Preferences of typora.","title":"It's quite easy"},{"location":"misc/doc-guide/#tools-to-draw-diagrams","text":"You can take any drawing tools to create diagrams. You can save them as PNG format, but the better way is to save to SVG format. For the diagrams from Microsoft PowerPoint, you can select the region of a diagram in PPT, Ctrl-C to copy it, and Ctrl-V to paste it in Typora directly. In this case, the diagram is saved as an PNG file. But there is a better way to get the smallest file size and best quality: In PowerPoint, select the region of diagram, right-click mouse -> \"Save as Picture ...\" and save it as \"PDF\" format. Open the PDF file with Inkscape . (Right-click the file -> \"Open with ...\", choose Inkscape in the poped up list). Unclick \"Embed images\" and then \"OK\". In Inkscape, save it as SVG file. Insert the SVG file into Typora. In my experiment, the PNG file is 188KB. But with the above flow to save it as SVG file, its size is 62KB. As a vectored diagram, it doesn't have any quality loss when zooming in. Warning Please use normal fonts in PPT, for example \"Arial\". Otherwise you may get a SVG file with a replaced font and that may look different.","title":"Tools to draw diagrams"},{"location":"misc/doc-guide/#code-blocks-and-admonitions","text":"","title":"Code blocks and Admonitions"},{"location":"misc/doc-guide/#code-blocks","text":"Please assign the code language so it can be correctly rendered. For example ``` C for C language. // A function to implement bubble sort void bubbleSort(int arr[], int n) { int i, j; for (i = 0; i < n-1; i++) // Last i elements are already in place for (j = 0; j < n-i-1; j++) if (arr[j] > arr[j+1]) swap(&arr[j], &arr[j+1]); }","title":"Code blocks"},{"location":"misc/doc-guide/#admonitions","text":"You can use !!! Note or !!! Warning or !!! Danger to start a paragraph of admonitions. Then use 4 spaces to start the admonition text. For example !!! Danger \"Error Message\" This is a dangerous error. It will be shown as: Error Message This is a dangerous error.","title":"Admonitions"},{"location":"misc/doc-guide/#deploy-to-github-pages","text":"When most of the edition work is done, and it's time to commit your documents to oc-accel github. First, you should commit and push your changes of source files (in web-doc ) to git repository. Create pull request, ask someone to review the documents, merge them into master branch after getting approvements. Then you can simply publish website with just one step: cd <PATH>/oc-accel-doc/web-doc mkdocs gh-deploy The entire website will be pushed to gh-pages branch of oc-snap repository. The documentation website will be available at https://opencapi.github.io/oc-accel/ !","title":"Deploy to Github Pages"},{"location":"user-guide/0-steps/","text":"Steps in a glance Under \"User-guide\" menu, the pages/steps are organized in this way: If you are new to OC-Accel, please go through (1), (2), and (8). With running helloworld, you will get an idea about the workflow and basic operations quickly. After that, it's time to create your own accelerator (\"action\"). Then go through (1), (3), (4/5), (6), (7), (8). If you already have an action developed in SNAP1/2 and want to move to OC-Accel, please read the notes in (9). Tips, possible developments and known issues are being updated in (10). Go to step 2 to use helloworld existing example Go to step 3 to create your own case Go to step 9 Migration from SNAP1 or 2","title":"(0) Steps in a glance"},{"location":"user-guide/0-steps/#steps-in-a-glance","text":"Under \"User-guide\" menu, the pages/steps are organized in this way: If you are new to OC-Accel, please go through (1), (2), and (8). With running helloworld, you will get an idea about the workflow and basic operations quickly. After that, it's time to create your own accelerator (\"action\"). Then go through (1), (3), (4/5), (6), (7), (8). If you already have an action developed in SNAP1/2 and want to move to OC-Accel, please read the notes in (9). Tips, possible developments and known issues are being updated in (10). Go to step 2 to use helloworld existing example Go to step 3 to create your own case Go to step 9 Migration from SNAP1 or 2","title":"Steps in a glance"},{"location":"user-guide/1-prepare-env/","text":"This page will introduce the basic environmental requests, tools, and general commands to run OC-Accel flow. Prepare Environment Basic Tools Firstly, you need to have an x86 machine for development with Vivado Tool and the license (Vivado is not available on Power) export XILINX_VIVADO=<...path...>/Xilinx/Vivado/<VERSION> export XILINXD_LICENSE_FILE=<pointer to Xilinx license> export PATH=$PATH:${XILINX_VIVADO}/bin Note OC-Accel started on Vivado 2018.2, 2018.3 and 2019.1. It now best runs on 2019.2 for all cards. For AD9H3 and AD9H7 cards with HBM, Vivado version is at least 2019.2 There is a file setup_tools.ksh in the root directory for reference. But for the beginning, only Vivado is required. Make sure you have gcc , make , sed , awk , xterm and python installed. setup_tools.ksh You may install other simulators to accelerate the speed of simulation. For example, Cadence xcelium . See in co-simulation for more information. Clone Github Repositories git clone https://github.com/OpenCAPI/oc-accel # note that subdirectory oc-bip containg the boards information will be automatically loaded as a submodule # note that ocse (the OC-ACCEL Simulation Engine) is also automatically loaded if required It's better to have ocse stay in the same directory parallel to oc-accel . That is the default path of $OCSE_ROOT . Or you need to assign $OCSE_ROOT explicitly in snap_env.sh . Basic terms Option1: All-in-one python script OC-Accel developed a \"all-in-one\" Python script to control the workflow. It's convenient to do batch work, or enable your regression verification or continuous integration. cd oc-accel ./ocaccel_workflow.py This script will Check environmental variables make snap_config build model start simulation There are many options provided by ocaccel_workflow.py . Check the help messages by ./ocaccel_workflow.py --help It helps you to do all kinds of operations in one command line. Option2: Traditional \"make\" steps If you have used SNAP for CAPI1.0 and CAPI2.0, you can continue to use these \"traditional\" make steps. Just typing \"make\" doesn't work. An explicit target is needed. You can find them in Makefile file. cd oc-accel make help Main targets for the SNAP Framework make process: ================================================= * snap_config Configure SNAP framework * model Build simulation model for simulator specified via target snap_config * sim Start a simulation * sim_tmux Start a simulation in tmux (no xterm window popped up) * hw_project Create Vivado project with oc-bip * image Build a complete FPGA bitstream after hw_project (takes more than one hour) * hardware One step to build FPGA bitstream (Combines targets 'model' and 'image') * software Build software libraries and tools for SNAP * apps Build the applications for all actions * clean Remove all files generated in make process * clean_config As target 'clean' plus reset of the configuration * help Print this message The hardware related targets 'model', 'image', 'hardware', 'hw_project' and 'sim' do only exist on an x86 platform For simulation make snap_config make model make sim Note After make model , you can continue to run make image to generate bitstreams. In fact, make model also creates a Vivado project framework.xpr in hardware/viv_project . Then it exports the simulation files and compiles them to a simulation model. For Image build make snap_config If it has already been executed, no need to run it again. make hw_project make image Note Use Vivado GUI : After make hw_project , you can open project framework.xpr in hardware/viv_project , and do following \"run Synthesis\" , \"run Implementation\" and \"generate Bitstream\" in Vivado GUI. Output files The log files during these steps are placed in hardware/logs . Simulation output files are placed in hardware/sim/<SIMULATOR>/latest . If you are using make image to generate bitstreams, the outputs are in hardware/build , including Images , Reports and Checkpoints . If you are using Vivado Gui mode to generate bitstream, the outputs are in hardware/viv_project/framework.runs , including synth_1 and impl_1 , etc. Let's go! Now, let's take an example hls_helloworld , and have a look at how it runs step by step. Please continue to read page Run helloworld .","title":"(1) Prepare environment"},{"location":"user-guide/1-prepare-env/#prepare-environment","text":"","title":"Prepare Environment"},{"location":"user-guide/1-prepare-env/#basic-tools","text":"Firstly, you need to have an x86 machine for development with Vivado Tool and the license (Vivado is not available on Power) export XILINX_VIVADO=<...path...>/Xilinx/Vivado/<VERSION> export XILINXD_LICENSE_FILE=<pointer to Xilinx license> export PATH=$PATH:${XILINX_VIVADO}/bin Note OC-Accel started on Vivado 2018.2, 2018.3 and 2019.1. It now best runs on 2019.2 for all cards. For AD9H3 and AD9H7 cards with HBM, Vivado version is at least 2019.2 There is a file setup_tools.ksh in the root directory for reference. But for the beginning, only Vivado is required. Make sure you have gcc , make , sed , awk , xterm and python installed. setup_tools.ksh You may install other simulators to accelerate the speed of simulation. For example, Cadence xcelium . See in co-simulation for more information.","title":"Basic Tools"},{"location":"user-guide/1-prepare-env/#clone-github-repositories","text":"git clone https://github.com/OpenCAPI/oc-accel # note that subdirectory oc-bip containg the boards information will be automatically loaded as a submodule # note that ocse (the OC-ACCEL Simulation Engine) is also automatically loaded if required It's better to have ocse stay in the same directory parallel to oc-accel . That is the default path of $OCSE_ROOT . Or you need to assign $OCSE_ROOT explicitly in snap_env.sh .","title":"Clone Github Repositories"},{"location":"user-guide/1-prepare-env/#basic-terms","text":"","title":"Basic terms"},{"location":"user-guide/1-prepare-env/#option1-all-in-one-python-script","text":"OC-Accel developed a \"all-in-one\" Python script to control the workflow. It's convenient to do batch work, or enable your regression verification or continuous integration. cd oc-accel ./ocaccel_workflow.py This script will Check environmental variables make snap_config build model start simulation There are many options provided by ocaccel_workflow.py . Check the help messages by ./ocaccel_workflow.py --help It helps you to do all kinds of operations in one command line.","title":"Option1: All-in-one python script"},{"location":"user-guide/1-prepare-env/#option2-traditional-make-steps","text":"If you have used SNAP for CAPI1.0 and CAPI2.0, you can continue to use these \"traditional\" make steps. Just typing \"make\" doesn't work. An explicit target is needed. You can find them in Makefile file. cd oc-accel make help Main targets for the SNAP Framework make process: ================================================= * snap_config Configure SNAP framework * model Build simulation model for simulator specified via target snap_config * sim Start a simulation * sim_tmux Start a simulation in tmux (no xterm window popped up) * hw_project Create Vivado project with oc-bip * image Build a complete FPGA bitstream after hw_project (takes more than one hour) * hardware One step to build FPGA bitstream (Combines targets 'model' and 'image') * software Build software libraries and tools for SNAP * apps Build the applications for all actions * clean Remove all files generated in make process * clean_config As target 'clean' plus reset of the configuration * help Print this message The hardware related targets 'model', 'image', 'hardware', 'hw_project' and 'sim' do only exist on an x86 platform","title":"Option2: Traditional \"make\" steps"},{"location":"user-guide/1-prepare-env/#for-simulation","text":"make snap_config make model make sim Note After make model , you can continue to run make image to generate bitstreams. In fact, make model also creates a Vivado project framework.xpr in hardware/viv_project . Then it exports the simulation files and compiles them to a simulation model.","title":"For simulation"},{"location":"user-guide/1-prepare-env/#for-image-build","text":"make snap_config If it has already been executed, no need to run it again. make hw_project make image Note Use Vivado GUI : After make hw_project , you can open project framework.xpr in hardware/viv_project , and do following \"run Synthesis\" , \"run Implementation\" and \"generate Bitstream\" in Vivado GUI.","title":"For Image build"},{"location":"user-guide/1-prepare-env/#output-files","text":"The log files during these steps are placed in hardware/logs . Simulation output files are placed in hardware/sim/<SIMULATOR>/latest . If you are using make image to generate bitstreams, the outputs are in hardware/build , including Images , Reports and Checkpoints . If you are using Vivado Gui mode to generate bitstream, the outputs are in hardware/viv_project/framework.runs , including synth_1 and impl_1 , etc.","title":"Output files"},{"location":"user-guide/1-prepare-env/#lets-go","text":"Now, let's take an example hls_helloworld , and have a look at how it runs step by step. Please continue to read page Run helloworld .","title":"Let's go!"},{"location":"user-guide/10-tips/","text":"ILA Debug Enable ILA_DEBUG in Kconfig Menu. Add your ILA core. Known issues and TODO Features to be supported: Interrupt Multiple-process supported Cleanup work: Renaming SNAP to OCAC; oc-snap to oc-accel Enrich example lists: Multiple-process example Ethernet example HBM example","title":"(10) Tips"},{"location":"user-guide/10-tips/#ila-debug","text":"Enable ILA_DEBUG in Kconfig Menu. Add your ILA core.","title":"ILA Debug"},{"location":"user-guide/10-tips/#known-issues-and-todo","text":"Features to be supported: Interrupt Multiple-process supported Cleanup work: Renaming SNAP to OCAC; oc-snap to oc-accel Enrich example lists: Multiple-process example Ethernet example HBM example","title":"Known issues and TODO"},{"location":"user-guide/2-run-helloworld/","text":"Run hls_helloworld snap_config cd oc-accel # either run automated python script ./ocaccel_workflow.py # or run step by step : make snap_config Then a KConfig window will popped up. If it doesn't, check Required tools and search 'kconfig' on the homepage. Select HLS HelloWorld in \"Action Type\". There are some other choices listed in the menu. Please check OCSE_ROOT path if default is not used. Select xsim (the default Vivado simulator). To select a TRUE/FALSE feature, press \"Y\" or \"N\". After everything done, move cursor to \"Exit\". Note This Kconfig menu is editable. If you want to add new features or enrich your own menu, please edit scripts/Kconfig file. Then it starts to execute many steps to build a simulation model. It needs several minutes. While waiting for it, open another terminal tab and try to get familiar with some environmental variables. Open snap_env.sh and check the very basic ones: export ACTION_ROOT=<path_of_oc-accel>/actions/hls_helloworld_1024 export TIMING_LABLIMIT=\"-200\" export OCSE_ROOT=<path_to_ocse>/ocse ACTION_ROOT should refer to your selected choice in the previous menu TIMINGS_LABLIMIT is refering to the hardware acceptation criteria. Should the hardware implementation not satisfy delay rules, for lab evaluation, we can afford to have some timings not fully respected in extreme conditions (voltages, temperature, etc ...). Having a -200ns is generally considered fair for evaluation purposes. Note that 0ns should be the production limit. The actual value is reported at the end of the hardware preparation. Finaly check the OC -accel S imulation E ngine location. Simulation If your were running in script mode: ocaccel-workflow.py continues running and prints: or simply run: make sim SNAP Configured You've got configuration like: ACTION_ROOT /home/castella/oc-accel/actions/hls_helloworld_1024 FPGACARD AD9V3 FPGACHIP xcvu3p-ffvc1517-2-e SIMULATOR xsim CAPI_VER opencapi30 OCSE_ROOT ../ocse --------> Environment Check gcc installed as /usr/bin/gcc vivado installed as /opt/Xilinx/Vivado/2019.2/bin/vivado xterm installed as /usr/bin/xterm OCSE path /home/castella/ocse is valid SNAP ROOT /home/castella/oc-accel is valid Environment check PASSED --------> Make the simulation model Runnig ... check ./ocaccel_workflow.make_model.log for details of full progress FINISHED! - [=============================] 100% Then a Xterm window will pop up. (If it doesn't, check if you have installed it by typing xterm in your terminal.) This Xterm window is where you run your application (software part). You can run anything as many times as you want in the xterm window, just like running in the terminal of a real server with FPGA card plugged. Warning If you want to save the content running in this xterm window, please use script before running any commands. When you exit xterm window, everything is saved to a file -- \"typescript\" is its default name. $ script Script started, file is typescript ...... Run Anything ..... $ exit exit Script done, file is typescript Now let's run application snap_helloworld . It is located in $ACTION_ROOT/sw , where $ACTION_ROOT is <path_of_oc-accel>/actions/hls_helloworld . In the above window, it prints the help messages because it requires two arguments: an input text file and an output text file. We have prepared a script in $ACTION_ROOT/tests/hw_test.sh and you can run it directly. This example is asking FPGA to read the input file from host memory, converting the letters to capital case, and write them back to host memory and save in the output file. Now you have finished the software/hardware co-simulation. Type 'exit' in xterm window. All the output logs, waveforms are in hardware/sim/<simulator>/latest . hdclf154: luyong /afs/bb/u/luyong/p9/nd2/oc_dev/oc-accel/hardware/sim/xsim/latest $ ls debug.log ocse_server.dat snap_helloworld.log tin webtalk.jou xsim.dir xsrun.tcl ocse.log shim_host.dat tCAP top.wdb webtalk.log xsim.jou ocse.parms sim.log terminal.log tout xsaet.tcl xsim.log And you can use following command to open the waveform. xsim top.wdb -gui & On the project scope (hierarchy) panel, the user logic is action_w . Make FPGA bit image In above steps, you actually have finished steps of: make snap_config make model make sim (These targets are introduced in Traditional \"make\" steps ) Now you can generate FPGA image by ./ocaccel_workflow.py --no_configure --no_make_model --no_run_sim --make_image It takes about 2 hours or more. For some big design or bad-timing design, it takes even longer. Check the progress: ./snap_workflow.make_image.log Or hardware/logs/snap_build.log for more detailed logs. For example, ./snap_workflow.make_image.log may tell you: [BUILD IMAGE.........] start 16:58:57 Sat Sep 14 2019 A complete FPGA bitstream build got kicked off. This might take more than an hour depending on the machine used The process may be terminated by pressing <CTRL>-C at any time. After termination it can be restarted later. open framework project 16:59:06 Sat Sep 14 2019 start synthesis with directive: Default 16:59:25 Sat Sep 14 2019 start opt_design with directive: Explore 17:18:58 Sat Sep 14 2019 reload opt_design DCP 17:27:19 Sat Sep 14 2019 start place_design with directive: Explore 17:28:23 Sat Sep 14 2019 start phys_opt_design with directive: Explore 17:42:58 Sat Sep 14 2019 start route_design with directive: Explore 18:04:55 Sat Sep 14 2019 start opt_routed_design with directive: Explore 18:39:01 Sat Sep 14 2019 generating reports 18:57:28 Sat Sep 14 2019 Timing (WNS) -11 ps WARNING: TIMING FAILED, but may be OK for lab use generating bitstreams type: user image 18:58:28 Sat Sep 14 2019 So you can have an estimation of the progress. After it's completed, you can find the FPGA bit image files in hardware/build/Images . The file names have the information of build date/time, action name, card type and timing slack (-11ps here). $ cd hardware/build/Images $ ls oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11.bit oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_primary.bin oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_primary.prm oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_secondary.bin oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_secondary.prm Note A small negative timing slack less than 200ps (set in variable $TIMING_LABLIMIT), is usually acceptable for Lab test, but for product, it's suggested to work out a timing cleaned FPGA image. See Build image for more information. Then go to Deploy on Power Server to see how to download the bitstream and run. Summary Now you understand how to run an existing example. You can use the same method to run other examples in actions directory.","title":"(2) Run helloworld"},{"location":"user-guide/2-run-helloworld/#run-hls_helloworld","text":"","title":"Run hls_helloworld"},{"location":"user-guide/2-run-helloworld/#snap_config","text":"cd oc-accel # either run automated python script ./ocaccel_workflow.py # or run step by step : make snap_config Then a KConfig window will popped up. If it doesn't, check Required tools and search 'kconfig' on the homepage. Select HLS HelloWorld in \"Action Type\". There are some other choices listed in the menu. Please check OCSE_ROOT path if default is not used. Select xsim (the default Vivado simulator). To select a TRUE/FALSE feature, press \"Y\" or \"N\". After everything done, move cursor to \"Exit\". Note This Kconfig menu is editable. If you want to add new features or enrich your own menu, please edit scripts/Kconfig file. Then it starts to execute many steps to build a simulation model. It needs several minutes. While waiting for it, open another terminal tab and try to get familiar with some environmental variables. Open snap_env.sh and check the very basic ones: export ACTION_ROOT=<path_of_oc-accel>/actions/hls_helloworld_1024 export TIMING_LABLIMIT=\"-200\" export OCSE_ROOT=<path_to_ocse>/ocse ACTION_ROOT should refer to your selected choice in the previous menu TIMINGS_LABLIMIT is refering to the hardware acceptation criteria. Should the hardware implementation not satisfy delay rules, for lab evaluation, we can afford to have some timings not fully respected in extreme conditions (voltages, temperature, etc ...). Having a -200ns is generally considered fair for evaluation purposes. Note that 0ns should be the production limit. The actual value is reported at the end of the hardware preparation. Finaly check the OC -accel S imulation E ngine location.","title":"snap_config"},{"location":"user-guide/2-run-helloworld/#simulation","text":"If your were running in script mode: ocaccel-workflow.py continues running and prints: or simply run: make sim SNAP Configured You've got configuration like: ACTION_ROOT /home/castella/oc-accel/actions/hls_helloworld_1024 FPGACARD AD9V3 FPGACHIP xcvu3p-ffvc1517-2-e SIMULATOR xsim CAPI_VER opencapi30 OCSE_ROOT ../ocse --------> Environment Check gcc installed as /usr/bin/gcc vivado installed as /opt/Xilinx/Vivado/2019.2/bin/vivado xterm installed as /usr/bin/xterm OCSE path /home/castella/ocse is valid SNAP ROOT /home/castella/oc-accel is valid Environment check PASSED --------> Make the simulation model Runnig ... check ./ocaccel_workflow.make_model.log for details of full progress FINISHED! - [=============================] 100% Then a Xterm window will pop up. (If it doesn't, check if you have installed it by typing xterm in your terminal.) This Xterm window is where you run your application (software part). You can run anything as many times as you want in the xterm window, just like running in the terminal of a real server with FPGA card plugged. Warning If you want to save the content running in this xterm window, please use script before running any commands. When you exit xterm window, everything is saved to a file -- \"typescript\" is its default name. $ script Script started, file is typescript ...... Run Anything ..... $ exit exit Script done, file is typescript Now let's run application snap_helloworld . It is located in $ACTION_ROOT/sw , where $ACTION_ROOT is <path_of_oc-accel>/actions/hls_helloworld . In the above window, it prints the help messages because it requires two arguments: an input text file and an output text file. We have prepared a script in $ACTION_ROOT/tests/hw_test.sh and you can run it directly. This example is asking FPGA to read the input file from host memory, converting the letters to capital case, and write them back to host memory and save in the output file. Now you have finished the software/hardware co-simulation. Type 'exit' in xterm window. All the output logs, waveforms are in hardware/sim/<simulator>/latest . hdclf154: luyong /afs/bb/u/luyong/p9/nd2/oc_dev/oc-accel/hardware/sim/xsim/latest $ ls debug.log ocse_server.dat snap_helloworld.log tin webtalk.jou xsim.dir xsrun.tcl ocse.log shim_host.dat tCAP top.wdb webtalk.log xsim.jou ocse.parms sim.log terminal.log tout xsaet.tcl xsim.log And you can use following command to open the waveform. xsim top.wdb -gui & On the project scope (hierarchy) panel, the user logic is action_w .","title":"Simulation"},{"location":"user-guide/2-run-helloworld/#make-fpga-bit-image","text":"In above steps, you actually have finished steps of: make snap_config make model make sim (These targets are introduced in Traditional \"make\" steps ) Now you can generate FPGA image by ./ocaccel_workflow.py --no_configure --no_make_model --no_run_sim --make_image It takes about 2 hours or more. For some big design or bad-timing design, it takes even longer. Check the progress: ./snap_workflow.make_image.log Or hardware/logs/snap_build.log for more detailed logs. For example, ./snap_workflow.make_image.log may tell you: [BUILD IMAGE.........] start 16:58:57 Sat Sep 14 2019 A complete FPGA bitstream build got kicked off. This might take more than an hour depending on the machine used The process may be terminated by pressing <CTRL>-C at any time. After termination it can be restarted later. open framework project 16:59:06 Sat Sep 14 2019 start synthesis with directive: Default 16:59:25 Sat Sep 14 2019 start opt_design with directive: Explore 17:18:58 Sat Sep 14 2019 reload opt_design DCP 17:27:19 Sat Sep 14 2019 start place_design with directive: Explore 17:28:23 Sat Sep 14 2019 start phys_opt_design with directive: Explore 17:42:58 Sat Sep 14 2019 start route_design with directive: Explore 18:04:55 Sat Sep 14 2019 start opt_routed_design with directive: Explore 18:39:01 Sat Sep 14 2019 generating reports 18:57:28 Sat Sep 14 2019 Timing (WNS) -11 ps WARNING: TIMING FAILED, but may be OK for lab use generating bitstreams type: user image 18:58:28 Sat Sep 14 2019 So you can have an estimation of the progress. After it's completed, you can find the FPGA bit image files in hardware/build/Images . The file names have the information of build date/time, action name, card type and timing slack (-11ps here). $ cd hardware/build/Images $ ls oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11.bit oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_primary.bin oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_primary.prm oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_secondary.bin oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_secondary.prm Note A small negative timing slack less than 200ps (set in variable $TIMING_LABLIMIT), is usually acceptable for Lab test, but for product, it's suggested to work out a timing cleaned FPGA image. See Build image for more information. Then go to Deploy on Power Server to see how to download the bitstream and run.","title":"Make FPGA bit image"},{"location":"user-guide/2-run-helloworld/#summary","text":"Now you understand how to run an existing example. You can use the same method to run other examples in actions directory.","title":"Summary"},{"location":"user-guide/3-new-action/","text":"A new git fork The first step before creating a new action is to create a git \"fork\". Now play on the forked git repository: git clone https://github.com/<MY_NAME>/oc-accel git submodule init git submodule update Delete the unnecessary branches, only keep \"master\", and create your own branches. When you want to sync with the original repository, do following steps: git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git Check status: git remote -v # origin https://github.com/MY_NAME/MY_FORK.git (fetch) # origin https://github.com/MY_NAME/MY_FORK.git (push) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) Do the sync: git fetch upstream git merge upstream/master When you have some fixes and added some new features, create a pull request from your fork to the original repository. It will be reviewed and then the contribution will be merged. Note OC-Accel encourages people to create their own actions and the links will be recommended on README.md . But to keep the repository relatively small and neat, it will not copy every user action design into its original actions folder. Submit an issue or pull request to start the discussion. A new action We have several examples as references. Current action example list is: Name Type Description hdl_example VHDL 512b hdl_example inherited from SNAP1/2. Optional FPGA DDR. hdl_single_engine Verilog 1024b example to send AXI read/write commands. Used to measure bandwidth and latency. No FPGA DDR. hls_helloworld_512 C/C++(HLS) 512b example to read data from host, convert to CAPITAL case and write back. No FPGA DDR. hls_helloworld_1024 C/C++(HLS) 1024b example to read data from host, convert to CAPITAL case and write back. No FPGA DDR. hls_memcopy_1024 C/C++(HLS) 1024b example to do memcopy. Enabled FPGA DDR. According to the action category, copy the folder of a proper example from actions and name it. Give it a name and type Step1: make snap_config Select HLS Action - manually set ... or HDL Action - manually set ... in the blue kconfig window. Step2: Edit snap_env.sh , point $ACTION_ROOT to the new action. export ACTION_ROOT=<...path...>/oc-accel/actions/my_new_action export TIMING_LABLIMIT=\"-200\" export OCSE_ROOT=<...path...>/ocse Step3: Edit software/tools/snap_actions.h , add a row of the new action, with the company/person name, action type ID, and a short description. static const struct actions_tab snap_actions[] = { { \"IBM\", 0x10140000, \"hdl_example in VHDL (512b)\" }, { \"IBM\", 0x10140002, \"hdl_single_engine in Verilog (1024b)\" }, { \"IBM\", 0x10140004, \"UVM test for unit verification (no OCSE and software)\" }, { \"IBM\", 0x10141001, \"HLS Sponge (512b)\" }, { \"IBM\", 0x10141008, \"HLS Hello World (512b)\" }, { \"IBM\", 0x1014100B, \"HLS Memcopy 1024 (1024b)\" }, }; Step4: The Action Type (for example, 0x10140000) should match with following places: actions/<my_new_action>/hw/ actions/<my_new_action>/sw/ Do a grep search and replace them. Understand the workflow Modify the example code (sw, hw and tests) to cook a new action. Understanding the workflow can help quickly identifying what's wrong. These steps are organized in Makefile hardware/Makefile software/Makefile Actions --> $ACTION_ROOT/hw/Makefile $ACTION_ROOT/sw/Makefile When adding a new action, before calling the \"All-in-one\" ocaccel_workflow.py, make sure the make process under $ACTION_ROOT works. cd $ACTION_ROOT/sw make cd $ACTION_ROOT/hw make Above figure shows the steps to make a simulation model. The Action related steps are marked in light orange color. There are also two important tricks: PREPROCESS : it will deal with the files with \"_source\" suffix. That means, \"FILE_source\" will be \"pre-processed\" and converted to \"FILE\". \"FILE\" is a generated one and should not be modified manually, and should not be committed to github either. Build Date/Time, Git Version and Card info will be hardcoded into snap_core logic by \" patch_version.sh \" Start simulation After clean up compiling errors in action sw and action hw, kick off a co-simulation by ./ocaccel_workflow.py","title":"(3) Create a new action"},{"location":"user-guide/3-new-action/#a-new-git-fork","text":"The first step before creating a new action is to create a git \"fork\". Now play on the forked git repository: git clone https://github.com/<MY_NAME>/oc-accel git submodule init git submodule update Delete the unnecessary branches, only keep \"master\", and create your own branches. When you want to sync with the original repository, do following steps: git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git Check status: git remote -v # origin https://github.com/MY_NAME/MY_FORK.git (fetch) # origin https://github.com/MY_NAME/MY_FORK.git (push) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) Do the sync: git fetch upstream git merge upstream/master When you have some fixes and added some new features, create a pull request from your fork to the original repository. It will be reviewed and then the contribution will be merged. Note OC-Accel encourages people to create their own actions and the links will be recommended on README.md . But to keep the repository relatively small and neat, it will not copy every user action design into its original actions folder. Submit an issue or pull request to start the discussion.","title":"A new git fork"},{"location":"user-guide/3-new-action/#a-new-action","text":"We have several examples as references. Current action example list is: Name Type Description hdl_example VHDL 512b hdl_example inherited from SNAP1/2. Optional FPGA DDR. hdl_single_engine Verilog 1024b example to send AXI read/write commands. Used to measure bandwidth and latency. No FPGA DDR. hls_helloworld_512 C/C++(HLS) 512b example to read data from host, convert to CAPITAL case and write back. No FPGA DDR. hls_helloworld_1024 C/C++(HLS) 1024b example to read data from host, convert to CAPITAL case and write back. No FPGA DDR. hls_memcopy_1024 C/C++(HLS) 1024b example to do memcopy. Enabled FPGA DDR. According to the action category, copy the folder of a proper example from actions and name it.","title":"A new action"},{"location":"user-guide/3-new-action/#give-it-a-name-and-type","text":"Step1: make snap_config Select HLS Action - manually set ... or HDL Action - manually set ... in the blue kconfig window. Step2: Edit snap_env.sh , point $ACTION_ROOT to the new action. export ACTION_ROOT=<...path...>/oc-accel/actions/my_new_action export TIMING_LABLIMIT=\"-200\" export OCSE_ROOT=<...path...>/ocse Step3: Edit software/tools/snap_actions.h , add a row of the new action, with the company/person name, action type ID, and a short description. static const struct actions_tab snap_actions[] = { { \"IBM\", 0x10140000, \"hdl_example in VHDL (512b)\" }, { \"IBM\", 0x10140002, \"hdl_single_engine in Verilog (1024b)\" }, { \"IBM\", 0x10140004, \"UVM test for unit verification (no OCSE and software)\" }, { \"IBM\", 0x10141001, \"HLS Sponge (512b)\" }, { \"IBM\", 0x10141008, \"HLS Hello World (512b)\" }, { \"IBM\", 0x1014100B, \"HLS Memcopy 1024 (1024b)\" }, }; Step4: The Action Type (for example, 0x10140000) should match with following places: actions/<my_new_action>/hw/ actions/<my_new_action>/sw/ Do a grep search and replace them.","title":"Give it a name and type"},{"location":"user-guide/3-new-action/#understand-the-workflow","text":"Modify the example code (sw, hw and tests) to cook a new action. Understanding the workflow can help quickly identifying what's wrong. These steps are organized in Makefile hardware/Makefile software/Makefile Actions --> $ACTION_ROOT/hw/Makefile $ACTION_ROOT/sw/Makefile When adding a new action, before calling the \"All-in-one\" ocaccel_workflow.py, make sure the make process under $ACTION_ROOT works. cd $ACTION_ROOT/sw make cd $ACTION_ROOT/hw make Above figure shows the steps to make a simulation model. The Action related steps are marked in light orange color. There are also two important tricks: PREPROCESS : it will deal with the files with \"_source\" suffix. That means, \"FILE_source\" will be \"pre-processed\" and converted to \"FILE\". \"FILE\" is a generated one and should not be modified manually, and should not be committed to github either. Build Date/Time, Git Version and Card info will be hardcoded into snap_core logic by \" patch_version.sh \"","title":"Understand the workflow"},{"location":"user-guide/3-new-action/#start-simulation","text":"After clean up compiling errors in action sw and action hw, kick off a co-simulation by ./ocaccel_workflow.py","title":"Start simulation"},{"location":"user-guide/4-hdl-design/","text":"VHDL/Verilog Action HW Design Take hdl_single_engine as an example, the top design is action_wrapper.v , you need to implement at least axi_lite_slave axi_master axi_lite_slave (Action Registers) Signal Width ADDR 32bits DATA 32bits The Action registers can be defined freely. For a single engine inside an action, the lower 22bits of ADDR are available. That means, you can define one million 4B registers inside 4MB range. The address definition should match with Action SW header file. For example, #define REG_SNAP_CONTROL 0x00 #define REG_SNAP_INT_ENABLE 0x04 #define REG_SNAP_ACTION_TYPE 0x10 #define REG_SNAP_ACTION_VERSION 0x14 // User defined below #define REG_USER_STATUS 0x30 #define REG_USER_CONTROL 0x34 #define REG_USER_MODE 0x38 ..... Note The User defined Action registers are suggested to start from 0x30 . OC-Accel has some pre-defined Action registers in the range of 0x00-0x2F to cooperate with libosnap and software tools. See Deep Dive: Registers for more information. axi_master (Data Path) Here lists the supported AXI feature list from the viewpoint of an Action . When DATA width is chosen to be 512b, a Xilinx IP \"axi_dwidth_converter\" (data width converter) will be inserted automatically ( diagram ), and this converter may not support all of the AXI features. (It also costs FPGA resources!) So we recommend to design a 1024b-wide axi_master and you can use unaligned address and write strobe freely to transfer small sized data. Read more in AXI4 feature list of snap_core. Note Choose less AXI IDs can save the area of snap_core. The AXI ID ports have at least 1 bit. Drive zero if the Action hardware design doesn't use it. Please drive zero to AXI signals cache , lock , qos , region . Another reason for 1024b-wide axi_master is to make full use of OpenCAPI bandwidth. When an Action runs at 200MHz, 1024b continuous data transferring can get: 1024b * 200MHz = 200Gb/s = 25GB/s And for 512b-wide action with the same clock frequency only half of the possible bandwidth is utilized. And increasing clock frequency may bring difficulties to get timing closure. Action Clock The default clock ap_clk ( clock_act ) frequency is 200MHz, same as the clock feeding snap_core ( clock_afu ). If a different Action Clock is required, please modify hardware/hdl/oc_functions.vhd_source to add a clock generator. The script for this clock generator should be added into hardware/setup/create_snap_ip.tcl . For more information, please read Clock Domain . Action IPs Take hdl_single_engine as an example, actions/hdl_single_engine/hw/Makefile calls a script action_config.sh . Many additional steps can be added in this way to construct the Action hw design, for example, create some IPs: echo \" Call create_action_ip.tcl to generate IPs\" vivado -mode batch -source $ACTION_ROOT/ip/create_action_ip.tcl -notrace -nojournal -tclargs $ACTION_ROOT $FPGACHIP Add action specific IPs into $ACTION_ROOT/ip/create_action_ip.tcl . After they are generated in the Action HW \"make\" process, they will be imported into the project by hardware/setup/create_framework.tcl : # HDL Action IP foreach ip_xci [glob -nocomplain -dir $action_ip_dir */*.xci] { set ip_name [exec basename $ip_xci .xci] puts \" adding HDL Action IP $ip_name\" add_files -norecurse $ip_xci -force >> $log_file export_ip_user_files -of_objects [get_files \"$ip_xci\"] -no_script -sync -force >> $log_file } OC-Accel has enabled one channel of DDR memory controller for AD9V3 card. Take hdl_example (VHDL) and enable \"DDR\" in the KConfig menu. It will ask hardware/setup/create_snap_ip.tcl to generate the required memory controller and simulation model and create_framework.tcl will integrate it. Any other peripheral IPs can be generated similarly. Note Whether to enable a peripheral IP depends on the user case and also depends on the FPGA card. Putting the IP creation script in create_snap_ip.tcl or create_action_ip.tcl are both fine. Action Tcls hardware/setup/create_framework.tcl will also parse $ACTION_ROOT/hw/tcl folder to execute Action tcl scripts. # Action Specific tcl if { [file exists $action_tcl] == 1 } { set tcl_exists [exec find $action_tcl -name *.tcl] if { $tcl_exists != \"\" } { foreach tcl_file [glob -nocomplain -dir $action_tcl *.tcl] { set tcl_file_name [exec basename $tcl_file] puts \" sourcing $tcl_file_name\" source $tcl_file } } } Unit Verification Unit verification is optional and depends on the developers. Co-simulation with OCSE is a good way to verify the correctness, but developers can also build their own unit verification environment to enable more advanced verification tools and methodologies like UVM. Developers and apply more stress random tests and coverage harvest to assure the design quality. VHDL/Verilog Action SW Design The steps include: snap_card_alloc_dev() snap_attach_action() Main body: ACTION registers interaction with hardware snap_detach_action() snap_card_free() About \"main body\" there are a lot of area to explore here. Two APIs are used to read and write Action registers: snap_action_read32() snap_action_write32() Interrupt is supported ( TODO : not fully done yet.) Multiple-process access is also supported. ( TODO : read Examples: hdl_multple_engine)","title":"(4) Verilog/VHDL design"},{"location":"user-guide/4-hdl-design/#vhdlverilog-action-hw-design","text":"Take hdl_single_engine as an example, the top design is action_wrapper.v , you need to implement at least axi_lite_slave axi_master","title":"VHDL/Verilog Action HW Design"},{"location":"user-guide/4-hdl-design/#axi_lite_slave-action-registers","text":"Signal Width ADDR 32bits DATA 32bits The Action registers can be defined freely. For a single engine inside an action, the lower 22bits of ADDR are available. That means, you can define one million 4B registers inside 4MB range. The address definition should match with Action SW header file. For example, #define REG_SNAP_CONTROL 0x00 #define REG_SNAP_INT_ENABLE 0x04 #define REG_SNAP_ACTION_TYPE 0x10 #define REG_SNAP_ACTION_VERSION 0x14 // User defined below #define REG_USER_STATUS 0x30 #define REG_USER_CONTROL 0x34 #define REG_USER_MODE 0x38 ..... Note The User defined Action registers are suggested to start from 0x30 . OC-Accel has some pre-defined Action registers in the range of 0x00-0x2F to cooperate with libosnap and software tools. See Deep Dive: Registers for more information.","title":"axi_lite_slave (Action Registers)"},{"location":"user-guide/4-hdl-design/#axi_master-data-path","text":"Here lists the supported AXI feature list from the viewpoint of an Action . When DATA width is chosen to be 512b, a Xilinx IP \"axi_dwidth_converter\" (data width converter) will be inserted automatically ( diagram ), and this converter may not support all of the AXI features. (It also costs FPGA resources!) So we recommend to design a 1024b-wide axi_master and you can use unaligned address and write strobe freely to transfer small sized data. Read more in AXI4 feature list of snap_core. Note Choose less AXI IDs can save the area of snap_core. The AXI ID ports have at least 1 bit. Drive zero if the Action hardware design doesn't use it. Please drive zero to AXI signals cache , lock , qos , region . Another reason for 1024b-wide axi_master is to make full use of OpenCAPI bandwidth. When an Action runs at 200MHz, 1024b continuous data transferring can get: 1024b * 200MHz = 200Gb/s = 25GB/s And for 512b-wide action with the same clock frequency only half of the possible bandwidth is utilized. And increasing clock frequency may bring difficulties to get timing closure.","title":"axi_master (Data Path)"},{"location":"user-guide/4-hdl-design/#action-clock","text":"The default clock ap_clk ( clock_act ) frequency is 200MHz, same as the clock feeding snap_core ( clock_afu ). If a different Action Clock is required, please modify hardware/hdl/oc_functions.vhd_source to add a clock generator. The script for this clock generator should be added into hardware/setup/create_snap_ip.tcl . For more information, please read Clock Domain .","title":"Action Clock"},{"location":"user-guide/4-hdl-design/#action-ips","text":"Take hdl_single_engine as an example, actions/hdl_single_engine/hw/Makefile calls a script action_config.sh . Many additional steps can be added in this way to construct the Action hw design, for example, create some IPs: echo \" Call create_action_ip.tcl to generate IPs\" vivado -mode batch -source $ACTION_ROOT/ip/create_action_ip.tcl -notrace -nojournal -tclargs $ACTION_ROOT $FPGACHIP Add action specific IPs into $ACTION_ROOT/ip/create_action_ip.tcl . After they are generated in the Action HW \"make\" process, they will be imported into the project by hardware/setup/create_framework.tcl : # HDL Action IP foreach ip_xci [glob -nocomplain -dir $action_ip_dir */*.xci] { set ip_name [exec basename $ip_xci .xci] puts \" adding HDL Action IP $ip_name\" add_files -norecurse $ip_xci -force >> $log_file export_ip_user_files -of_objects [get_files \"$ip_xci\"] -no_script -sync -force >> $log_file } OC-Accel has enabled one channel of DDR memory controller for AD9V3 card. Take hdl_example (VHDL) and enable \"DDR\" in the KConfig menu. It will ask hardware/setup/create_snap_ip.tcl to generate the required memory controller and simulation model and create_framework.tcl will integrate it. Any other peripheral IPs can be generated similarly. Note Whether to enable a peripheral IP depends on the user case and also depends on the FPGA card. Putting the IP creation script in create_snap_ip.tcl or create_action_ip.tcl are both fine.","title":"Action IPs"},{"location":"user-guide/4-hdl-design/#action-tcls","text":"hardware/setup/create_framework.tcl will also parse $ACTION_ROOT/hw/tcl folder to execute Action tcl scripts. # Action Specific tcl if { [file exists $action_tcl] == 1 } { set tcl_exists [exec find $action_tcl -name *.tcl] if { $tcl_exists != \"\" } { foreach tcl_file [glob -nocomplain -dir $action_tcl *.tcl] { set tcl_file_name [exec basename $tcl_file] puts \" sourcing $tcl_file_name\" source $tcl_file } } }","title":"Action Tcls"},{"location":"user-guide/4-hdl-design/#unit-verification","text":"Unit verification is optional and depends on the developers. Co-simulation with OCSE is a good way to verify the correctness, but developers can also build their own unit verification environment to enable more advanced verification tools and methodologies like UVM. Developers and apply more stress random tests and coverage harvest to assure the design quality.","title":"Unit Verification"},{"location":"user-guide/4-hdl-design/#vhdlverilog-action-sw-design","text":"The steps include: snap_card_alloc_dev() snap_attach_action() Main body: ACTION registers interaction with hardware snap_detach_action() snap_card_free() About \"main body\" there are a lot of area to explore here. Two APIs are used to read and write Action registers: snap_action_read32() snap_action_write32() Interrupt is supported ( TODO : not fully done yet.) Multiple-process access is also supported. ( TODO : read Examples: hdl_multple_engine)","title":"VHDL/Verilog Action SW Design"},{"location":"user-guide/5-hls-design/","text":"HLS Action HW Design Take hls_memcopy_1024 as an example, the top design is hardware/hdl/hls/action_wrapper.vhd_source but the HLS developer doesn't need to modify it. That is a common wrapper. hls_action() HLS developer needs to modify $ACTION_ROOT/hw/xxx.CPP , starting from hls_action() : //--- TOP LEVEL MODULE ------------------------------------------------- void hls_action(snap_membus_1024_t *din_gmem, snap_membus_1024_t *dout_gmem, snap_membus_512_t *d_ddrmem, action_reg *act_reg, action_RO_config_reg *Action_Config) { // Host Memory AXI Interface #pragma HLS INTERFACE m_axi port=din_gmem bundle=host_mem offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=din_gmem bundle=ctrl_reg offset=0x030 #pragma HLS INTERFACE m_axi port=dout_gmem bundle=host_mem offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=dout_gmem bundle=ctrl_reg offset=0x040 // DDR memory Interface #pragma HLS INTERFACE m_axi port=d_ddrmem bundle=card_mem0 offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=d_ddrmem bundle=ctrl_reg offset=0x050 // Host Memory AXI Lite Interface #pragma HLS DATA_PACK variable=Action_Config #pragma HLS INTERFACE s_axilite port=Action_Config bundle=ctrl_reg offset=0x010 #pragma HLS DATA_PACK variable=act_reg #pragma HLS INTERFACE s_axilite port=act_reg bundle=ctrl_reg offset=0x100 #pragma HLS INTERFACE s_axilite port=return bundle=ctrl_reg ... In Vivado High Level Synthesis, the above C code can be synthesized to have: Host Memory AXI Interface din_gmem dout_gmem DDR memory Interface d_ddrmem AXI Lite Interface Action_Config at offset 0x10 act_reg at offset 0x100 din_gmem and dout_gmem They represent the whole 2^64 Host memory space. But this memory space is shaped as a 128Bytes width , (2^64/128) depth array. With CAPI/OpenCAPI, the action hardware can directly use EA (effective address, the same pointer in software) to access host memory. You will see this shape from the definition of \"snap_membus_1024_t\". snap_membus_1024_t *din_gmem, snap_membus_1024_t *dout_gmem, Because din_gmem and dout_gmem are shaped as 128B width, the EA address needs to be right-shifted by log2(128) = 7 before being used as din_gmem/dout_gmem's array index. // byte address received need to be aligned with port width InputIndex = (act_reg->Data.in.addr) >> ADDR_RIGHT_SHIFT_1024; OutputIndex = (act_reg->Data.out.addr) >> ADDR_RIGHT_SHIFT_1024; Then you can use memcpy() or data = din_gmem[index] or dout_gmem[index] = data to use them. din_gmem is for reading data from Host. dout_gmem is for writing data to Host. They should be able to be combined just like d_ddrmem but we want to keep the same style as SNAP1/2 for now. d_ddrmem It represents one channel of DDR on FPGA board. In SNAP1/2 and OC-Accel, only one channel (one DDR memory controller) is implemented as an example. However, usually the FPGA board provides more than one DDR channel. There are multiple ways to arrange them so it's user-case dependent. Some FPGA boards have HBM. The developer can enable it similarly like DDR, which is introduced in Deep Dive: New board support . Please be aware of the address range you can use for a single DDR channel. d_ddrmem is shaped as a 64Bytes width , (Capacity/64) depth array. Exceeding the available DDR address range will lead to an unknown error. AXI Lite Registers The first 16bytes (0x00 to 0x0F) are pre-defined by software/include/osnap_hls_if.h , the user is not supposed to change that. It controls the start , stop and interrupt of the HLS Action. The following 8bytes (0x10 to 0x17) is action_RO_config_reg (Action_Config), defined in actions/include/hls_snap*.H , the user should set action_type and release_level information in above two fields. typedef struct { snapu32_t action_type; // 4 bytes snapu32_t release_level; // 4 bytes } action_RO_config_reg; The action_reg has 128bytes (0x100 to 0x180) , composed of two segments Control and job Data , defined in $ACTION_ROOT/hw/*.H typedef struct { CONTROL Control; /* 16 bytes */ memcopy_job_t Data; /* up to 108 bytes */ uint8_t padding[SNAP_HLS_JOBSIZE - sizeof(memcopy_job_t)]; } action_reg; Struct CONTROL is also defined in actions/include/hls_snap*.H , it defines the flags and return code . typedef struct { snapu8_t sat; snapu8_t flags; snapu16_t seq; snapu32_t Retc; snapu64_t Reserved; } CONTROL; At last, what the developer can really freely define is just job Data : memcopy_job_t Data; /* up to 108 bytes */ Note If 108 bytes are not enough, please define a small buffer in Host memory as WED buffer (Work Element Descriptor), store all of the parameters in this WED buffer, and just put the address pointers of this WED buffer into xxxx_job_t . Ask HLS Action to read the content of WED buffer first, then do the following jobs. The above register layout is also drawn in Deep Dive: Registers HLS optimization Xilinx Document UG902: Vivado High-Level Synthesis is an important guide book to understand how to add \"directives\" to your HLS C/C++ code. You can also refer to SNAP1/2 document How to Optimize HLS Action to learn how to run standalone testing before OCSE Co-simulation, and how to fully explore UNROLL and PIPELINE directives in HLS. HLS Action SW Design The steps include: snap_card_alloc_dev() snap_attach_action() Prepare job : snap_set_job() Main body : snap_action_sync_execute_job(), which has three steps: snap_action_sync_execute_job_set_regs (action, cjob); //Set action_reg by MMIO snap_action_start(action); snap_action_sync_execute_job_check_completion (action, cjob, timeout_sec); Check Return code : cjob.retc snap_detach_action() snap_card_free()","title":"(5) HLS C++ design"},{"location":"user-guide/5-hls-design/#hls-action-hw-design","text":"Take hls_memcopy_1024 as an example, the top design is hardware/hdl/hls/action_wrapper.vhd_source but the HLS developer doesn't need to modify it. That is a common wrapper.","title":"HLS Action HW Design"},{"location":"user-guide/5-hls-design/#hls_action","text":"HLS developer needs to modify $ACTION_ROOT/hw/xxx.CPP , starting from hls_action() : //--- TOP LEVEL MODULE ------------------------------------------------- void hls_action(snap_membus_1024_t *din_gmem, snap_membus_1024_t *dout_gmem, snap_membus_512_t *d_ddrmem, action_reg *act_reg, action_RO_config_reg *Action_Config) { // Host Memory AXI Interface #pragma HLS INTERFACE m_axi port=din_gmem bundle=host_mem offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=din_gmem bundle=ctrl_reg offset=0x030 #pragma HLS INTERFACE m_axi port=dout_gmem bundle=host_mem offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=dout_gmem bundle=ctrl_reg offset=0x040 // DDR memory Interface #pragma HLS INTERFACE m_axi port=d_ddrmem bundle=card_mem0 offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=d_ddrmem bundle=ctrl_reg offset=0x050 // Host Memory AXI Lite Interface #pragma HLS DATA_PACK variable=Action_Config #pragma HLS INTERFACE s_axilite port=Action_Config bundle=ctrl_reg offset=0x010 #pragma HLS DATA_PACK variable=act_reg #pragma HLS INTERFACE s_axilite port=act_reg bundle=ctrl_reg offset=0x100 #pragma HLS INTERFACE s_axilite port=return bundle=ctrl_reg ... In Vivado High Level Synthesis, the above C code can be synthesized to have: Host Memory AXI Interface din_gmem dout_gmem DDR memory Interface d_ddrmem AXI Lite Interface Action_Config at offset 0x10 act_reg at offset 0x100","title":"hls_action()"},{"location":"user-guide/5-hls-design/#din_gmem-and-dout_gmem","text":"They represent the whole 2^64 Host memory space. But this memory space is shaped as a 128Bytes width , (2^64/128) depth array. With CAPI/OpenCAPI, the action hardware can directly use EA (effective address, the same pointer in software) to access host memory. You will see this shape from the definition of \"snap_membus_1024_t\". snap_membus_1024_t *din_gmem, snap_membus_1024_t *dout_gmem, Because din_gmem and dout_gmem are shaped as 128B width, the EA address needs to be right-shifted by log2(128) = 7 before being used as din_gmem/dout_gmem's array index. // byte address received need to be aligned with port width InputIndex = (act_reg->Data.in.addr) >> ADDR_RIGHT_SHIFT_1024; OutputIndex = (act_reg->Data.out.addr) >> ADDR_RIGHT_SHIFT_1024; Then you can use memcpy() or data = din_gmem[index] or dout_gmem[index] = data to use them. din_gmem is for reading data from Host. dout_gmem is for writing data to Host. They should be able to be combined just like d_ddrmem but we want to keep the same style as SNAP1/2 for now.","title":"din_gmem and dout_gmem"},{"location":"user-guide/5-hls-design/#d_ddrmem","text":"It represents one channel of DDR on FPGA board. In SNAP1/2 and OC-Accel, only one channel (one DDR memory controller) is implemented as an example. However, usually the FPGA board provides more than one DDR channel. There are multiple ways to arrange them so it's user-case dependent. Some FPGA boards have HBM. The developer can enable it similarly like DDR, which is introduced in Deep Dive: New board support . Please be aware of the address range you can use for a single DDR channel. d_ddrmem is shaped as a 64Bytes width , (Capacity/64) depth array. Exceeding the available DDR address range will lead to an unknown error.","title":"d_ddrmem"},{"location":"user-guide/5-hls-design/#axi-lite-registers","text":"The first 16bytes (0x00 to 0x0F) are pre-defined by software/include/osnap_hls_if.h , the user is not supposed to change that. It controls the start , stop and interrupt of the HLS Action. The following 8bytes (0x10 to 0x17) is action_RO_config_reg (Action_Config), defined in actions/include/hls_snap*.H , the user should set action_type and release_level information in above two fields. typedef struct { snapu32_t action_type; // 4 bytes snapu32_t release_level; // 4 bytes } action_RO_config_reg; The action_reg has 128bytes (0x100 to 0x180) , composed of two segments Control and job Data , defined in $ACTION_ROOT/hw/*.H typedef struct { CONTROL Control; /* 16 bytes */ memcopy_job_t Data; /* up to 108 bytes */ uint8_t padding[SNAP_HLS_JOBSIZE - sizeof(memcopy_job_t)]; } action_reg; Struct CONTROL is also defined in actions/include/hls_snap*.H , it defines the flags and return code . typedef struct { snapu8_t sat; snapu8_t flags; snapu16_t seq; snapu32_t Retc; snapu64_t Reserved; } CONTROL; At last, what the developer can really freely define is just job Data : memcopy_job_t Data; /* up to 108 bytes */ Note If 108 bytes are not enough, please define a small buffer in Host memory as WED buffer (Work Element Descriptor), store all of the parameters in this WED buffer, and just put the address pointers of this WED buffer into xxxx_job_t . Ask HLS Action to read the content of WED buffer first, then do the following jobs. The above register layout is also drawn in Deep Dive: Registers","title":"AXI Lite Registers"},{"location":"user-guide/5-hls-design/#hls-optimization","text":"Xilinx Document UG902: Vivado High-Level Synthesis is an important guide book to understand how to add \"directives\" to your HLS C/C++ code. You can also refer to SNAP1/2 document How to Optimize HLS Action to learn how to run standalone testing before OCSE Co-simulation, and how to fully explore UNROLL and PIPELINE directives in HLS.","title":"HLS optimization"},{"location":"user-guide/5-hls-design/#hls-action-sw-design","text":"The steps include: snap_card_alloc_dev() snap_attach_action() Prepare job : snap_set_job() Main body : snap_action_sync_execute_job(), which has three steps: snap_action_sync_execute_job_set_regs (action, cjob); //Set action_reg by MMIO snap_action_start(action); snap_action_sync_execute_job_check_completion (action, cjob, timeout_sec); Check Return code : cjob.retc snap_detach_action() snap_card_free()","title":"HLS Action SW Design"},{"location":"user-guide/6-co-simulation/","text":"Overview of Co-Simulation Co-Simulation lets you run the software (application on the host) together with the hardware (acceleration logic on the FPGA) simulated by functional simulators from either Xilinx Vivado ( xsim ) or other EDA vendors such as Cadence Xcelium ( xrun ), Synopsys VCS ( vcs ), etc. Note All-in-one script ocaccel_workflow.py supports Xilinx Vivado xsim and Cadence Xcelium xrun . It's possible to let make model and make sim flow to support a wider range of simulators. With co-simulation, the functional correctness of your acceleration can be verified with all pieces including software and hardware before you have a real FPGA card to program. Co-simulation in OpenCAPI acceleration framework consists of three processes as shown in the picture below: Process 0 runs RTL simulator for simulating logic behavior of FPGA design Process 1 runs OCSE (OpenCAPI Simulation Engine) Process 2 runs user application. The three processes are communicated via sockets. During the simulation, the user application is exactly the one that will be ran on a real Power9 system, that is to say, for the software designer, co-simulation provides exactly the same behavior as if they are running on a real system with a real FPGA card. In this way, OCSE acts as the proxy to intercept OpenCAPI related requests from user application and re-route them to the RTL simulator, vice versa. The file structure and module level hierarchy is covered in top hierarchy in simulation step . Start Co-Simulation The basic flow of running co-simulation has been covered in simulation section of running helloworld . Please refer to it for a quick getting started. To start the co-simulation, the simplest way is to use ocaccel_workflow.py with its rich command line options. For example, run co-simulation with cadence xcelium where the user application is snap_helloworld : ./ocaccel_workflow.py -o <path to ocse> -s xcelium -t \"snap_helloworld <command line options of snap_helloworld>\" Note In the -t options, ocaccel_workflow.py searches actions/<your action>/sw for the availability of command executable ( snap_helloworld in above example). If the command you are going to run is not in actions/<your action>/sw , please specify the absolute path. or, run co-simulation with vivado xsim without specifying the user application: ./ocaccel_workflow.py -o <path to ocse> -s xsim If the user application is not specified, an xterm will pop up after co-simulation startup. In the xterm , user is expected to run any application they want as if they were running on a real POWER system with an OpenCAPI enabled FPGA card. Note In the popped up xterm , by default the path of the current working directory is where the simulation starts (the simout path). For any command that will trigger access to FPGA card, it should be ran in this simout path, otherwise the attempt of running any of these commands would fail. Check the Simulation Result Running with -t option Warning If there are fatal errors encountered in the RTL simulator or OCSE during simulation, the simulation result will be treated as FAILED regardless of the result of user application, aka, the command specified in -t option. Running without -t option If the test is running without -t option, i.e., running with xterm . Since it is hard to track user behaviors in the xterm (users are expected to run multiple commands with and/or without OpenCAPI traffics), the simulation result is determined by the user themselves. Simulation Output Path Each simulation has a simout path structure as below: <...>/hardware/sim/<simulator>/<timestamp>.<rand_num> For example, if xcelium is used, we have the following directory as the simulation output: <...>/hardware/sim/xcelium/1568471461533.1648652400 All of the simulation related logs, files and waveforms can be found in the simout directory. After each simulation, a soft link named as latest will be created pointing to the simout of the latest simulation. Check the Waveform By default, ocaccel_workflow.py enables waveform dumping. The waveform can be found in the following directory: For Vivado xsim : <...>/hardware/sim/xsim/<timestamp>.<rand_num>/top.wdb For Cadence xcelium : <...>/hardware/sim/xcelium/timestamp>.<rand_num>/capiWave/ To control the simulator specific waveform dumping options (written in tcl), please edit the following files: For Vivado xsim : <...>/hardware/sim/xsaet.tcl For Cadence xcelium : <...>/hardware/sim/ncaet.tcl To disable the waveform dumping, use the following command options in ocaccel_workflow.py : ./ocaccel_workflow.py --no_wave Recompile for small design changes If you have modified the design just a little bit (no file name and hierarchy changed), you can just simply rerun the compile script under: <...>/hardware/sim/xsim/top.sh or <...>/hardware/sim/xcelium/top.sh The simulation will be rebuilt and just run ./run_sim to start the xterm window. It can save time to debug your logic. Warning Be aware of the files having \" _source \" suffixes. If you need to change \"FILE\", please make the modifications on \"FILE_source\", then do following: rm -f FILE cd <...>/hardware make snap_preprocess_execute So \"FILE\" will be regenerated from \"FILE_source\". However, if your design change is big, you should still start from ./ocaccel_workflow.py .","title":"(6) Co-Simulation"},{"location":"user-guide/6-co-simulation/#overview-of-co-simulation","text":"Co-Simulation lets you run the software (application on the host) together with the hardware (acceleration logic on the FPGA) simulated by functional simulators from either Xilinx Vivado ( xsim ) or other EDA vendors such as Cadence Xcelium ( xrun ), Synopsys VCS ( vcs ), etc. Note All-in-one script ocaccel_workflow.py supports Xilinx Vivado xsim and Cadence Xcelium xrun . It's possible to let make model and make sim flow to support a wider range of simulators. With co-simulation, the functional correctness of your acceleration can be verified with all pieces including software and hardware before you have a real FPGA card to program. Co-simulation in OpenCAPI acceleration framework consists of three processes as shown in the picture below: Process 0 runs RTL simulator for simulating logic behavior of FPGA design Process 1 runs OCSE (OpenCAPI Simulation Engine) Process 2 runs user application. The three processes are communicated via sockets. During the simulation, the user application is exactly the one that will be ran on a real Power9 system, that is to say, for the software designer, co-simulation provides exactly the same behavior as if they are running on a real system with a real FPGA card. In this way, OCSE acts as the proxy to intercept OpenCAPI related requests from user application and re-route them to the RTL simulator, vice versa. The file structure and module level hierarchy is covered in top hierarchy in simulation step .","title":"Overview of Co-Simulation"},{"location":"user-guide/6-co-simulation/#start-co-simulation","text":"The basic flow of running co-simulation has been covered in simulation section of running helloworld . Please refer to it for a quick getting started. To start the co-simulation, the simplest way is to use ocaccel_workflow.py with its rich command line options. For example, run co-simulation with cadence xcelium where the user application is snap_helloworld : ./ocaccel_workflow.py -o <path to ocse> -s xcelium -t \"snap_helloworld <command line options of snap_helloworld>\" Note In the -t options, ocaccel_workflow.py searches actions/<your action>/sw for the availability of command executable ( snap_helloworld in above example). If the command you are going to run is not in actions/<your action>/sw , please specify the absolute path. or, run co-simulation with vivado xsim without specifying the user application: ./ocaccel_workflow.py -o <path to ocse> -s xsim If the user application is not specified, an xterm will pop up after co-simulation startup. In the xterm , user is expected to run any application they want as if they were running on a real POWER system with an OpenCAPI enabled FPGA card. Note In the popped up xterm , by default the path of the current working directory is where the simulation starts (the simout path). For any command that will trigger access to FPGA card, it should be ran in this simout path, otherwise the attempt of running any of these commands would fail.","title":"Start Co-Simulation"},{"location":"user-guide/6-co-simulation/#check-the-simulation-result","text":"","title":"Check the Simulation Result"},{"location":"user-guide/6-co-simulation/#running-with-t-option","text":"Warning If there are fatal errors encountered in the RTL simulator or OCSE during simulation, the simulation result will be treated as FAILED regardless of the result of user application, aka, the command specified in -t option.","title":"Running with -t option"},{"location":"user-guide/6-co-simulation/#running-without-t-option","text":"If the test is running without -t option, i.e., running with xterm . Since it is hard to track user behaviors in the xterm (users are expected to run multiple commands with and/or without OpenCAPI traffics), the simulation result is determined by the user themselves.","title":"Running without -t option"},{"location":"user-guide/6-co-simulation/#simulation-output-path","text":"Each simulation has a simout path structure as below: <...>/hardware/sim/<simulator>/<timestamp>.<rand_num> For example, if xcelium is used, we have the following directory as the simulation output: <...>/hardware/sim/xcelium/1568471461533.1648652400 All of the simulation related logs, files and waveforms can be found in the simout directory. After each simulation, a soft link named as latest will be created pointing to the simout of the latest simulation.","title":"Simulation Output Path"},{"location":"user-guide/6-co-simulation/#check-the-waveform","text":"By default, ocaccel_workflow.py enables waveform dumping. The waveform can be found in the following directory: For Vivado xsim : <...>/hardware/sim/xsim/<timestamp>.<rand_num>/top.wdb For Cadence xcelium : <...>/hardware/sim/xcelium/timestamp>.<rand_num>/capiWave/ To control the simulator specific waveform dumping options (written in tcl), please edit the following files: For Vivado xsim : <...>/hardware/sim/xsaet.tcl For Cadence xcelium : <...>/hardware/sim/ncaet.tcl To disable the waveform dumping, use the following command options in ocaccel_workflow.py : ./ocaccel_workflow.py --no_wave","title":"Check the Waveform"},{"location":"user-guide/6-co-simulation/#recompile-for-small-design-changes","text":"If you have modified the design just a little bit (no file name and hierarchy changed), you can just simply rerun the compile script under: <...>/hardware/sim/xsim/top.sh or <...>/hardware/sim/xcelium/top.sh The simulation will be rebuilt and just run ./run_sim to start the xterm window. It can save time to debug your logic. Warning Be aware of the files having \" _source \" suffixes. If you need to change \"FILE\", please make the modifications on \"FILE_source\", then do following: rm -f FILE cd <...>/hardware make snap_preprocess_execute So \"FILE\" will be regenerated from \"FILE_source\". However, if your design change is big, you should still start from ./ocaccel_workflow.py .","title":"Recompile for small design changes"},{"location":"user-guide/7-build-image/","text":"Build the FPGA Bitstream To build FPGA bitstream, you can simply add the following command line option to ocaccel_workflow.py : ./ocaccel_workflow.py --make_image With --make_image , ocaccel_workflow.py will start building image after finishing simulation. If you just want to start building the bitstream without running simulation, please run with the following commands: ./ocaccel_workflow.py --no_make_model --no_run_sim --make_image Check the Result of FPGA Bitstream Basic Result The basic concepts and flows of building FPGA bitstream, including synthesis, place, route and optimization, can be found in Xilinx official documentation ug904 . After finishing the whole bitstream building process, ocaccel_workflow.py will notify the user if vivado is managed to generate bitstream by giving a FAILED or PASSED message. More detailed messaging can be found in ocaccel_workflow.make_image.log in the root directory. If the bitstream is generated, they can be found in the following directory: <ocaccel root>/hardware/build/Images Just take the *.bin or *.bit file in that directory to program the FPGA card according to your own system requirements. Further Debugging Materials if Bitstream Generation Failed If the bitstream is not generated, possibly due to failure of closing timing and/or over-utilization, there are following files for user to further root cause. The reports, including timing report and utilization report: <ocaccel root>/hardware/build/Reports Reports in this directory is generated by Vivado for different stages/phases during the bitstream generation. Please refer to Xilinx Vivado documentation for further explanation. The checkpoints, which allows user to check the status after each stages/phases in Vivado GUI: <ocaccel root>/hardware/build/Checkpoints Specify Bitstream Generation Strategy Vivado has a rich set of strategies/directives for user to choose during bitstream generation (implementation), which has been explained in ug904 . By default, ocaccel_workflow.py uses \" Explore \" as the strategy which means for all the stages/phases, the directive is \" Explore \". You can change this setting by putting a tcl file with strategy settings in the following directory: <oaccel root>/actions/<your action>/hw/tcl/ For example, a file named strategy.tcl can be put in above directory, with the following contents: set_property strategy \"Flow_PerfOptimized_high\" [get_runs synth_1]; set_property strategy \"Performance_NetDelay_high\" [get_runs impl_1]; These settings tell Vivado to use Flow_PerfOptimized_high during synthesis, and Performance_NetDelay_high during implementation. Note The file name doesn't matter in this case. ocaccel_workflow.py is going to source any tcl file in <ocaccel root>actions/<your action>/hw/tcl/ directory.","title":"(7) Build image"},{"location":"user-guide/7-build-image/#build-the-fpga-bitstream","text":"To build FPGA bitstream, you can simply add the following command line option to ocaccel_workflow.py : ./ocaccel_workflow.py --make_image With --make_image , ocaccel_workflow.py will start building image after finishing simulation. If you just want to start building the bitstream without running simulation, please run with the following commands: ./ocaccel_workflow.py --no_make_model --no_run_sim --make_image","title":"Build the FPGA Bitstream"},{"location":"user-guide/7-build-image/#check-the-result-of-fpga-bitstream","text":"","title":"Check the Result of FPGA Bitstream"},{"location":"user-guide/7-build-image/#basic-result","text":"The basic concepts and flows of building FPGA bitstream, including synthesis, place, route and optimization, can be found in Xilinx official documentation ug904 . After finishing the whole bitstream building process, ocaccel_workflow.py will notify the user if vivado is managed to generate bitstream by giving a FAILED or PASSED message. More detailed messaging can be found in ocaccel_workflow.make_image.log in the root directory. If the bitstream is generated, they can be found in the following directory: <ocaccel root>/hardware/build/Images Just take the *.bin or *.bit file in that directory to program the FPGA card according to your own system requirements.","title":"Basic Result"},{"location":"user-guide/7-build-image/#further-debugging-materials-if-bitstream-generation-failed","text":"If the bitstream is not generated, possibly due to failure of closing timing and/or over-utilization, there are following files for user to further root cause. The reports, including timing report and utilization report: <ocaccel root>/hardware/build/Reports Reports in this directory is generated by Vivado for different stages/phases during the bitstream generation. Please refer to Xilinx Vivado documentation for further explanation. The checkpoints, which allows user to check the status after each stages/phases in Vivado GUI: <ocaccel root>/hardware/build/Checkpoints","title":"Further Debugging Materials if Bitstream Generation Failed"},{"location":"user-guide/7-build-image/#specify-bitstream-generation-strategy","text":"Vivado has a rich set of strategies/directives for user to choose during bitstream generation (implementation), which has been explained in ug904 . By default, ocaccel_workflow.py uses \" Explore \" as the strategy which means for all the stages/phases, the directive is \" Explore \". You can change this setting by putting a tcl file with strategy settings in the following directory: <oaccel root>/actions/<your action>/hw/tcl/ For example, a file named strategy.tcl can be put in above directory, with the following contents: set_property strategy \"Flow_PerfOptimized_high\" [get_runs synth_1]; set_property strategy \"Performance_NetDelay_high\" [get_runs impl_1]; These settings tell Vivado to use Flow_PerfOptimized_high during synthesis, and Performance_NetDelay_high during implementation. Note The file name doesn't matter in this case. ocaccel_workflow.py is going to source any tcl file in <ocaccel root>actions/<your action>/hw/tcl/ directory.","title":"Specify Bitstream Generation Strategy"},{"location":"user-guide/8-deploy/","text":"Deploy to Power Server Program FPGA There are two ways to program FPGA: * Program Flash * Program FPGA chip Program Flash This is the default way to program FPGA. Log on to Power9 server, $ git clone https://github.com/OpenCAPI/oc-utils/ $ make $ sudo make install Copy the generated hardware/build/Images/*.bin from the development machine to Power9 server, and execute: For cards with SPIx8 Flash interface: $ sudo oc-flash-script <file_primary.bin> <file_secondary.bin> A oc-reload script is called automatically if the flash programming succeeds. This script reloads the bitstream from Flash and set up the OpenCAPI links again. Warning AD9H7 card doesn't support above scripts temporally. (TODO) Check if the device is valid: $ ls /dev/ocxl IBM,oc-snap.0007:00:00.1.0 Program FPGA chip Not like \"programming flash\" which permanently stores FPGA image into the flash on the FPGA board, programming FPGA chip is a temporal method and mainly used for debugging purpose. It uses *.bit file and the programmed image will be lost if the server is powered off. Prepare a laptop/desktop machine and install Vivado Lab . Use USB cable to connect it to the FPGA board's USB-JTAG debugging port. Then in Vivado Lab, right click the FPGA device name and select \"program device...\" Then run sudo oc-reset This command will bring the new bitstream working. Warning Host Flashing and oc-reset require firmware and OS kernel support. Contact your system provider for detailed information. Install libocxl See https://github.com/OpenCAPI/libocxl/ Compile OC-Accel software and actions $ git clone https://github.com/<MY_NAME>/oc-accel $ make apps You can check the FPGA image version, name and build date/time by $ cd software/tools $ sudo ./oc_maint -vvv [main] Enter [snap_open] Enter: IBM,oc-snap [snap_open] Exit 0x141730670 [snap_version] Enter SNAP Card Id: 0x31 Name: AD9V3. NVME disabled, 0 MB DRAM available. (Align: 1 Min_DMA: 1) SNAP FPGA Release: v0.2.0 Distance: 255 GIT: 0x12fb0b24 SNAP FPGA Build (Y/M/D): 2019/09/13 Time (H:M): 11:24 [snap_version] Exit [snap_action_info] Enter Short | Action Type | Level | Action Name ------+--------------+-----------+------------ 0 0x10140002 0x00000002 IBM hdl_single_engine in Verilog (1024b) [snap_action_info] Exit rc: 0 [main] Exit rc: 0 [snap_close] Enter [snap_close] Exit 0 Run Application $ cd actions/<my_new_action>/sw $ sudo ./<app_name> Note Whenever calling the FPGA card, sudo is needed.","title":"(8) Deploy on Power Server"},{"location":"user-guide/8-deploy/#deploy-to-power-server","text":"","title":"Deploy to Power Server"},{"location":"user-guide/8-deploy/#program-fpga","text":"There are two ways to program FPGA: * Program Flash * Program FPGA chip","title":"Program FPGA"},{"location":"user-guide/8-deploy/#program-flash","text":"This is the default way to program FPGA. Log on to Power9 server, $ git clone https://github.com/OpenCAPI/oc-utils/ $ make $ sudo make install Copy the generated hardware/build/Images/*.bin from the development machine to Power9 server, and execute: For cards with SPIx8 Flash interface: $ sudo oc-flash-script <file_primary.bin> <file_secondary.bin> A oc-reload script is called automatically if the flash programming succeeds. This script reloads the bitstream from Flash and set up the OpenCAPI links again. Warning AD9H7 card doesn't support above scripts temporally. (TODO) Check if the device is valid: $ ls /dev/ocxl IBM,oc-snap.0007:00:00.1.0","title":"Program Flash"},{"location":"user-guide/8-deploy/#program-fpga-chip","text":"Not like \"programming flash\" which permanently stores FPGA image into the flash on the FPGA board, programming FPGA chip is a temporal method and mainly used for debugging purpose. It uses *.bit file and the programmed image will be lost if the server is powered off. Prepare a laptop/desktop machine and install Vivado Lab . Use USB cable to connect it to the FPGA board's USB-JTAG debugging port. Then in Vivado Lab, right click the FPGA device name and select \"program device...\" Then run sudo oc-reset This command will bring the new bitstream working. Warning Host Flashing and oc-reset require firmware and OS kernel support. Contact your system provider for detailed information.","title":"Program FPGA chip"},{"location":"user-guide/8-deploy/#install-libocxl","text":"See https://github.com/OpenCAPI/libocxl/","title":"Install libocxl"},{"location":"user-guide/8-deploy/#compile-oc-accel-software-and-actions","text":"$ git clone https://github.com/<MY_NAME>/oc-accel $ make apps You can check the FPGA image version, name and build date/time by $ cd software/tools $ sudo ./oc_maint -vvv [main] Enter [snap_open] Enter: IBM,oc-snap [snap_open] Exit 0x141730670 [snap_version] Enter SNAP Card Id: 0x31 Name: AD9V3. NVME disabled, 0 MB DRAM available. (Align: 1 Min_DMA: 1) SNAP FPGA Release: v0.2.0 Distance: 255 GIT: 0x12fb0b24 SNAP FPGA Build (Y/M/D): 2019/09/13 Time (H:M): 11:24 [snap_version] Exit [snap_action_info] Enter Short | Action Type | Level | Action Name ------+--------------+-----------+------------ 0 0x10140002 0x00000002 IBM hdl_single_engine in Verilog (1024b) [snap_action_info] Exit rc: 0 [main] Exit rc: 0 [snap_close] Enter [snap_close] Exit 0","title":"Compile OC-Accel software and actions"},{"location":"user-guide/8-deploy/#run-application","text":"$ cd actions/<my_new_action>/sw $ sudo ./<app_name> Note Whenever calling the FPGA card, sudo is needed.","title":"Run Application"},{"location":"user-guide/9-migrate/","text":"Migrate from SNAP1.0/2.0 Data width change The AXI data port width of OC-Accel is 1024bit. But the actions developed in SNAP (CAPI1.0/2.0) use 512b. Please select 512b in Kconfig Menu. Then a dwidth_converter will be inserted automatically. All of the AXI4 features in SNAP are supported by OC-Accel, and it has added more. Read OC-Accel AXI4 feature list for more details. Clock frequency The default clock frequency in SNAP (CAPI1.0/2.0) was 250MHz . There was no asynchronous logic between capi2-bsp, snap_core and action_wrapper so the frequency had to be adjusted together. The default clock frequency for action_wrapper in OC-Accel is 200MHz . Asynchronous clocks have been designed for oc-bip, snap_core and action_wrapper so the clock frequency can be adjusted more flexibly for each part. See Clock domains . Library name SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) libcxl libocxl libsnap libosnap Included headers Many headers have changed from \"snap\" to \"osnap\" to avoid conflicts. SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) snap_types.h osnap_types.h snap_tools.h osnap_tools.h snap_queue.h osnap_queue.h snap_internal.h osnap_internal.h snap_hls_if.h osnap_hls_if.h snap_m_regs.h osnap_global_regs.h snap_s_regs.h N/A snap_regs.h N/A libsnap.h libosnap.h There is a big change of the Register Map. OC-Accel has simplified and enlarged the Register Layout. SNAP1/2 Register map OC-Accel Register map API changes The APIs to operate registers have been renamed to clarify it is action region or global region. | SNAP (CAPI1.0/2.0) | OC-Accel (OpenCAPI3.0) | | --- | --- | | snap_mmio_read32() | snap_action_read32() | | snap_mmio_write32() | snap_action_write32() | | snap_mmio_read64() | snap_global_read64()| | snap_mmio_write64()| snap_global_write64() | The API changes also reflect the Register map changes. SNAP_CONFIG=CPU discarded In SNAP (CAPI1.0/2.0), it has implemented a group of function pointers for CPU to emulate the FPGA action, aka \"software action\". It is enabled when setting SNAP_CONFIG=CPU : /* Software version of the lowlevel functions */ static struct snap_funcs software_funcs = { .card_alloc_dev = sw_card_alloc_dev, .attach_action = sw_attach_action, /* attach Action */ .detach_action = sw_detach_action, /* detach Action */ .mmio_write32 = sw_mmio_write32, .mmio_read32 = sw_mmio_read32, .mmio_write64 = sw_mmio_write64, .mmio_read64 = sw_mmio_read64, .card_free = sw_card_free, .card_ioctl = sw_card_ioctl, }; These functions have been removed. The original purpose of SNAP_CONFIG=CPU was to provide a way to fall back to software execution when FPGA is not available. However, this actually can be easily done by higher level of application control, for example: if (!snap_card_alloc_dev()) //Failed to open FPGA card call_original_software_function So there is no need to rewrite the original software function at all. The corresponding concept is SNAP_CONFIG=FPGA and it becomes the ONLY working mode in OC-Accel. So the variable SNAP_CONFIG has been deleted. Open the card OpenCAPI device name is slightly different compared to SNAP1/2. // Allocate the card that will be used if(card_no == 0) snprintf(device, sizeof(device)-1, \"IBM,oc-snap\"); else snprintf(device, sizeof(device)-1, \"/dev/ocxl/IBM,oc-snap.000%d:00:00.1.0\", card_no); card = snap_card_alloc_dev(device, SNAP_VENDOR_ID_IBM, SNAP_DEVICE_ID_SNAP); Use interrupt When action_irq=1, please pay attention to following: snap_action_flag_t action_irq = SNAP_ACTION_DONE_IRQ; Add a step to set irq handle: if(action_irq) snap_action_assign_irq(action, ACTION_IRQ_SRC_LO); ACTION_TYPE and RELEASE LEVEL Note These changes are only for HLS Actions Action types convention 0x1014 is IBM's reference, action's IDs begining with a 2 (eg 0x1014 2 002 are conventionnaly HDL like actions (where actions is described with hardware design languages), while those with a 3 (eg 0x1014 3 008) are HLS like (actions are described using HLS tool) SNAP Application modificications In actions/<hls_xxx>/include/xxx.h , define your ACTION_TYPE and RELEASE_LEVEL as following: // ------------ MUST READ ----------- // ACTION_TYPE and RELEASE_LEVEL are automatically handled. // 1. Define them in header file (here), use HEX 32bits numbers // 2. They will be extracted by hardware/setup/patch_version.sh // 3. And put into snap_global_vars.v // 4. Used by hardware/hls/action_wrapper.v #define ACTION_TYPE 0x10143008 #define RELEASE_LEVEL 0x00000022 // For snap_maint, Action descriptions are decoded with the help of software/tools/snap_actions.h // Please modify this file so snap_maint can recognize this action. // ------------ MUST READ ----------- In actions/<hls_xxx>/sw/xxx.c when using snap_attach_action , just use ACTION_TYPE : action = snap_attach_action(card, ACTION_TYPE, action_irq, 60); And in actions/<hls_xxx>/hw/xxx.cpp : Remove action_RO_config_reg *Action_Config . It was only used to handle ACTION_TYPE and RELEASE_LEVEL and they are not needed anymore. Remove following: #pragma HLS DATA_PACK variable=Action_Config #pragma HLS INTERFACE s_axilite port=Action_Config bundle=ctrl_reg offset=0x010 Replace following: switch (Action_Register->Control.flags) { case 0: Action_Config->action_type = (snapu32_t) CHECKSUM_ACTION_TYPE; Action_Config->release_level = (snapu32_t) RELEASE_LEVEL; Action_Register->Control.Retc = (snapu32_t)0xe00f; return; break; default: Action_Register->Control.Retc = (snapu32_t)0x0; process_action(Action_Register); break; } to simply call process_action(Action_Register);","title":"(9) Migrate from SNAP1/2"},{"location":"user-guide/9-migrate/#migrate-from-snap1020","text":"","title":"Migrate from SNAP1.0/2.0"},{"location":"user-guide/9-migrate/#data-width-change","text":"The AXI data port width of OC-Accel is 1024bit. But the actions developed in SNAP (CAPI1.0/2.0) use 512b. Please select 512b in Kconfig Menu. Then a dwidth_converter will be inserted automatically. All of the AXI4 features in SNAP are supported by OC-Accel, and it has added more. Read OC-Accel AXI4 feature list for more details.","title":"Data width change"},{"location":"user-guide/9-migrate/#clock-frequency","text":"The default clock frequency in SNAP (CAPI1.0/2.0) was 250MHz . There was no asynchronous logic between capi2-bsp, snap_core and action_wrapper so the frequency had to be adjusted together. The default clock frequency for action_wrapper in OC-Accel is 200MHz . Asynchronous clocks have been designed for oc-bip, snap_core and action_wrapper so the clock frequency can be adjusted more flexibly for each part. See Clock domains .","title":"Clock frequency"},{"location":"user-guide/9-migrate/#library-name","text":"SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) libcxl libocxl libsnap libosnap","title":"Library name"},{"location":"user-guide/9-migrate/#included-headers","text":"Many headers have changed from \"snap\" to \"osnap\" to avoid conflicts. SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) snap_types.h osnap_types.h snap_tools.h osnap_tools.h snap_queue.h osnap_queue.h snap_internal.h osnap_internal.h snap_hls_if.h osnap_hls_if.h snap_m_regs.h osnap_global_regs.h snap_s_regs.h N/A snap_regs.h N/A libsnap.h libosnap.h There is a big change of the Register Map. OC-Accel has simplified and enlarged the Register Layout. SNAP1/2 Register map OC-Accel Register map","title":"Included headers"},{"location":"user-guide/9-migrate/#api-changes","text":"The APIs to operate registers have been renamed to clarify it is action region or global region. | SNAP (CAPI1.0/2.0) | OC-Accel (OpenCAPI3.0) | | --- | --- | | snap_mmio_read32() | snap_action_read32() | | snap_mmio_write32() | snap_action_write32() | | snap_mmio_read64() | snap_global_read64()| | snap_mmio_write64()| snap_global_write64() | The API changes also reflect the Register map changes.","title":"API changes"},{"location":"user-guide/9-migrate/#snap_configcpu-discarded","text":"In SNAP (CAPI1.0/2.0), it has implemented a group of function pointers for CPU to emulate the FPGA action, aka \"software action\". It is enabled when setting SNAP_CONFIG=CPU : /* Software version of the lowlevel functions */ static struct snap_funcs software_funcs = { .card_alloc_dev = sw_card_alloc_dev, .attach_action = sw_attach_action, /* attach Action */ .detach_action = sw_detach_action, /* detach Action */ .mmio_write32 = sw_mmio_write32, .mmio_read32 = sw_mmio_read32, .mmio_write64 = sw_mmio_write64, .mmio_read64 = sw_mmio_read64, .card_free = sw_card_free, .card_ioctl = sw_card_ioctl, }; These functions have been removed. The original purpose of SNAP_CONFIG=CPU was to provide a way to fall back to software execution when FPGA is not available. However, this actually can be easily done by higher level of application control, for example: if (!snap_card_alloc_dev()) //Failed to open FPGA card call_original_software_function So there is no need to rewrite the original software function at all. The corresponding concept is SNAP_CONFIG=FPGA and it becomes the ONLY working mode in OC-Accel. So the variable SNAP_CONFIG has been deleted.","title":"SNAP_CONFIG=CPU discarded"},{"location":"user-guide/9-migrate/#open-the-card","text":"OpenCAPI device name is slightly different compared to SNAP1/2. // Allocate the card that will be used if(card_no == 0) snprintf(device, sizeof(device)-1, \"IBM,oc-snap\"); else snprintf(device, sizeof(device)-1, \"/dev/ocxl/IBM,oc-snap.000%d:00:00.1.0\", card_no); card = snap_card_alloc_dev(device, SNAP_VENDOR_ID_IBM, SNAP_DEVICE_ID_SNAP);","title":"Open the card"},{"location":"user-guide/9-migrate/#use-interrupt","text":"When action_irq=1, please pay attention to following: snap_action_flag_t action_irq = SNAP_ACTION_DONE_IRQ; Add a step to set irq handle: if(action_irq) snap_action_assign_irq(action, ACTION_IRQ_SRC_LO);","title":"Use interrupt"},{"location":"user-guide/9-migrate/#action_type-and-release-level","text":"Note These changes are only for HLS Actions","title":"ACTION_TYPE and RELEASE LEVEL"},{"location":"user-guide/9-migrate/#action-types-convention","text":"0x1014 is IBM's reference, action's IDs begining with a 2 (eg 0x1014 2 002 are conventionnaly HDL like actions (where actions is described with hardware design languages), while those with a 3 (eg 0x1014 3 008) are HLS like (actions are described using HLS tool)","title":"Action types convention"},{"location":"user-guide/9-migrate/#snap-application-modificications","text":"In actions/<hls_xxx>/include/xxx.h , define your ACTION_TYPE and RELEASE_LEVEL as following: // ------------ MUST READ ----------- // ACTION_TYPE and RELEASE_LEVEL are automatically handled. // 1. Define them in header file (here), use HEX 32bits numbers // 2. They will be extracted by hardware/setup/patch_version.sh // 3. And put into snap_global_vars.v // 4. Used by hardware/hls/action_wrapper.v #define ACTION_TYPE 0x10143008 #define RELEASE_LEVEL 0x00000022 // For snap_maint, Action descriptions are decoded with the help of software/tools/snap_actions.h // Please modify this file so snap_maint can recognize this action. // ------------ MUST READ ----------- In actions/<hls_xxx>/sw/xxx.c when using snap_attach_action , just use ACTION_TYPE : action = snap_attach_action(card, ACTION_TYPE, action_irq, 60); And in actions/<hls_xxx>/hw/xxx.cpp : Remove action_RO_config_reg *Action_Config . It was only used to handle ACTION_TYPE and RELEASE_LEVEL and they are not needed anymore. Remove following: #pragma HLS DATA_PACK variable=Action_Config #pragma HLS INTERFACE s_axilite port=Action_Config bundle=ctrl_reg offset=0x010 Replace following: switch (Action_Register->Control.flags) { case 0: Action_Config->action_type = (snapu32_t) CHECKSUM_ACTION_TYPE; Action_Config->release_level = (snapu32_t) RELEASE_LEVEL; Action_Register->Control.Retc = (snapu32_t)0xe00f; return; break; default: Action_Register->Control.Retc = (snapu32_t)0x0; process_action(Action_Register); break; } to simply call process_action(Action_Register);","title":"SNAP Application modificications"}]}